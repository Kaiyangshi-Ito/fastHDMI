{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the class fundementals \n",
    "\n",
    "## Kai Yang\n",
    "## <kai.yang2 \"at\" mail.mcgill.ca>\n",
    "## License :: OSI Approved :: GNU Affero General Public License v3 or later (AGPLv3+)\n",
    "## [GPG Public key Fingerprint: B9F863A56220DBD56B91C3E835022A1A5941D810](https://keys.openpgp.org/vks/v1/by-fingerprint/B9F863A56220DBD56B91C3E835022A1A5941D810)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-09T22:53:29.209420Z",
     "start_time": "2022-08-09T22:53:28.636095Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "# import os, sys\n",
    "# import collections\n",
    "import numpy as _np\n",
    "# import matplotlib.markers as markers\n",
    "# import matplotlib.pyplot as plt\n",
    "# import timeit\n",
    "# import collections\n",
    "# from scipy.stats import median_abs_deviation as mad\n",
    "# import multiprocessing\n",
    "# import cProfile\n",
    "# import itertools\n",
    "from numba import jit as _jit\n",
    "from numba import njit as _njit\n",
    "from bed_reader import open_bed as _open_bed\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore') # this is just to hide all the warnings\n",
    "# import rpy2.robjects as robjects\n",
    "# import matplotlib.pyplot as plt # change font globally to Times\n",
    "# plt.style.use('ggplot')\n",
    "# plt.rcParams.update({\n",
    "#     \"text.usetex\": True,\n",
    "#     \"font.family\": \"Times New Roman\",\n",
    "#     \"font.sans-serif\": [\"Times New Roman\"],\n",
    "#     \"font.size\": 12})\n",
    "\n",
    "# os.chdir(sys.path[0]) # ensure working direcotry is set same as the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-09T22:53:29.411988Z",
     "start_time": "2022-08-09T22:53:29.210970Z"
    }
   },
   "outputs": [],
   "source": [
    "######################################  some SCAD and MCP things  #######################################\n",
    "@_jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def soft_thresholding_PCA(x, lambda_, pca_p):\n",
    "    '''\n",
    "    To calculate soft-thresholding mapping of a given ONE-DIMENSIONAL tensor, BESIDES THE FIRST TERM (so beta_0 will not be penalized). \n",
    "    This function is to be used for calculation involving L1 penalty term later. \n",
    "    '''\n",
    "    return _np.hstack(\n",
    "        (x[0:pca_p + 1],\n",
    "         _np.where(\n",
    "             _np.abs(x[pca_p + 1:]) > lambda_,\n",
    "             x[pca_p + 1:] - _np.sign(x[pca_p + 1:]) * lambda_, 0)))\n",
    "\n",
    "\n",
    "soft_thresholding_PCA(_np.arange(-1000, 1000) / 1000, .5, 10)\n",
    "\n",
    "\n",
    "@_jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def SCAD_PCA(x, lambda_, a, pca_p):\n",
    "    '''\n",
    "    To calculate SCAD penalty value;\n",
    "    #x can be a multi-dimensional tensor;\n",
    "    lambda_, a are scalars;\n",
    "    Fan and Li suggests to take a as 3.7 \n",
    "    '''\n",
    "    # here I notice the function is de facto a function of absolute value of x, therefore take absolute value first to simplify calculation\n",
    "    x = _np.abs(x)\n",
    "    temp = _np.where(\n",
    "        x <= lambda_, lambda_ * x,\n",
    "        _np.where(x < a * lambda_,\n",
    "                  (2 * a * lambda_ * x - x**2 - lambda_**2) / (2 * (a - 1)),\n",
    "                  lambda_**2 * (a + 1) / 2))\n",
    "    temp[0:pca_p + 1] = 0.  # this is to NOT penalize intercept beta later\n",
    "    return temp\n",
    "\n",
    "\n",
    "@_jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def SCAD_grad_PCA(x, lambda_, a, pca_p):\n",
    "    '''\n",
    "    To calculate the gradient of SCAD wrt. input x; \n",
    "    #x can be a multi-dimensional tensor. \n",
    "    '''\n",
    "    # here decompose x to sign and its absolute value for easier calculation\n",
    "    sgn = _np.sign(x)\n",
    "    x = _np.abs(x)\n",
    "    temp = _np.where(\n",
    "        x <= lambda_, lambda_ * sgn,\n",
    "        _np.where(x < a * lambda_, (a * lambda_ * sgn - sgn * x) / (a - 1), 0))\n",
    "    temp[0:pca_p + 1] = 0.  # this is to NOT penalize intercept beta later\n",
    "    return temp\n",
    "\n",
    "\n",
    "@_jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def MCP_PCA(x, lambda_, gamma, pca_p):\n",
    "    '''\n",
    "    To calculate MCP penalty value; \n",
    "    #x can be a multi-dimensional tensor. \n",
    "    '''\n",
    "    # the function is a function of absolute value of x\n",
    "    x = _np.abs(x)\n",
    "    temp = _np.where(x <= gamma * lambda_, lambda_ * x - x**2 / (2 * gamma),\n",
    "                     .5 * gamma * lambda_**2)\n",
    "    temp[0:pca_p + 1] = 0.  # this is to NOT penalize intercept beta later\n",
    "    return temp\n",
    "\n",
    "\n",
    "@_jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def MCP_grad_PCA(x, lambda_, gamma, pca_p):\n",
    "    '''\n",
    "    To calculate MCP gradient wrt. input x; \n",
    "    #x can be a multi-dimensional tensor. \n",
    "    '''\n",
    "    temp = _np.where(\n",
    "        _np.abs(x) < gamma * lambda_,\n",
    "        lambda_ * _np.sign(x) - x / gamma, _np.zeros_like(x))\n",
    "    temp[0:pca_p + 1] = 0.  # this is to NOT penalize intercept beta later\n",
    "    return temp\n",
    "\n",
    "\n",
    "@_jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def SCAD_concave_PCA(x, lambda_, a, pca_p):\n",
    "    '''\n",
    "    The value of concave part of SCAD penalty; \n",
    "    #x can be a multi-dimensional tensor. \n",
    "    '''\n",
    "    x = _np.abs(x)\n",
    "    temp = _np.where(\n",
    "        x <= lambda_, 0.,\n",
    "        _np.where(x < a * lambda_,\n",
    "                  (lambda_ * x - (x**2 + lambda_**2) / 2) / (a - 1),\n",
    "                  (a + 1) / 2 * lambda_**2 - lambda_ * x))\n",
    "    temp[0:pca_p + 1] = 0.  # this is to NOT penalize intercept beta later\n",
    "    return temp\n",
    "\n",
    "\n",
    "@_jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def SCAD_concave_grad_PCA(x, lambda_, a, pca_p):\n",
    "    '''\n",
    "    The gradient of concave part of SCAD penalty wrt. input x; \n",
    "    #x can be a multi-dimensional tensor. \n",
    "    '''\n",
    "    sgn = _np.sign(x)\n",
    "    x = _np.abs(x)\n",
    "    temp = _np.where(\n",
    "        x <= lambda_, 0.,\n",
    "        _np.where(x < a * lambda_, (lambda_ * sgn - sgn * x) / (a - 1),\n",
    "                  -lambda_ * sgn))\n",
    "    temp[0:pca_p + 1] = 0.  # this is to NOT penalize intercept beta later\n",
    "    return temp\n",
    "\n",
    "\n",
    "@_jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def MCP_concave_PCA(x, lambda_, gamma, pca_p):\n",
    "    '''\n",
    "    The value of concave part of MCP penalty; \n",
    "    #x can be a multi-dimensional tensor. \n",
    "    '''\n",
    "    # similiar as in MCP\n",
    "    x = _np.abs(x)\n",
    "    temp = _np.where(x <= gamma * lambda_, -(x**2) / (2 * gamma),\n",
    "                     (gamma * lambda_**2) / 2 - lambda_ * x)\n",
    "    temp[0:pca_p + 1] = 0.  # this is to NOT penalize intercept beta later\n",
    "    return temp\n",
    "\n",
    "\n",
    "@_jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def MCP_concave_grad_PCA(x, lambda_, gamma, pca_p):\n",
    "    '''\n",
    "    The gradient of concave part of MCP penalty wrt. input x; \n",
    "    #x can be a multi-dimensional tensor. \n",
    "    '''\n",
    "    temp = _np.where(\n",
    "        _np.abs(x) < gamma * lambda_, -x / gamma, -lambda_ * _np.sign(x))\n",
    "    temp[0:pca_p + 1] = 0.  # this is to NOT penalize intercept beta later\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-09T22:53:29.442367Z",
     "start_time": "2022-08-09T22:53:29.415517Z"
    }
   },
   "outputs": [],
   "source": [
    "# @_jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def _SNP_update_smooth_grad_convex_LM_PCA(N, SNP_ind, bed, beta_md, y,\n",
    "                                          outcome_iid, pca_p, pca):\n",
    "    '''\n",
    "    Update the gradient of the smooth convex objective component.\n",
    "    '''\n",
    "    p = len(list(bed.sid))\n",
    "    gene_iid = _np.array(list(bed.iid))\n",
    "    _y = y[_np.intersect1d(outcome_iid,\n",
    "                           gene_iid,\n",
    "                           assume_unique=True,\n",
    "                           return_indices=True)[1]]\n",
    "    gene_ind = _np.intersect1d(gene_iid,\n",
    "                               outcome_iid,\n",
    "                               assume_unique=True,\n",
    "                               return_indices=True)[1]\n",
    "    # first calcualte _=X@beta_md-y\n",
    "    _ = _np.zeros(N)\n",
    "    for j in SNP_ind:\n",
    "        _X = bed.read(_np.s_[:, j], dtype=_np.int8).flatten()\n",
    "        _X = _X[gene_ind]  # get gene iid also in outcome iid\n",
    "        _ += _X * beta_md[j + 1]  # +1 because intercept\n",
    "    _ += beta_md[0]  # add the intercept\n",
    "    _ += pca[gene_ind, :] @ beta_md[1:pca_p + 1]\n",
    "    _ -= _y\n",
    "    # then calculate _XTXbeta = X.T@X@beta_md = X.T@_\n",
    "    _XTXbeta = _np.zeros(p)\n",
    "    for j in SNP_ind:\n",
    "        _X = bed.read(_np.s_[:, j], dtype=_np.int8).flatten()\n",
    "        _X = _X[gene_ind]  # get gene iid also in outcome iid\n",
    "        _XTXbeta[j] = _X @ _\n",
    "    _XTXbeta = _np.hstack(\n",
    "        (_np.array([_np.sum(_)]), _ @ pca[gene_ind, :], _XTXbeta))\n",
    "    del _\n",
    "    return 1 / N * _XTXbeta\n",
    "\n",
    "\n",
    "# @_jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def _SNP_update_smooth_grad_SCAD_LM_PCA(N, SNP_ind, bed, beta_md, y,\n",
    "                                        outcome_iid, _lambda, a, pca_p, pca):\n",
    "    '''\n",
    "    Update the gradient of the smooth objective component for SCAD penalty.\n",
    "    '''\n",
    "    return _SNP_update_smooth_grad_convex_LM_PCA(\n",
    "        N=N,\n",
    "        SNP_ind=SNP_ind,\n",
    "        bed=bed,\n",
    "        beta_md=beta_md,\n",
    "        y=y,\n",
    "        outcome_iid=outcome_iid,\n",
    "        pca_p=pca_p,\n",
    "        pca=pca) + SCAD_concave_grad_PCA(\n",
    "            x=beta_md, lambda_=_lambda, a=a, pca_p=pca_p)\n",
    "\n",
    "\n",
    "# @_jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def _SNP_update_smooth_grad_MCP_LM_PCA(N, SNP_ind, bed, beta_md, y,\n",
    "                                       outcome_iid, _lambda, gamma, pca_p,\n",
    "                                       pca):\n",
    "    '''\n",
    "    Update the gradient of the smooth objective component for MCP penalty.\n",
    "    '''\n",
    "    return _SNP_update_smooth_grad_convex_LM_PCA(\n",
    "        N=N,\n",
    "        SNP_ind=SNP_ind,\n",
    "        bed=bed,\n",
    "        beta_md=beta_md,\n",
    "        y=y,\n",
    "        outcome_iid=outcome_iid,\n",
    "        pca_p=pca_p,\n",
    "        pca=pca) + MCP_concave_grad_PCA(\n",
    "            x=beta_md, lambda_=_lambda, gamma=gamma, pca_p=pca_p)\n",
    "\n",
    "\n",
    "# @_jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def _SNP_lambda_max_LM_PCA(bed, y, outcome_iid, N, SNP_ind):\n",
    "    \"\"\"\n",
    "    Calculate the lambda_max, i.e., the minimum lambda to nullify all penalized betas.\n",
    "    \"\"\"\n",
    "    #     X_temp = X.copy()\n",
    "    #     X_temp = X_temp[:,1:]\n",
    "    #     X_temp -= _np.mean(X_temp,0).reshape(1,-1)\n",
    "    #     X_temp /= _np.std(X_temp,0)\n",
    "    #     y_temp = y.copy()\n",
    "    #     y_temp -= _np.mean(y)\n",
    "    #     y_temp /= _np.std(y)\n",
    "    p = len(list(bed.sid))\n",
    "    grad_at_0 = _SNP_update_smooth_grad_convex_LM_PCA(N=N,\n",
    "                                                      SNP_ind=SNP_ind,\n",
    "                                                      bed=bed,\n",
    "                                                      beta_md=_np.zeros(p),\n",
    "                                                      y=y,\n",
    "                                                      outcome_iid=outcome_iid)\n",
    "    return _np.linalg.norm(grad_at_0[1:], ord=_np.infty)\n",
    "\n",
    "\n",
    "# @_jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def SNP_UAG_LM_SCAD_MCP_PCA(bed_file,\n",
    "                            bim_file,\n",
    "                            fam_file,\n",
    "                            outcome,\n",
    "                            outcome_iid,\n",
    "                            SNP_ind,\n",
    "                            L_convex,\n",
    "                            pca,\n",
    "                            beta_0=_np.ones(1),\n",
    "                            tol=1e-5,\n",
    "                            maxit=500,\n",
    "                            _lambda=.5,\n",
    "                            penalty=\"SCAD\",\n",
    "                            a=3.7,\n",
    "                            gamma=2.):\n",
    "    '''\n",
    "    Carry out the optimization for penalized LM for a fixed lambda.\n",
    "    '''\n",
    "    bed = _open_bed(filepath=bed_file,\n",
    "                    fam_filepath=fam_file,\n",
    "                    bim_filepath=bim_file)\n",
    "    pca_p = pca.shape[1]\n",
    "    y = outcome\n",
    "    p = bed.sid_count\n",
    "    gene_iid = _np.array(list(bed.iid))\n",
    "    N = len(\n",
    "        _np.intersect1d(outcome_iid,\n",
    "                        gene_iid,\n",
    "                        assume_unique=True,\n",
    "                        return_indices=True)[1])\n",
    "    if _np.all(beta_0 == _np.ones(1)):\n",
    "        _ = _np.zeros(p)\n",
    "        _y = y[_np.intersect1d(outcome_iid,\n",
    "                               gene_iid,\n",
    "                               assume_unique=True,\n",
    "                               return_indices=True)[1]]\n",
    "        _y -= _np.mean(_y)\n",
    "        for j in SNP_ind:\n",
    "            _X = bed.read(_np.s_[:, j], dtype=_np.float64).flatten()\n",
    "            _X = _X[gene_ind]  # get gene iid also in outcome iid\n",
    "            _X -= _np.mean(_X)\n",
    "            _[j] = _X @ _y / N / _np.var(_X)\n",
    "        beta = _  #_np.sign(_)\n",
    "        _pca = _y @ pca[gene_ind, :] / N\n",
    "        beta = _np.hstack((_np.array([_np.mean(_y)]), _pca, beta))\n",
    "    else:\n",
    "        beta = beta_0\n",
    "    # passing other parameters\n",
    "    smooth_grad = _np.ones(p + 1 + pca_p)\n",
    "    beta_ag = beta.copy()\n",
    "    beta_md = beta.copy()\n",
    "    k = 0\n",
    "    converged = False\n",
    "    opt_alpha = 1.\n",
    "    old_speed_norm = 1.\n",
    "    speed_norm = 1.\n",
    "    restart_k = 0\n",
    "\n",
    "    if penalty == \"SCAD\":\n",
    "        #         L = _np.max(_np.array([L_convex, 1./(a-1)]))\n",
    "        L = _np.linalg.norm(_np.array([L_convex, 1. / (a - 1)]), ord=_np.infty)\n",
    "        opt_beta = .99 / L\n",
    "        while ((not converged) or (k < 3)) and k <= maxit:\n",
    "            k += 1\n",
    "            if old_speed_norm > speed_norm and k - restart_k >= 3:  # in this case, restart\n",
    "                opt_alpha = 1.  # restarting\n",
    "                restart_k = k  # restarting\n",
    "            else:  # restarting\n",
    "                opt_alpha = 2 / (\n",
    "                    1 + (1 + 4. / opt_alpha**2)**.5\n",
    "                )  #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound\n",
    "            opt_lambda = opt_beta / opt_alpha  #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound\n",
    "            beta_md_old = beta_md.copy()  # restarting\n",
    "            beta_md = (1 - opt_alpha) * beta_ag + opt_alpha * beta\n",
    "            old_speed_norm = speed_norm  # restarting\n",
    "            speed_norm = _np.linalg.norm(beta_md - beta_md_old,\n",
    "                                         ord=2)  # restarting\n",
    "            converged = (_np.linalg.norm(beta_md - beta_md_old, ord=_np.infty)\n",
    "                         < tol)\n",
    "            smooth_grad = _SNP_update_smooth_grad_SCAD_LM_PCA(\n",
    "                N=N,\n",
    "                SNP_ind=SNP_ind,\n",
    "                bed=bed,\n",
    "                beta_md=beta_md,\n",
    "                y=y,\n",
    "                outcome_iid=outcome_iid,\n",
    "                _lambda=_lambda,\n",
    "                a=a,\n",
    "                pca_p=pca_p,\n",
    "                pca=pca)\n",
    "            beta = soft_thresholding_PCA(x=beta - opt_lambda * smooth_grad,\n",
    "                                         lambda_=opt_lambda * _lambda,\n",
    "                                         pca_p=pca_p)\n",
    "            beta_ag = soft_thresholding_PCA(x=beta_md - opt_beta * smooth_grad,\n",
    "                                            lambda_=opt_beta * _lambda,\n",
    "                                            pca_p=pca_p)\n",
    "#             converged = _np.all(_np.max(_np.abs(beta_md - beta_ag)/opt_beta) < tol).item()\n",
    "#             converged = (_np.linalg.norm(beta_md - beta_ag, ord=_np.infty) < (tol*opt_beta))\n",
    "    else:\n",
    "        #         L = _np.max(_np.array([L_convex, 1./(gamma)]))\n",
    "        L = _np.linalg.norm(_np.array([L_convex, 1. / (gamma)]), ord=_np.infty)\n",
    "        opt_beta = .99 / L\n",
    "        while ((not converged) or (k < 3)) and k <= maxit:\n",
    "            k += 1\n",
    "            if old_speed_norm > speed_norm and k - restart_k >= 3:  # in this case, restart\n",
    "                opt_alpha = 1.  # restarting\n",
    "                restart_k = k  # restarting\n",
    "            else:  # restarting\n",
    "                opt_alpha = 2 / (\n",
    "                    1 + (1 + 4. / opt_alpha**2)**.5\n",
    "                )  #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound\n",
    "            opt_lambda = opt_beta / opt_alpha  #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound\n",
    "            beta_md_old = beta_md.copy()  # restarting\n",
    "            beta_md = (1 - opt_alpha) * beta_ag + opt_alpha * beta\n",
    "            old_speed_norm = speed_norm  # restarting\n",
    "            speed_norm = _np.linalg.norm(beta_md - beta_md_old,\n",
    "                                         ord=2)  # restarting\n",
    "            converged = (_np.linalg.norm(beta_md - beta_md_old, ord=_np.infty)\n",
    "                         < tol)\n",
    "            smooth_grad = _SNP_update_smooth_grad_MCP_LM_PCA(\n",
    "                N=N,\n",
    "                SNP_ind=SNP_ind,\n",
    "                bed=bed,\n",
    "                beta_md=beta_md,\n",
    "                y=y,\n",
    "                outcome_iid=outcome_iid,\n",
    "                _lambda=_lambda,\n",
    "                gamma=gamma,\n",
    "                pca_p=pca_p,\n",
    "                pca=pca)\n",
    "            beta = soft_thresholding_PCA(x=beta - opt_lambda * smooth_grad,\n",
    "                                         lambda_=opt_lambda * _lambda,\n",
    "                                         pca_p=pca_p)\n",
    "            beta_ag = soft_thresholding_PCA(x=beta_md - opt_beta * smooth_grad,\n",
    "                                            lambda_=opt_beta * _lambda,\n",
    "                                            pca_p=pca_p)\n",
    "#             converged = _np.all(_np.max(_np.abs(beta_md - beta_ag)/opt_beta) < tol).item()\n",
    "#             converged = (_np.linalg.norm(beta_md - beta_ag, ord=_np.infty) < (tol*opt_beta))\n",
    "    return k, beta_md\n",
    "\n",
    "\n",
    "# @_jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def SNP_solution_path_LM_PCA(bed_file,\n",
    "                             bim_file,\n",
    "                             fam_file,\n",
    "                             outcome,\n",
    "                             outcome_iid,\n",
    "                             lambda_,\n",
    "                             L_convex,\n",
    "                             SNP_ind,\n",
    "                             pca,\n",
    "                             beta_0=_np.ones(1),\n",
    "                             tol=1e-5,\n",
    "                             maxit=500,\n",
    "                             penalty=\"SCAD\",\n",
    "                             a=3.7,\n",
    "                             gamma=2.):\n",
    "    '''\n",
    "    Carry out the optimization for the solution path without the strong rule.\n",
    "    '''\n",
    "    pca_p = pca.shape[1]\n",
    "    bed = _open_bed(filepath=bed_file,\n",
    "                    fam_filepath=fam_file,\n",
    "                    bim_filepath=bim_file)\n",
    "    p = bed.sid_count\n",
    "\n",
    "    y = outcome\n",
    "    gene_iid = _np.array(list(bed.iid))\n",
    "    gene_ind = _np.intersect1d(gene_iid,\n",
    "                               outcome_iid,\n",
    "                               assume_unique=True,\n",
    "                               return_indices=True)[1]\n",
    "    N = len(\n",
    "        _np.intersect1d(outcome_iid,\n",
    "                        gene_iid,\n",
    "                        assume_unique=True,\n",
    "                        return_indices=True)[1])\n",
    "    _ = _np.zeros(p)\n",
    "    _y = y[_np.intersect1d(outcome_iid,\n",
    "                           gene_iid,\n",
    "                           assume_unique=True,\n",
    "                           return_indices=True)[1]]\n",
    "    _y -= _np.mean(_y)\n",
    "    for j in SNP_ind:\n",
    "        _X = bed.read(_np.s_[:, j], dtype=_np.float64).flatten()\n",
    "        _X = _X[gene_ind]  # get gene iid also in outcome iid\n",
    "        _X -= _np.mean(_X)\n",
    "        _[j] = _X @ _y / N / _np.var(_X)\n",
    "    beta = _  #_np.sign(_)\n",
    "    _pca = _y @ pca[gene_ind, :] / N\n",
    "    beta = _np.hstack((_np.array([_np.mean(_y)]), _pca, beta)).reshape(1, -1)\n",
    "    beta_mat = _np.repeat(beta, len(lambda_) + 1, axis=0)\n",
    "    for j in range(len(lambda_)):\n",
    "        beta_mat[j + 1, :] = SNP_UAG_LM_SCAD_MCP_PCA(bed_file=bed_file,\n",
    "                                                     bim_file=bim_file,\n",
    "                                                     fam_file=fam_file,\n",
    "                                                     outcome=outcome,\n",
    "                                                     SNP_ind=SNP_ind,\n",
    "                                                     L_convex=L_convex,\n",
    "                                                     pca=pca,\n",
    "                                                     beta_0=beta_mat[j, :],\n",
    "                                                     tol=tol,\n",
    "                                                     maxit=maxit,\n",
    "                                                     _lambda=lambda_[j],\n",
    "                                                     penalty=penalty,\n",
    "                                                     outcome_iid=outcome_iid,\n",
    "                                                     a=a,\n",
    "                                                     gamma=gamma)[1]\n",
    "    return beta_mat[1:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-09T23:01:14.897117Z",
     "start_time": "2022-08-09T22:53:29.443583Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3)\n",
      "L_convex is: 9843.632966454592\n",
      "[1. 0. 1. ... 2. 1. 1.]\n",
      "[1. 2. 0. ... 0. 2. 1.]\n",
      "[2. 1. 2. ... 2. 2. 2.]\n",
      "[[ 4.62636142e+00 -2.31792193e-03 -1.13518432e-03  3.12527793e-03\n",
      "   1.29506923e+01  0.00000000e+00  8.34959389e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 4.75166053e+00 -4.00678913e-03 -2.52834614e-03  6.29903212e-03\n",
      "   1.99285327e+01  0.00000000e+00  1.26737084e+01  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 4.73534658e+00 -5.67378502e-03 -3.92785943e-03  9.47157813e-03\n",
      "   2.67692848e+01  0.00000000e+00  1.68523970e+01  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 4.73733458e+00 -7.34169083e-03 -5.32492291e-03  1.26420609e-02\n",
      "   3.36308025e+01  0.00000000e+00  2.10441063e+01  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 4.73695577e+00 -9.00756337e-03 -6.72067526e-03  1.58105920e-02\n",
      "   4.04926803e+01  0.00000000e+00  2.52283608e+01  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 4.73688156e+00 -1.06717843e-02 -8.11497046e-03  1.89771582e-02\n",
      "   4.73575524e+01  0.00000000e+00  2.94078106e+01  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 4.73676667e+00 -1.23343054e-02 -9.50782850e-03  2.21417624e-02\n",
      "   5.42250754e+01  0.00000000e+00  3.35821166e+01  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 4.73665573e+00 -1.39951343e-02 -1.08992480e-02  2.53044052e-02\n",
      "   6.10952905e+01  0.00000000e+00  3.77513264e+01  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 4.73654297e+00 -1.56542712e-02 -1.22892301e-02  2.84650878e-02\n",
      "   6.79681893e+01 -2.39886285e-01  4.19154373e+01  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 4.73642914e+00 -1.73117172e-02 -1.36777760e-02  3.16238111e-02\n",
      "   7.48437697e+01 -8.16933087e-01  4.60744532e+01  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 4.73631415e+00 -1.89674735e-02 -1.50648865e-02  3.47805762e-02\n",
      "   8.17220290e+01 -1.99585623e+00  5.02283772e+01  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 4.73619801e+00 -2.06215411e-02 -1.64505628e-02  3.79353840e-02\n",
      "   8.86029644e+01 -5.51583854e+00  5.43772124e+01  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 4.73608072e+00 -2.22739211e-02 -1.78348057e-02  4.10882356e-02\n",
      "   9.54865732e+01 -9.47998502e+00  5.85209620e+01  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 4.73596227e+00 -2.39246147e-02 -1.92176163e-02  4.42391319e-02\n",
      "   1.02372853e+02 -1.34419338e+01  6.26596292e+01  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 4.73584268e+00 -2.55736229e-02 -2.05989957e-02  4.73880740e-02\n",
      "   1.09261800e+02 -1.74016872e+01  6.67932171e+01  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 4.73572193e+00 -2.72209467e-02 -2.19789447e-02  5.05350628e-02\n",
      "   1.16153412e+02 -2.13592474e+01  7.09217288e+01  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 4.73560004e+00 -2.88665873e-02 -2.33574645e-02  5.36800993e-02\n",
      "   1.23047687e+02 -2.53146168e+01  7.50451675e+01  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 4.73547701e+00 -3.05105458e-02 -2.47345559e-02  5.68231846e-02\n",
      "   1.29944622e+02 -2.92677976e+01  7.91635363e+01  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 4.73535282e+00 -3.21528232e-02 -2.61102201e-02  5.99643197e-02\n",
      "   1.36844214e+02 -3.32187920e+01  8.32768384e+01  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 4.73522750e+00 -3.37934206e-02 -2.74844579e-02  6.31035054e-02\n",
      "   1.43746460e+02 -3.71676024e+01  8.73850770e+01  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from bed_reader import open_bed\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "bed_file = r\"./MI_AG/tests/sim/sim1.bed\"\n",
    "bim_file = r\"./MI_AG/tests/sim/sim1.bim\"\n",
    "fam_file = r\"./MI_AG/tests/sim/sim1.fam\"\n",
    "\n",
    "_bed = open_bed(filepath=bed_file,\n",
    "                fam_filepath=fam_file,\n",
    "                bim_filepath=bim_file)\n",
    "outcome = np.random.rand(_bed.iid_count)\n",
    "outcome_iid = _bed.iid\n",
    "true_beta = np.array([4.2, -2.5, 2.6])\n",
    "\n",
    "# here since the plink files are very small, I just use Python to calculate L_convex -- normally it should be calculated using other softwares, e.g., flashpca\n",
    "temp = np.zeros((_bed.iid_count, _bed.sid_count))\n",
    "for j in np.arange(_bed.sid_count):\n",
    "    temp[:, j] = _bed.read(_np.s_[:, j], dtype=_np.int8).flatten()\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit(temp)\n",
    "pca = pca.components_.T\n",
    "print(pca.shape)\n",
    "\n",
    "L_convex = 1 / _bed.iid_count * (_np.linalg.eigvalsh(temp @ temp.T)[-1])\n",
    "print(\"L_convex is:\", L_convex)\n",
    "\n",
    "for j in np.arange(3):\n",
    "    outcome += true_beta[j] * _bed.read(_np.s_[:, j], dtype=_np.int8).flatten()\n",
    "    print(_bed.read(_np.s_[:, j], dtype=_np.float64).flatten())\n",
    "\n",
    "iid_ind = np.random.permutation(np.arange(_bed.iid_count))\n",
    "\n",
    "outcome = outcome[iid_ind]\n",
    "outcome_iid = outcome_iid[iid_ind]\n",
    "\n",
    "print(\n",
    "    SNP_solution_path_LM_PCA(bed_file=bed_file,\n",
    "                             bim_file=bim_file,\n",
    "                             fam_file=fam_file,\n",
    "                             outcome=outcome,\n",
    "                             outcome_iid=outcome_iid,\n",
    "                             lambda_=np.linspace(.00001, 2, 20)[::-1],\n",
    "                             SNP_ind=np.arange(3),\n",
    "                             L_convex=L_convex,\n",
    "                             pca=pca,\n",
    "                             beta_0=np.ones(10),\n",
    "                             tol=1e-5,\n",
    "                             maxit=500,\n",
    "                             penalty=\"SCAD\",\n",
    "                             a=3.7,\n",
    "                             gamma=2.)[:, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "877.844px",
    "left": "2188px",
    "right": "20px",
    "top": "120px",
    "width": "352px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
