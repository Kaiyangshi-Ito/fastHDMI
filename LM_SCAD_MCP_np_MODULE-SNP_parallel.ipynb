{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the class fundementals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-23T19:24:54.684286Z",
     "start_time": "2022-07-23T19:24:54.309469Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Snoopy/anaconda3/lib/python3.9/site-packages/paramiko/transport.py:236: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    }
   ],
   "source": [
    "# import os, sys\n",
    "# import collections\n",
    "import numpy as _np\n",
    "# import matplotlib.markers as markers\n",
    "# import matplotlib.pyplot as plt\n",
    "# import timeit\n",
    "# import collections\n",
    "# from scipy.stats import median_abs_deviation as mad\n",
    "# import multiprocessing\n",
    "# import cProfile\n",
    "# import itertools\n",
    "from numba import jit as _jit\n",
    "from numba import njit as _njit\n",
    "from bed_reader import open_bed as _open_bed\n",
    "import multiprocess as _mp\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore') # this is just to hide all the warnings\n",
    "# import rpy2.robjects as robjects\n",
    "# import matplotlib.pyplot as plt # change font globally to Times \n",
    "# plt.style.use('ggplot')\n",
    "# plt.rcParams.update({\n",
    "#     \"text.usetex\": True,\n",
    "#     \"font.family\": \"Times New Roman\",\n",
    "#     \"font.sans-serif\": [\"Times New Roman\"],\n",
    "#     \"font.size\": 12})\n",
    "\n",
    "# os.chdir(sys.path[0]) # ensure working direcotry is set same as the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-23T19:24:54.849061Z",
     "start_time": "2022-07-23T19:24:54.686202Z"
    }
   },
   "outputs": [],
   "source": [
    "######################################  some SCAD and MCP things  #######################################\n",
    "@_jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def soft_thresholding(x, lambda_):\n",
    "    '''\n",
    "    To calculate soft-thresholding mapping of a given ONE-DIMENSIONAL tensor, BESIDES THE FIRST TERM (so beta_0 will not be penalized). \n",
    "    This function is to be used for calculation involving L1 penalty term later. \n",
    "    '''\n",
    "    return _np.hstack((_np.array([x[0]]), _np.where(_np.abs(x[1:])>lambda_, x[1:] - _np.sign(x[1:])*lambda_, 0)))\n",
    "\n",
    "soft_thresholding(_np.random.rand(20),3.1)\n",
    "\n",
    "@_jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def SCAD(x, lambda_, a=3.7):\n",
    "    '''\n",
    "    To calculate SCAD penalty value;\n",
    "    #x can be a multi-dimensional tensor;\n",
    "    lambda_, a are scalars;\n",
    "    Fan and Li suggests to take a as 3.7 \n",
    "    '''\n",
    "    # here I notice the function is de facto a function of absolute value of x, therefore take absolute value first to simplify calculation \n",
    "    x = _np.abs(x)\n",
    "    temp = _np.where(x<=lambda_, lambda_*x, _np.where(x<a*lambda_, (2*a*lambda_*x - x**2 - lambda_**2)/(2*(a - 1)), lambda_**2 * (a+1)/2))\n",
    "    temp[0] = 0. # this is to NOT penalize intercept beta later \n",
    "    return temp \n",
    "\n",
    "@_jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def SCAD_grad(x, lambda_, a=3.7):\n",
    "    '''\n",
    "    To calculate the gradient of SCAD wrt. input x; \n",
    "    #x can be a multi-dimensional tensor. \n",
    "    '''\n",
    "    # here decompose x to sign and its absolute value for easier calculation\n",
    "    sgn = _np.sign(x)\n",
    "    x = _np.abs(x)\n",
    "    temp = _np.where(x<=lambda_, lambda_*sgn, _np.where(x<a*lambda_, (a*lambda_*sgn-sgn*x)/(a-1), 0))\n",
    "    temp[0] = 0. # this is to NOT penalize intercept beta later \n",
    "    return temp \n",
    "\n",
    "@_jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def MCP(x, lambda_, gamma):\n",
    "    '''\n",
    "    To calculate MCP penalty value; \n",
    "    #x can be a multi-dimensional tensor. \n",
    "    '''\n",
    "    # the function is a function of absolute value of x \n",
    "    x = _np.abs(x)\n",
    "    temp = _np.where(x<=gamma*lambda_, lambda_*x - x**2/(2*gamma), .5*gamma*lambda_**2)\n",
    "    temp[0] = 0. # this is to NOT penalize intercept beta later \n",
    "    return temp \n",
    "\n",
    "@_jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def MCP_grad(x, lambda_, gamma):\n",
    "    '''\n",
    "    To calculate MCP gradient wrt. input x; \n",
    "    #x can be a multi-dimensional tensor. \n",
    "    '''\n",
    "    temp = _np.where(_np.abs(x)<gamma*lambda_, lambda_*_np.sign(x)-x/gamma, _np.zeros_like(x))\n",
    "    temp[0] = 0. # this is to NOT penalize intercept beta later \n",
    "    return temp \n",
    "\n",
    "@_jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def SCAD_concave(x, lambda_, a=3.7):\n",
    "    '''\n",
    "    The value of concave part of SCAD penalty; \n",
    "    #x can be a multi-dimensional tensor. \n",
    "    '''\n",
    "    x = _np.abs(x)\n",
    "    temp = _np.where(x<=lambda_, 0., _np.where(x<a*lambda_, (lambda_*x - (x**2 + lambda_**2)/2)/(a-1), (a+1)/2*lambda_**2 - lambda_*x))\n",
    "    temp[0] = 0. # this is to NOT penalize intercept beta later \n",
    "    return temp \n",
    "\n",
    "@_jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def SCAD_concave_grad(x, lambda_, a=3.7):\n",
    "    '''\n",
    "    The gradient of concave part of SCAD penalty wrt. input x; \n",
    "    #x can be a multi-dimensional tensor. \n",
    "    '''\n",
    "    sgn = _np.sign(x)\n",
    "    x = _np.abs(x)\n",
    "    temp = _np.where(x<=lambda_, 0., _np.where(x<a*lambda_, (lambda_*sgn-sgn*x)/(a-1), -lambda_*sgn))\n",
    "    temp[0] = 0. # this is to NOT penalize intercept beta later \n",
    "    return temp \n",
    "\n",
    "@_jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def MCP_concave(x, lambda_, gamma):\n",
    "    '''\n",
    "    The value of concave part of MCP penalty; \n",
    "    #x can be a multi-dimensional tensor. \n",
    "    '''\n",
    "    # similiar as in MCP\n",
    "    x = _np.abs(x)\n",
    "    temp = _np.where(x<=gamma*lambda_, -(x**2)/(2*gamma), (gamma*lambda_**2)/2 - lambda_*x)\n",
    "    temp[0] = 0. # this is to NOT penalize intercept beta later \n",
    "    return temp \n",
    "\n",
    "@_jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def MCP_concave_grad(x, lambda_, gamma):\n",
    "    '''\n",
    "    The gradient of concave part of MCP penalty wrt. input x; \n",
    "    #x can be a multi-dimensional tensor. \n",
    "    '''\n",
    "    temp = _np.where(_np.abs(x) < gamma*lambda_, -x/gamma, -lambda_*_np.sign(x))\n",
    "    temp[0] = 0. # this is to NOT penalize intercept beta later \n",
    "    return temp \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-23T19:24:54.872242Z",
     "start_time": "2022-07-23T19:24:54.850136Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# @_jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def _SNP_update_smooth_grad_convex_LM_parallel(N, SNP_ind, bed, beta_md, y, outcome_iid, chunck_size):\n",
    "    '''\n",
    "    Update the gradient of the smooth convex objective component.\n",
    "    '''\n",
    "    p=len(list(bed.sid))\n",
    "    gene_iid = _np.array(list(bed.iid))\n",
    "    _y = y[_np.intersect1d(outcome_iid, gene_iid, assume_unique=True, return_indices=True)[1]]\n",
    "    gene_ind = _np.intersect1d(gene_iid, outcome_iid, assume_unique=True, return_indices=True)[1]\n",
    "    # first calcualte _=X@beta_md-y\n",
    "    def __parallel_plus(_ind):\n",
    "        import numpy as _np\n",
    "        __ = _np.zeros(N)\n",
    "        for j in _ind:\n",
    "            _X = bed.read(_np.s_[:,j], dtype=_np.int8).flatten()\n",
    "            _X = _X[gene_ind] # get gene iid also in outcome iid\n",
    "            __ += _X*beta_md[j+1]\n",
    "        return __\n",
    "    # multiprocessing starts here\n",
    "    n_slices = _np.ceil(len(SNP_ind)/chunck_size)\n",
    "    with _mp.Pool(_mp.cpu_count()) as pl:\n",
    "        _ = pl.map(__parallel_plus, _np.array_split(SNP_ind, n_slices))\n",
    "    _ = _np.array(_).sum(0)\n",
    "    _ += beta_md[0] # add the intercept\n",
    "    _ -= _y\n",
    "    # then calculate _XTXbeta = X.T@X@beta_md = X.T@_\n",
    "    def __parallel_assign(_ind):\n",
    "        import numpy as _np\n",
    "        k=0\n",
    "        __ = _np.zeros(len(_ind))\n",
    "        for j in _ind:\n",
    "            _X = bed.read(_np.s_[:,j], dtype=_np.int8).flatten()\n",
    "            _X = _X[gene_ind] # get gene iid also in outcome iid\n",
    "            __[k] = _X@_\n",
    "            k += 1\n",
    "        return __\n",
    "    # multiprocessing starts here\n",
    "    n_slices = _np.ceil(len(SNP_ind)/chunck_size)\n",
    "    with _mp.Pool(_mp.cpu_count()) as pl:\n",
    "        _XTXbeta = pl.map(__parallel_assign, _np.array_split(SNP_ind, n_slices))\n",
    "    __XTXbeta = _np.hstack(_XTXbeta)\n",
    "    _XTXbeta = _np.zeros(p+1)\n",
    "    _XTXbeta[SNP_ind+1] = __XTXbeta\n",
    "    _XTXbeta[0] = _np.sum(_)\n",
    "    del _\n",
    "    del __XTXbeta\n",
    "\n",
    "    return 1/N*_XTXbeta\n",
    "\n",
    "# @_jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def _SNP_update_smooth_grad_SCAD_LM_parallel(N, SNP_ind, bed, beta_md, y, outcome_iid, _lambda, a, chunck_size):\n",
    "    '''\n",
    "    Update the gradient of the smooth objective component for SCAD penalty.\n",
    "    '''\n",
    "    return _SNP_update_smooth_grad_convex_LM_parallel(N=N, SNP_ind=SNP_ind, bed=bed, beta_md=beta_md, y=y, outcome_iid=outcome_iid, chunck_size=chunck_size) + SCAD_concave_grad(x=beta_md, lambda_=_lambda, a=a)\n",
    "\n",
    "# @_jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def _SNP_update_smooth_grad_MCP_LM_parallel(N, SNP_ind, bed, beta_md, y, outcome_iid, _lambda, gamma, chunck_size):\n",
    "    '''\n",
    "    Update the gradient of the smooth objective component for MCP penalty.\n",
    "    '''\n",
    "    return _SNP_update_smooth_grad_convex_LM_parallel(N=N, SNP_ind=SNP_ind, bed=bed, beta_md=beta_md, y=y, outcome_iid=outcome_iid, chunck_size=chunck_size) + MCP_concave_grad(x=beta_md, lambda_=_lambda, gamma=gamma)\n",
    "\n",
    "\n",
    "# @_jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def _SNP_lambda_max_LM_parallel(bed, y, outcome_iid, N, SNP_ind, chunck_size):\n",
    "    \"\"\"\n",
    "    Calculate the lambda_max, i.e., the minimum lambda to nullify all penalized betas.\n",
    "    \"\"\"\n",
    "#     X_temp = X.copy()\n",
    "#     X_temp = X_temp[:,1:]\n",
    "#     X_temp -= _np.mean(X_temp,0).reshape(1,-1)\n",
    "#     X_temp /= _np.std(X_temp,0)\n",
    "#     y_temp = y.copy()\n",
    "#     y_temp -= _np.mean(y)\n",
    "#     y_temp /= _np.std(y)\n",
    "    grad_at_0 = _SNP_update_smooth_grad_convex_LM_parallel(N=N, SNP_ind=SNP_ind, bed=bed, beta_md=_np.zeros(len(SNP_ind)), y=y, outcome_iid=outcome_iid, chunck_size=chunck_size)\n",
    "    return _np.linalg.norm(grad_at_0[1:], ord=_np.infty)\n",
    "\n",
    "\n",
    "\n",
    "# @_jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def SNP_UAG_LM_SCAD_MCP_parallel(bed_file, bim_file, fam_file, outcome, outcome_iid, SNP_ind, L_convex, beta_0 = _np.ones(1), tol=1e-5, maxit=500, _lambda=.5, penalty=\"SCAD\", a=3.7, gamma=2., chunck_size=50000):\n",
    "    '''\n",
    "    Carry out the optimization for penalized LM for a fixed lambda.\n",
    "    '''\n",
    "    bed = _open_bed(filepath=bed_file, fam_filepath=fam_file, bim_filepath=bim_file)\n",
    "    y = outcome\n",
    "    p = bed.sid_count\n",
    "    gene_iid = _np.array(list(bed.iid))\n",
    "    N = len(_np.intersect1d(outcome_iid, gene_iid, assume_unique=True, return_indices=True)[1])\n",
    "    if _np.all(beta_0==_np.ones(1)):        \n",
    "        _y = y[_np.intersect1d(outcome_iid, gene_iid, assume_unique=True, return_indices=True)[1]]\n",
    "        _y -= _np.mean(_y)\n",
    "        def __parallel_assign(_ind):\n",
    "            import numpy as _np\n",
    "            k=0\n",
    "            __ = _np.zeros(len(_ind))\n",
    "            for j in _ind:\n",
    "                _X = bed.read(_np.s_[:,j], dtype=_np.float64).flatten()\n",
    "                _X = _X[gene_ind] # get gene iid also in outcome iid\n",
    "                _X -= _np.mean(_X)\n",
    "                __[k] = _X@_y/N\n",
    "                k += 1\n",
    "            return __\n",
    "        # multiprocessing starts here\n",
    "        n_slices = _np.ceil(len(SNP_ind)/chunck_size)\n",
    "        with _mp.Pool(_mp.cpu_count()) as pl:\n",
    "            _XTy = pl.map(__parallel_assign, _np.array_split(SNP_ind, n_slices))\n",
    "        _XTy = _np.hstack(_XTy)\n",
    "        beta = _np.zeros(p+1)\n",
    "        beta[SNP_ind+1] = _#_np.sign(_XTy)\n",
    "    else:\n",
    "        beta = beta_0\n",
    "    # passing other parameters\n",
    "    smooth_grad = _np.ones(p+1)\n",
    "    beta_ag = beta.copy()\n",
    "    beta_md = beta.copy()\n",
    "    k = 0\n",
    "    converged = False\n",
    "    opt_alpha = 1.\n",
    "    old_speed_norm = 1.\n",
    "    speed_norm = 1.\n",
    "    restart_k = 0\n",
    "    \n",
    "    if penalty == \"SCAD\":\n",
    "#         L = _np.max(_np.array([L_convex, 1./(a-1)]))\n",
    "        L = _np.linalg.norm(_np.array([L_convex, 1./(a-1)]), ord=_np.infty)\n",
    "        opt_beta = .99/L\n",
    "        while ((not converged) or (k<3)) and k <= maxit:\n",
    "            k += 1\n",
    "            if old_speed_norm > speed_norm and k - restart_k>=3: # in this case, restart\n",
    "                opt_alpha = 1. # restarting\n",
    "                restart_k = k # restarting\n",
    "            else: # restarting\n",
    "                opt_alpha = 2/(1+(1+4./opt_alpha**2)**.5) #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound \n",
    "            opt_lambda = opt_beta/opt_alpha #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound\n",
    "            beta_md_old = beta_md.copy() # restarting\n",
    "            beta_md = (1-opt_alpha)*beta_ag + opt_alpha*beta\n",
    "            old_speed_norm = speed_norm # restarting\n",
    "            speed_norm = _np.linalg.norm(beta_md - beta_md_old, ord=2) # restarting\n",
    "            converged = (_np.linalg.norm(beta_md - beta_md_old, ord=_np.infty) < tol)\n",
    "            smooth_grad = _SNP_update_smooth_grad_SCAD_LM_parallel(N=N, SNP_ind=SNP_ind, bed=bed, beta_md=beta_md, y=y, outcome_iid=outcome_iid, _lambda=_lambda, a=a, chunck_size=chunck_size)\n",
    "            beta = soft_thresholding(x=beta - opt_lambda*smooth_grad, lambda_=opt_lambda*_lambda)\n",
    "            beta_ag = soft_thresholding(x=beta_md - opt_beta*smooth_grad, lambda_=opt_beta*_lambda)\n",
    "#             converged = _np.all(_np.max(_np.abs(beta_md - beta_ag)/opt_beta) < tol).item()\n",
    "#             converged = (_np.linalg.norm(beta_md - beta_ag, ord=_np.infty) < (tol*opt_beta))\n",
    "    else:\n",
    "#         L = _np.max(_np.array([L_convex, 1./(gamma)]))\n",
    "        L = _np.linalg.norm(_np.array([L_convex, 1./(gamma)]), ord=_np.infty)\n",
    "        opt_beta = .99/L\n",
    "        while ((not converged) or (k<3)) and k <= maxit:\n",
    "            k += 1\n",
    "            if old_speed_norm > speed_norm and k - restart_k>=3: # in this case, restart\n",
    "                opt_alpha = 1. # restarting\n",
    "                restart_k = k # restarting\n",
    "            else: # restarting\n",
    "                opt_alpha = 2/(1+(1+4./opt_alpha**2)**.5) #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound \n",
    "            opt_lambda = opt_beta/opt_alpha #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound\n",
    "            beta_md_old = beta_md.copy() # restarting\n",
    "            beta_md = (1-opt_alpha)*beta_ag + opt_alpha*beta\n",
    "            old_speed_norm = speed_norm # restarting\n",
    "            speed_norm = _np.linalg.norm(beta_md - beta_md_old, ord=2) # restarting\n",
    "            converged = (_np.linalg.norm(beta_md - beta_md_old, ord=_np.infty) < tol)\n",
    "            smooth_grad = _SNP_update_smooth_grad_MCP_LM_parallel(N=N, SNP_ind=SNP_ind, bed=bed, beta_md=beta_md, y=y, outcome_iid=outcome_iid, _lambda=_lambda, gamma=gamma, chunck_size=chunck_size)\n",
    "            beta = soft_thresholding(x=beta - opt_lambda*smooth_grad, lambda_=opt_lambda*_lambda)\n",
    "            beta_ag = soft_thresholding(x=beta_md - opt_beta*smooth_grad, lambda_=opt_beta*_lambda)\n",
    "#             converged = _np.all(_np.max(_np.abs(beta_md - beta_ag)/opt_beta) < tol).item()\n",
    "#             converged = (_np.linalg.norm(beta_md - beta_ag, ord=_np.infty) < (tol*opt_beta))\n",
    "    return k, beta_md\n",
    "\n",
    "\n",
    "# @_jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def SNP_solution_path_LM_parallel(bed_file, bim_file, fam_file, outcome, outcome_iid, lambda_, L_convex, SNP_ind, beta_0 = _np.ones(1), tol=1e-5, maxit=500, penalty=\"SCAD\", a=3.7, gamma=2., chunck_size=50000):\n",
    "    '''\n",
    "    Carry out the optimization for the solution path without the strong rule.\n",
    "    '''\n",
    "    bed = _open_bed(filepath=bed_file, fam_filepath=fam_file, bim_filepath=bim_file)\n",
    "    y = outcome\n",
    "    p = bed.sid_count\n",
    "    gene_iid = _np.array(list(bed.iid))\n",
    "    gene_ind = _np.intersect1d(gene_iid, outcome_iid, assume_unique=True, return_indices=True)[1]\n",
    "    N = len(_np.intersect1d(outcome_iid, gene_iid, assume_unique=True, return_indices=True)[1])\n",
    "    _y = y[_np.intersect1d(outcome_iid, gene_iid, assume_unique=True, return_indices=True)[1]]\n",
    "    _y -= _np.mean(_y)\n",
    "    def __parallel_assign(_ind):\n",
    "        import numpy as _np\n",
    "        k=0\n",
    "        __ = _np.zeros(len(_ind))\n",
    "        for j in _ind:\n",
    "            _X = bed.read(_np.s_[:,j], dtype=_np.float64).flatten()\n",
    "            _X = _X[gene_ind] # get gene iid also in outcome iid\n",
    "            _X -= _np.mean(_X)\n",
    "            __[k] = _X@_y/N\n",
    "            k += 1\n",
    "        return __\n",
    "    # multiprocessing starts here\n",
    "    n_slices = _np.ceil(len(SNP_ind)/chunck_size)\n",
    "    with _mp.Pool(_mp.cpu_count()) as pl:\n",
    "        _XTy = pl.map(__parallel_assign, _np.array_split(SNP_ind, n_slices))\n",
    "    _XTy = _np.hstack(_XTy)\n",
    "    beta = _np.zeros(p+1)\n",
    "    beta[SNP_ind+1] = _XTy#_np.sign(_XTy)\n",
    "    beta =beta.reshape(1,-1)\n",
    "    \n",
    "    beta_mat = _np.zeros((len(lambda_)+1, p+1))\n",
    "    beta_mat = _np.repeat(beta, len(lambda_)+1, axis=0)\n",
    "    for j in range(len(lambda_)): \n",
    "        beta_mat[j+1,:] = SNP_UAG_LM_SCAD_MCP_parallel(bed_file=bed_file, bim_file=bim_file, fam_file=fam_file, outcome=outcome, SNP_ind=SNP_ind, L_convex=L_convex, beta_0 = beta_mat[j,:], tol=tol, maxit=maxit, _lambda=lambda_[j], penalty=penalty, outcome_iid=outcome_iid, a=a, gamma=gamma, chunck_size=chunck_size)[1]\n",
    "    return beta_mat[1:,:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-23T19:28:02.587290Z",
     "start_time": "2022-07-23T19:24:54.873584Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L_convex is: 9843.632966454592\n",
      "[1. 0. 1. ... 2. 1. 1.]\n",
      "[1. 2. 0. ... 0. 2. 1.]\n",
      "[2. 1. 2. ... 2. 2. 2.]\n",
      "(5000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-15457:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/Snoopy/anaconda3/lib/python3.9/site-packages/multiprocess/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/Snoopy/anaconda3/lib/python3.9/site-packages/multiprocess/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/Snoopy/anaconda3/lib/python3.9/site-packages/multiprocess/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/Snoopy/anaconda3/lib/python3.9/site-packages/multiprocess/queues.py\", line 367, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/Snoopy/anaconda3/lib/python3.9/site-packages/multiprocess/synchronize.py\", line 101, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m outcome \u001b[38;5;241m=\u001b[39m outcome[iid_ind]\n\u001b[1;32m     29\u001b[0m outcome_iid \u001b[38;5;241m=\u001b[39m outcome_iid[iid_ind]\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mSNP_solution_path_LM_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbed_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbed_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mbim_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbim_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mfam_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfam_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43moutcome\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutcome\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43moutcome_iid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutcome_iid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mlambda_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinspace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m.00001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mSNP_ind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mL_convex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mL_convex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mbeta_0\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSCAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3.7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2.\u001b[39;49m\u001b[43m)\u001b[49m[:,:\u001b[38;5;241m10\u001b[39m])\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mSNP_solution_path_LM_parallel\u001b[0;34m(bed_file, bim_file, fam_file, outcome, outcome_iid, lambda_, L_convex, SNP_ind, beta_0, tol, maxit, penalty, a, gamma, chunck_size)\u001b[0m\n\u001b[1;32m    207\u001b[0m beta_mat \u001b[38;5;241m=\u001b[39m _np\u001b[38;5;241m.\u001b[39mrepeat(beta, \u001b[38;5;28mlen\u001b[39m(lambda_)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(lambda_)): \n\u001b[0;32m--> 209\u001b[0m     beta_mat[j\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,:] \u001b[38;5;241m=\u001b[39m \u001b[43mSNP_UAG_LM_SCAD_MCP_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbed_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbed_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbim_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbim_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfam_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfam_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutcome\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutcome\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSNP_ind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSNP_ind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mL_convex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mL_convex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta_0\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta_mat\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_lambda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlambda_\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutcome_iid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutcome_iid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunck_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunck_size\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m beta_mat[\u001b[38;5;241m1\u001b[39m:,:]\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mSNP_UAG_LM_SCAD_MCP_parallel\u001b[0;34m(bed_file, bim_file, fam_file, outcome, outcome_iid, SNP_ind, L_convex, beta_0, tol, maxit, _lambda, penalty, a, gamma, chunck_size)\u001b[0m\n\u001b[1;32m    141\u001b[0m speed_norm \u001b[38;5;241m=\u001b[39m _np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(beta_md \u001b[38;5;241m-\u001b[39m beta_md_old, \u001b[38;5;28mord\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# restarting\u001b[39;00m\n\u001b[1;32m    142\u001b[0m converged \u001b[38;5;241m=\u001b[39m (_np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(beta_md \u001b[38;5;241m-\u001b[39m beta_md_old, \u001b[38;5;28mord\u001b[39m\u001b[38;5;241m=\u001b[39m_np\u001b[38;5;241m.\u001b[39minfty) \u001b[38;5;241m<\u001b[39m tol)\n\u001b[0;32m--> 143\u001b[0m smooth_grad \u001b[38;5;241m=\u001b[39m \u001b[43m_SNP_update_smooth_grad_SCAD_LM_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSNP_ind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSNP_ind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta_md\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta_md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutcome_iid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutcome_iid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_lambda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_lambda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunck_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunck_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m beta \u001b[38;5;241m=\u001b[39m soft_thresholding(x\u001b[38;5;241m=\u001b[39mbeta \u001b[38;5;241m-\u001b[39m opt_lambda\u001b[38;5;241m*\u001b[39msmooth_grad, lambda_\u001b[38;5;241m=\u001b[39mopt_lambda\u001b[38;5;241m*\u001b[39m_lambda)\n\u001b[1;32m    145\u001b[0m beta_ag \u001b[38;5;241m=\u001b[39m soft_thresholding(x\u001b[38;5;241m=\u001b[39mbeta_md \u001b[38;5;241m-\u001b[39m opt_beta\u001b[38;5;241m*\u001b[39msmooth_grad, lambda_\u001b[38;5;241m=\u001b[39mopt_beta\u001b[38;5;241m*\u001b[39m_lambda)\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m_SNP_update_smooth_grad_SCAD_LM_parallel\u001b[0;34m(N, SNP_ind, bed, beta_md, y, outcome_iid, _lambda, a, chunck_size)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_SNP_update_smooth_grad_SCAD_LM_parallel\u001b[39m(N, SNP_ind, bed, beta_md, y, outcome_iid, _lambda, a, chunck_size):\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m    Update the gradient of the smooth objective component for SCAD penalty.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_SNP_update_smooth_grad_convex_LM_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSNP_ind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSNP_ind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta_md\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta_md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutcome_iid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutcome_iid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunck_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunck_size\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m SCAD_concave_grad(x\u001b[38;5;241m=\u001b[39mbeta_md, lambda_\u001b[38;5;241m=\u001b[39m_lambda, a\u001b[38;5;241m=\u001b[39ma)\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m_SNP_update_smooth_grad_convex_LM_parallel\u001b[0;34m(N, SNP_ind, bed, beta_md, y, outcome_iid, chunck_size)\u001b[0m\n\u001b[1;32m     38\u001b[0m n_slices \u001b[38;5;241m=\u001b[39m _np\u001b[38;5;241m.\u001b[39mceil(\u001b[38;5;28mlen\u001b[39m(SNP_ind)\u001b[38;5;241m/\u001b[39mchunck_size)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _mp\u001b[38;5;241m.\u001b[39mPool(_mp\u001b[38;5;241m.\u001b[39mcpu_count()) \u001b[38;5;28;01mas\u001b[39;00m pl:\n\u001b[0;32m---> 40\u001b[0m     _XTXbeta \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mmap(__parallel_assign, _np\u001b[38;5;241m.\u001b[39marray_split(SNP_ind, n_slices))\n\u001b[1;32m     41\u001b[0m __XTXbeta \u001b[38;5;241m=\u001b[39m _np\u001b[38;5;241m.\u001b[39mhstack(_XTXbeta)\n\u001b[1;32m     42\u001b[0m _XTXbeta \u001b[38;5;241m=\u001b[39m _np\u001b[38;5;241m.\u001b[39mzeros(p\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/multiprocess/pool.py:736\u001b[0m, in \u001b[0;36mPool.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[0;32m--> 736\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mterminate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/multiprocess/pool.py:654\u001b[0m, in \u001b[0;36mPool.terminate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    652\u001b[0m util\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mterminating pool\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m=\u001b[39m TERMINATE\n\u001b[0;32m--> 654\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_terminate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/multiprocess/util.py:224\u001b[0m, in \u001b[0;36mFinalize.__call__\u001b[0;34m(self, wr, _finalizer_registry, sub_debug, getpid)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    222\u001b[0m     sub_debug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinalizer calling \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m with args \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m and kwargs \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    223\u001b[0m               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kwargs)\n\u001b[0;32m--> 224\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_callback\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_weakref \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m    226\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/multiprocess/pool.py:692\u001b[0m, in \u001b[0;36mPool._terminate_pool\u001b[0;34m(cls, taskqueue, inqueue, outqueue, pool, change_notifier, worker_handler, task_handler, result_handler, cache)\u001b[0m\n\u001b[1;32m    689\u001b[0m task_handler\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m=\u001b[39m TERMINATE\n\u001b[1;32m    691\u001b[0m util\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhelping task handler/workers to finish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 692\u001b[0m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_help_stuff_finish\u001b[49m\u001b[43m(\u001b[49m\u001b[43minqueue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_handler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpool\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m result_handler\u001b[38;5;241m.\u001b[39mis_alive()) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(cache) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    695\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    696\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot have cache with result_hander not alive\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/multiprocess/pool.py:672\u001b[0m, in \u001b[0;36mPool._help_stuff_finish\u001b[0;34m(inqueue, task_handler, size)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_help_stuff_finish\u001b[39m(inqueue, task_handler, size):\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;66;03m# task_handler may be blocked trying to put items on inqueue\u001b[39;00m\n\u001b[1;32m    671\u001b[0m     util\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mremoving tasks from inqueue until task handler finished\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 672\u001b[0m     \u001b[43minqueue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    673\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m task_handler\u001b[38;5;241m.\u001b[39mis_alive() \u001b[38;5;129;01mand\u001b[39;00m inqueue\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mpoll():\n\u001b[1;32m    674\u001b[0m         inqueue\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mrecv()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from bed_reader import open_bed\n",
    "\n",
    "bed_file = r\"./sim/sim1.bed\"\n",
    "bim_file = r\"./sim/sim1.bim\"\n",
    "fam_file = r\"./sim/sim1.fam\"\n",
    "\n",
    "_bed = open_bed(filepath=bed_file, fam_filepath=fam_file, bim_filepath=bim_file)\n",
    "outcome = np.random.rand(_bed.iid_count)\n",
    "outcome_iid = _bed.iid\n",
    "true_beta = np.array([4.2,-2.5,2.6])\n",
    "\n",
    "# here since the plink files are very small, I just use Python to calculate L_convex -- normally it should be calculated using other softwares, e.g., flashpca \n",
    "temp = np.zeros((_bed.iid_count, _bed.sid_count))\n",
    "for j in np.arange(_bed.sid_count):\n",
    "    temp[:,j] = _bed.read(_np.s_[:,j], dtype=_np.int8).flatten()\n",
    "L_convex = 1/_bed.iid_count*(_np.linalg.eigvalsh(temp@temp.T)[-1])\n",
    "print(\"L_convex is:\", L_convex)\n",
    "\n",
    "for j in np.arange(3):\n",
    "    outcome += true_beta[j]*_bed.read(_np.s_[:,j], dtype=_np.int8).flatten()\n",
    "    print(_bed.read(_np.s_[:,j], dtype=_np.float64).flatten())\n",
    "\n",
    "iid_ind = np.random.permutation(np.arange(_bed.iid_count))\n",
    "\n",
    "outcome = outcome[iid_ind]\n",
    "outcome_iid = outcome_iid[iid_ind]\n",
    "\n",
    "print(SNP_solution_path_LM_parallel(bed_file=bed_file, \n",
    "                                    bim_file=bim_file, \n",
    "                                    fam_file=fam_file, \n",
    "                                    outcome=outcome, \n",
    "                                    outcome_iid=outcome_iid, \n",
    "                                    lambda_=np.linspace(.00001,2,20)[::-1],\n",
    "                                    SNP_ind=np.arange(9), \n",
    "                                    L_convex=L_convex, \n",
    "                                    beta_0 = np.ones(10),\n",
    "                                    tol=1e-5, maxit=500, penalty=\"SCAD\", a=3.7, gamma=2., chunck_size=1000)[:,:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "877.844px",
    "left": "2188px",
    "right": "20px",
    "top": "120px",
    "width": "352px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
