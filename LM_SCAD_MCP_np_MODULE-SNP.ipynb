{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the class fundementals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T02:52:22.556845Z",
     "start_time": "2022-07-18T02:52:22.060275Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bed_reader'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# import matplotlib.markers as markers\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# import matplotlib.pyplot as plt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# import timeit\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# import cProfile\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# import itertools\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m jit, njit\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbed_reader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m open_bed\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bed_reader'"
     ]
    }
   ],
   "source": [
    "# import os, sys\n",
    "# import collections\n",
    "import numpy as np\n",
    "# import matplotlib.markers as markers\n",
    "# import matplotlib.pyplot as plt\n",
    "# import timeit\n",
    "# import collections\n",
    "# from scipy.stats import median_abs_deviation as mad\n",
    "# import multiprocessing\n",
    "# import cProfile\n",
    "# import itertools\n",
    "from numba import jit, njit\n",
    "from bed_reader import open_bed\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore') # this is just to hide all the warnings\n",
    "# import rpy2.robjects as robjects\n",
    "# import matplotlib.pyplot as plt # change font globally to Times \n",
    "# plt.style.use('ggplot')\n",
    "# plt.rcParams.update({\n",
    "#     \"text.usetex\": True,\n",
    "#     \"font.family\": \"Times New Roman\",\n",
    "#     \"font.sans-serif\": [\"Times New Roman\"],\n",
    "#     \"font.size\": 12})\n",
    "\n",
    "# os.chdir(sys.path[0]) # ensure working direcotry is set same as the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T17:49:32.491623Z",
     "start_time": "2022-07-15T17:49:32.229624Z"
    }
   },
   "outputs": [],
   "source": [
    "opy1######################################  some SCAD and MCP things  #######################################\n",
    "@jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def soft_thresholding(x, lambda_):\n",
    "    '''\n",
    "    To calculate soft-thresholding mapping of a given ONE-DIMENSIONAL tensor, BESIDES THE FIRST TERM (so beta_0 will not be penalized). \n",
    "    This function is to be used for calculation involving L1 penalty term later. \n",
    "    '''\n",
    "    return np.hstack((np.array([x[0]]), np.where(np.abs(x[1:])>lambda_, x[1:] - np.sign(x[1:])*lambda_, 0)))\n",
    "\n",
    "soft_thresholding(np.random.rand(20),3.1)\n",
    "\n",
    "@jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def SCAD(x, lambda_, a=3.7):\n",
    "    '''\n",
    "    To calculate SCAD penalty value;\n",
    "    #x can be a multi-dimensional tensor;\n",
    "    lambda_, a are scalars;\n",
    "    Fan and Li suggests to take a as 3.7 \n",
    "    '''\n",
    "    # here I notice the function is de facto a function of absolute value of x, therefore take absolute value first to simplify calculation \n",
    "    x = np.abs(x)\n",
    "    temp = np.where(x<=lambda_, lambda_*x, np.where(x<a*lambda_, (2*a*lambda_*x - x**2 - lambda_**2)/(2*(a - 1)), lambda_**2 * (a+1)/2))\n",
    "    temp[0] = 0. # this is to NOT penalize intercept beta later \n",
    "    return temp \n",
    "\n",
    "@jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def SCAD_grad(x, lambda_, a=3.7):\n",
    "    '''\n",
    "    To calculate the gradient of SCAD wrt. input x; \n",
    "    #x can be a multi-dimensional tensor. \n",
    "    '''\n",
    "    # here decompose x to sign and its absolute value for easier calculation\n",
    "    sgn = np.sign(x)\n",
    "    x = np.abs(x)\n",
    "    temp = np.where(x<=lambda_, lambda_*sgn, np.where(x<a*lambda_, (a*lambda_*sgn-sgn*x)/(a-1), 0))\n",
    "    temp[0] = 0. # this is to NOT penalize intercept beta later \n",
    "    return temp \n",
    "\n",
    "@jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def MCP(x, lambda_, gamma):\n",
    "    '''\n",
    "    To calculate MCP penalty value; \n",
    "    #x can be a multi-dimensional tensor. \n",
    "    '''\n",
    "    # the function is a function of absolute value of x \n",
    "    x = np.abs(x)\n",
    "    temp = np.where(x<=gamma*lambda_, lambda_*x - x**2/(2*gamma), .5*gamma*lambda_**2)\n",
    "    temp[0] = 0. # this is to NOT penalize intercept beta later \n",
    "    return temp \n",
    "\n",
    "@jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def MCP_grad(x, lambda_, gamma):\n",
    "    '''\n",
    "    To calculate MCP gradient wrt. input x; \n",
    "    #x can be a multi-dimensional tensor. \n",
    "    '''\n",
    "    temp = np.where(np.abs(x)<gamma*lambda_, lambda_*np.sign(x)-x/gamma, np.zeros_like(x))\n",
    "    temp[0] = 0. # this is to NOT penalize intercept beta later \n",
    "    return temp \n",
    "\n",
    "@jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def SCAD_concave(x, lambda_, a=3.7):\n",
    "    '''\n",
    "    The value of concave part of SCAD penalty; \n",
    "    #x can be a multi-dimensional tensor. \n",
    "    '''\n",
    "    x = np.abs(x)\n",
    "    temp = np.where(x<=lambda_, 0., np.where(x<a*lambda_, (lambda_*x - (x**2 + lambda_**2)/2)/(a-1), (a+1)/2*lambda_**2 - lambda_*x))\n",
    "    temp[0] = 0. # this is to NOT penalize intercept beta later \n",
    "    return temp \n",
    "\n",
    "@jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def SCAD_concave_grad(x, lambda_, a=3.7):\n",
    "    '''\n",
    "    The gradient of concave part of SCAD penalty wrt. input x; \n",
    "    #x can be a multi-dimensional tensor. \n",
    "    '''\n",
    "    sgn = np.sign(x)\n",
    "    x = np.abs(x)\n",
    "    temp = np.where(x<=lambda_, 0., np.where(x<a*lambda_, (lambda_*sgn-sgn*x)/(a-1), -lambda_*sgn))\n",
    "    temp[0] = 0. # this is to NOT penalize intercept beta later \n",
    "    return temp \n",
    "\n",
    "@jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def MCP_concave(x, lambda_, gamma):\n",
    "    '''\n",
    "    The value of concave part of MCP penalty; \n",
    "    #x can be a multi-dimensional tensor. \n",
    "    '''\n",
    "    # similiar as in MCP\n",
    "    x = np.abs(x)\n",
    "    temp = np.where(x<=gamma*lambda_, -(x**2)/(2*gamma), (gamma*lambda_**2)/2 - lambda_*x)\n",
    "    temp[0] = 0. # this is to NOT penalize intercept beta later \n",
    "    return temp \n",
    "\n",
    "@jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def MCP_concave_grad(x, lambda_, gamma):\n",
    "    '''\n",
    "    The gradient of concave part of MCP penalty wrt. input x; \n",
    "    #x can be a multi-dimensional tensor. \n",
    "    '''\n",
    "    temp = np.where(np.abs(x) < gamma*lambda_, -x/gamma, -lambda_*np.sign(x))\n",
    "    temp[0] = 0. # this is to NOT penalize intercept beta later \n",
    "    return temp \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T17:49:32.585732Z",
     "start_time": "2022-07-15T17:49:32.493626Z"
    }
   },
   "outputs": [],
   "source": [
    "# @jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def _SNP_update_smooth_grad_convex_LM(N, p, bed, beta_md, y, _dtype, _order):\n",
    "    '''\n",
    "    Update the gradient of the smooth convex objective component.\n",
    "    '''\n",
    "    _itemsize = np.dtype(_dtype).itemsize\n",
    "    # first calcualte _=X@beta_md-y\n",
    "    _ = np.zeros(N)\n",
    "    if _order == \"F\":\n",
    "        for j in np.arange(p):\n",
    "            _X = np.memmap(bed, dtype=_dtype, mode='r', offset=j*_itemsize*N, shape = (N,))\n",
    "            _ += _X*beta_md[j]\n",
    "    elif _order == \"C\":\n",
    "        for j in np.arange(N):\n",
    "            _X = np.memmap(bed, dtype=_dtype, mode='r', offset=j*_itemsize*p, shape = (p,))\n",
    "            _[j] = _X@beta_md\n",
    "    _ -= y\n",
    "    # then calculate _XTXbeta = X.T@X@beta_md = X.T@_\n",
    "    _XTXbeta = np.zeros(p)\n",
    "    if _order == \"F\":\n",
    "        for j in np.arange(p):\n",
    "            _X = np.memmap(bed, dtype=_dtype, mode='r', offset=j*_itemsize*N, shape = (N,))\n",
    "            _XTXbeta[j] = _X@_\n",
    "    elif _order == \"C\":\n",
    "        for j in np.arange(N):\n",
    "            _X = np.memmap(bed, dtype=_dtype, mode='r', offset=j*_itemsize*p, shape = (p,))\n",
    "            _XTXbeta += _X*_[j]\n",
    "    del _\n",
    "    return 1/N*_XTXbeta\n",
    "\n",
    "# @jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def _SNP_update_smooth_grad_SCAD_LM(N, p, bed, beta_md, y, _lambda, a, _dtype, _order):\n",
    "    '''\n",
    "    Update the gradient of the smooth objective component for SCAD penalty.\n",
    "    '''\n",
    "    return _SNP_update_smooth_grad_convex_LM(N=N, p=p, X=bed, beta_md=beta_md, y=y, _dtype=_dtype, _order=_order) + SCAD_concave_grad(x=beta_md, lambda_=_lambda, a=a)\n",
    "\n",
    "# @jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def _SNP_update_smooth_grad_MCP_LM(N, p, bed, beta_md, y, _lambda, gamma, _dtype, _order):\n",
    "    '''\n",
    "    Update the gradient of the smooth objective component for MCP penalty.\n",
    "    '''\n",
    "    return _SNP_update_smooth_grad_convex_LM(N=N, p=p, X=bed, beta_md=beta_md, y=y, _dtype=_dtype, _order=_order) + MCP_concave_grad(x=beta_md, lambda_=_lambda, gamma=gamma)\n",
    "\n",
    "\n",
    "# @jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def _SNP_lambda_max_LM(bed, y, N, p, _dtype, _order):\n",
    "    \"\"\"\n",
    "    Calculate the lambda_mabed, i.e., the minimum lambda to nullify all penalized betas.\n",
    "    \"\"\"\n",
    "#     X_temp = X.copy()\n",
    "#     X_temp = X_temp[:,1:]\n",
    "#     X_temp -= np.mean(X_temp,0).reshape(1,-1)\n",
    "#     X_temp /= np.std(X_temp,0)\n",
    "#     y_temp = y.copy()\n",
    "#     y_temp -= np.mean(y)\n",
    "#     y_temp /= np.std(y)\n",
    "    _itemsize = np.dtype(_dtype).itemsize\n",
    "    _ = np.zeros(p)\n",
    "    if _order == \"F\":\n",
    "        for j in np.arange(p):\n",
    "            _X = np.memmap(bed, dtype=_dtype, mode='r', offset=j*_itemsize*N, shape = (N,))\n",
    "            _[j] = _X@y\n",
    "    elif _order == \"C\":\n",
    "        for j in np.arange(N):\n",
    "            _X = np.memmap(bed, dtype=_dtype, mode='r', offset=j*_itemsize*p, shape = (p,))\n",
    "            _ += _X*y[j]\n",
    "\n",
    "    grad_at_0 = _[1:]/N\n",
    "    lambda_max = np.linalg.norm(grad_at_0, ord=np.infty)\n",
    "    return lambda_max\n",
    "\n",
    "\n",
    "\n",
    "# @jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def SNP_UAG_LM_SCAD_MCP(design_matribed, outcome, N, p, L_convebed, _dtype, _order, beta_0 = np.ones(1), tol=1e-2, maxit=500, _lambda=.5, penalty=\"SCAD\", a=3.7, gamma=2.):\n",
    "    '''\n",
    "    Carry out the optimization for penalized LM for a fixed lambda.\n",
    "    '''\n",
    "    X = design_matrix\n",
    "    y = outcome\n",
    "    _itemsize = np.dtype(_dtype).itemsize\n",
    "    if np.all(beta_0==np.ones(1)):\n",
    "        _ = np.zeros(p)\n",
    "        if _order == \"F\":\n",
    "            for j in np.arange(p):\n",
    "                _X = np.memmap(bed, dtype=_dtype, mode='r', offset=j*_itemsize*N, shape = (N,))\n",
    "                _[j] = _X@y\n",
    "        elif _order == \"C\":\n",
    "            for j in np.arange(N):\n",
    "                _X = np.memmap(bed, dtype=_dtype, mode='r', offset=j*_itemsize*p, shape = (p,))\n",
    "                _ += _X*y[j]\n",
    "        beta = np.sign(_)\n",
    "    else:\n",
    "        beta = beta_0\n",
    "    # passing other parameters\n",
    "    smooth_grad = np.ones(p)\n",
    "    beta_ag = beta.copy()\n",
    "    beta_md = beta.copy()\n",
    "    k = 0\n",
    "    converged = False\n",
    "    opt_alpha = 1.\n",
    "    old_speed_norm = 1.\n",
    "    speed_norm = 1.\n",
    "    restart_k = 0\n",
    "    \n",
    "    if penalty == \"SCAD\":\n",
    "#         L = np.max(np.array([L_convebed, 1./(a-1)]))\n",
    "        L = np.linalg.norm(np.array([L_convebed, 1./(a-1)]), ord=np.infty)\n",
    "        opt_beta = .99/L\n",
    "        while ((not converged) or (k<3)) and k <= maxit:\n",
    "            k += 1\n",
    "            if old_speed_norm > speed_norm and k - restart_k>=3: # in this case, restart\n",
    "                opt_alpha = 1. # restarting\n",
    "                restart_k = k # restarting\n",
    "            else: # restarting\n",
    "                opt_alpha = 2/(1+(1+4./opt_alpha**2)**.5) #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound \n",
    "            opt_lambda = opt_beta/opt_alpha #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound\n",
    "            beta_md_old = beta_md.copy() # restarting\n",
    "            beta_md = (1-opt_alpha)*beta_ag + opt_alpha*beta\n",
    "            old_speed_norm = speed_norm # restarting\n",
    "            speed_norm = np.linalg.norm(beta_md - beta_md_old, ord=2) # restarting\n",
    "            converged = (np.linalg.norm(beta_md - beta_md_old, ord=np.infty) < tol)\n",
    "            smooth_grad = _SNP_update_smooth_grad_SCAD_LM(N=N, p=p, X=bed, beta_md=beta_md, y=y, _lambda=_lambda, a=a, _dtype=_dtype, _order=_order)\n",
    "            beta = soft_thresholding(x=beta - opt_lambda*smooth_grad, lambda_=opt_lambda*_lambda)\n",
    "            beta_ag = soft_thresholding(x=beta_md - opt_beta*smooth_grad, lambda_=opt_beta*_lambda)\n",
    "#             converged = np.all(np.max(np.abs(beta_md - beta_ag)/opt_beta) < tol).item()\n",
    "#             converged = (np.linalg.norm(beta_md - beta_ag, ord=np.infty) < (tol*opt_beta))\n",
    "    else:\n",
    "#         L = np.max(np.array([L_convebed, 1./(gamma)]))\n",
    "        L = np.linalg.norm(np.array([L_convebed, 1./(gamma)]), ord=np.infty)\n",
    "        opt_beta = .99/L\n",
    "        while ((not converged) or (k<3)) and k <= maxit:\n",
    "            k += 1\n",
    "            if old_speed_norm > speed_norm and k - restart_k>=3: # in this case, restart\n",
    "                opt_alpha = 1. # restarting\n",
    "                restart_k = k # restarting\n",
    "            else: # restarting\n",
    "                opt_alpha = 2/(1+(1+4./opt_alpha**2)**.5) #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound \n",
    "            opt_lambda = opt_beta/opt_alpha #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound\n",
    "            beta_md_old = beta_md.copy() # restarting\n",
    "            beta_md = (1-opt_alpha)*beta_ag + opt_alpha*beta\n",
    "            old_speed_norm = speed_norm # restarting\n",
    "            speed_norm = np.linalg.norm(beta_md - beta_md_old, ord=2) # restarting\n",
    "            converged = (np.linalg.norm(beta_md - beta_md_old, ord=np.infty) < tol)\n",
    "            smooth_grad = _SNP_update_smooth_grad_MCP_LM(N=N, p=p, X=bed, beta_md=beta_md, y=y, _lambda=_lambda, gamma=gamma, _dtype=_dtype, _order=_order)\n",
    "            beta = soft_thresholding(x=beta - opt_lambda*smooth_grad, lambda_=opt_lambda*_lambda)\n",
    "            beta_ag = soft_thresholding(x=beta_md - opt_beta*smooth_grad, lambda_=opt_beta*_lambda)\n",
    "#             converged = np.all(np.max(np.abs(beta_md - beta_ag)/opt_beta) < tol).item()\n",
    "#             converged = (np.linalg.norm(beta_md - beta_ag, ord=np.infty) < (tol*opt_beta))\n",
    "    return k, beta_md\n",
    "\n",
    "\n",
    "# @jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def SNP_solution_path_LM(design_matribed, outcome, lambda_, L_convebed, N, p, beta_0 = np.ones(1), tol=1e-2, maxit=500, penalty=\"SCAD\", a=3.7, gamma=2., _dtype='float32', _order=\"F\"):\n",
    "    '''\n",
    "    Carry out the optimization for the solution path without the strong rule.\n",
    "    '''\n",
    "    beta_mat = np.zeros((len(lambda_)+1, p))\n",
    "    for j in range(len(lambda_)): \n",
    "        beta_mat[j+1,:] = SNP_UAG_LM_SCAD_MCP(design_matrix=design_matribed, outcome=outcome, N=N, p=p, L_convex=L_convebed, beta_0 = beta_mat[j,:], tol=tol, maxit=maxit, _lambda=lambda_[j], penalty=penalty, a=a, gamma=gamma, _dtype=_dtype, _order=_order,)[1]\n",
    "    return beta_mat[1:,:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T17:50:28.525459Z",
     "start_time": "2022-07-15T17:49:32.587623Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alphaIto\\AppData\\Local\\Temp\\ipykernel_16288\\932946465.py:345: NumbaPerformanceWarning: \u001b[1m\u001b[1m\u001b[1m'@' is faster on contiguous arrays, called on (array(float64, 1d, C), array(float64, 2d, A))\u001b[0m\u001b[0m\u001b[0m\n",
      "  _lambda_max = lambda_max_LM(_design_matrix, outcome)\n",
      "C:\\Users\\alphaIto\\AppData\\Local\\Temp\\ipykernel_16288\\932946465.py:345: NumbaPerformanceWarning: \u001b[1m\u001b[1m'@' is faster on contiguous arrays, called on (array(float64, 1d, C), array(float64, 2d, A))\u001b[0m\u001b[0m\n",
      "  _lambda_max = lambda_max_LM(_design_matrix, outcome)\n",
      "C:\\Users\\alphaIto\\AppData\\Local\\Temp\\ipykernel_16288\\932946465.py:349: NumbaPerformanceWarning: \u001b[1m\u001b[1m\u001b[1m'@' is faster on contiguous arrays, called on (array(float64, 1d, C), array(float64, 2d, A))\u001b[0m\u001b[0m\u001b[0m\n",
      "  _elim = strong_rule_seq_LM(X=_design_matrix, y=outcome, beta_old=beta_mat[j,:], lambda_new=lambda_[j+1], lambda_old=lambda_[j])\n",
      "C:\\Users\\alphaIto\\AppData\\Local\\Temp\\ipykernel_16288\\932946465.py:349: NumbaPerformanceWarning: \u001b[1m\u001b[1m\u001b[1m'@' is faster on contiguous arrays, called on (array(float64, 2d, A), array(float64, 1d, C))\u001b[0m\u001b[0m\u001b[0m\n",
      "  _elim = strong_rule_seq_LM(X=_design_matrix, y=outcome, beta_old=beta_mat[j,:], lambda_new=lambda_[j+1], lambda_old=lambda_[j])\n",
      "C:\\Users\\alphaIto\\AppData\\Local\\Temp\\ipykernel_16288\\932946465.py:349: NumbaPerformanceWarning: \u001b[1m\u001b[1m'@' is faster on contiguous arrays, called on (array(float64, 1d, C), array(float64, 2d, A))\u001b[0m\u001b[0m\n",
      "  _elim = strong_rule_seq_LM(X=_design_matrix, y=outcome, beta_old=beta_mat[j,:], lambda_new=lambda_[j+1], lambda_old=lambda_[j])\n",
      "C:\\Users\\alphaIto\\AppData\\Local\\Temp\\ipykernel_16288\\932946465.py:349: NumbaPerformanceWarning: \u001b[1m\u001b[1m'@' is faster on contiguous arrays, called on (array(float64, 2d, A), array(float64, 1d, C))\u001b[0m\u001b[0m\n",
      "  _elim = strong_rule_seq_LM(X=_design_matrix, y=outcome, beta_old=beta_mat[j,:], lambda_new=lambda_[j+1], lambda_old=lambda_[j])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.25145030e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 1.25529911e+00  1.93834934e+00 -1.87491707e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 1.25529773e+00  1.94263335e+00 -1.88578904e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " ...\n",
      " [ 1.25529526e+00  1.87673754e+00 -1.91218195e+00 ...  7.69226132e-02\n",
      "  -8.24977017e-02  0.00000000e+00]\n",
      " [ 1.25529526e+00  1.87214777e+00 -1.90930101e+00 ...  8.90060269e-02\n",
      "  -8.95442224e-02  0.00000000e+00]\n",
      " [ 1.25529526e+00  1.86767007e+00 -1.90691276e+00 ...  1.00662533e-01\n",
      "  -9.61300285e-02  1.36140337e-04]]\n",
      "4.15 s ± 97.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.markers as markers\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit\n",
    "from scipy.linalg import toeplitz, block_diag\n",
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "N = 1000\n",
    "SNR = 5.\n",
    "true_beta = np.array([2,-2,8,-8]+[0]*1000)\n",
    "X_cov = toeplitz(.6**np.arange(true_beta.shape[0]))\n",
    "mean = np.zeros(true_beta.shape[0])\n",
    "X = np.random.multivariate_normal(mean, X_cov, N)\n",
    "X -= np.mean(X,0).reshape(1,-1)\n",
    "X /= np.std(X,0)\n",
    "intercept_design_column = np.ones(N).reshape(N, 1)\n",
    "X_sim = np.concatenate((intercept_design_column, X), 1)\n",
    "true_sigma_sim = np.sqrt(true_beta.T@X_cov@true_beta/SNR)\n",
    "true_beta_intercept = np.concatenate((np.array([1.23]), true_beta)) # here just define the intercept to be 1.23 for simulated data \n",
    "epsilon = np.random.normal(0, true_sigma_sim, N)\n",
    "y_sim = X_sim@true_beta_intercept + epsilon\n",
    "\n",
    "\n",
    "\n",
    "lambda_seq = np.arange(40)/400\n",
    "lambda_seq = lambda_seq[1:]\n",
    "lambda_seq = lambda_seq[::-1]\n",
    "\n",
    "# do NOT include the design matrix intercept column \n",
    "LM_beta = solution_path_LM_strongrule(design_matrix=X_sim, outcome=y_sim, lambda_=lambda_seq, beta_0 = np.ones(1), tol=1e-2, maxit=500, penalty=\"SCAD\", a=3.7, gamma=2., add_intercept_column=True)\n",
    "\n",
    "print(LM_beta) # to make sure strong rule runs correctly \n",
    "\n",
    "%timeit solution_path_LM_strongrule(design_matrix=X_sim, outcome=y_sim, lambda_=lambda_seq, beta_0 = np.ones(1), tol=1e-2, maxit=500, penalty=\"SCAD\", a=3.7, gamma=2., add_intercept_column=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T17:51:00.230141Z",
     "start_time": "2022-07-15T17:50:28.527461Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alphaIto\\AppData\\Local\\Temp\\ipykernel_16288\\932946465.py:198: NumbaWarning: \u001b[1mCannot cache compiled function \"solution_path_LM\" as it uses dynamic globals (such as ctypes pointers and large global arrays)\u001b[0m\n",
      "  @jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n"
     ]
    }
   ],
   "source": [
    "# this cell is for profiling the function \n",
    "np.random.seed(0)\n",
    "N = 1000\n",
    "p_zeros = 2000\n",
    "SNR = 5.\n",
    "true_beta = np.array([2,-2,8,-8]+[0]*p_zeros)\n",
    "X_cov = toeplitz(0.6**np.arange(len(true_beta)))\n",
    "mean = np.zeros(len(true_beta))\n",
    "X = np.random.multivariate_normal(mean, X_cov, N)\n",
    "X -= np.mean(X,0).reshape(1,-1)\n",
    "X /= np.std(X,0)\n",
    "intercept_design_column = np.ones(N).reshape(N, 1)\n",
    "X_sim = np.concatenate((intercept_design_column, X), 1)\n",
    "true_sigma_sim = np.sqrt(true_beta.T@X_cov@true_beta/SNR)\n",
    "true_beta_intercept = np.concatenate((np.array([1.23]), true_beta)) # here just define the intercept to be 1.23 for simulated data \n",
    "epsilon = np.random.normal(0, true_sigma_sim, N)\n",
    "y_sim = X_sim@true_beta_intercept + epsilon\n",
    "\n",
    "fit1 = UAG_LM_SCAD_MCP_strongrule(design_matrix=X_sim, outcome=y_sim, tol=1e-2, maxit=500, _lambda=.2, penalty=\"SCAD\", a=3.7, gamma=2.)\n",
    "\n",
    "fit2 = solution_path_LM(design_matrix=X_sim, outcome=y_sim, tol=1e-2, maxit=500, lambda_=np.linspace(.1,1,100), penalty=\"SCAD\", a=3.7, gamma=2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T17:51:09.564171Z",
     "start_time": "2022-07-15T17:51:00.232145Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.16 s ± 34.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit UAG_LM_SCAD_MCP_strongrule(design_matrix=X_sim, outcome=y_sim, tol=1e-2, maxit=500, _lambda=.5, penalty=\"MCP\", a=3.7, gamma=2.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T17:51:09.580173Z",
     "start_time": "2022-07-15T17:51:09.565170Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66,\n",
       " array([ 1.28598359,  2.15247208, -1.92370802, ...,  0.        ,\n",
       "         0.16219557,  0.        ]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T17:51:09.705170Z",
     "start_time": "2022-07-15T17:51:09.582175Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(len(fit1[1])), fit1[1], 'o')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "877.844px",
    "left": "2188px",
    "right": "20px",
    "top": "120px",
    "width": "352px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
