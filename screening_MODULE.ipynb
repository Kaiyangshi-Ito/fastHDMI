{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1cc105c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-14T04:37:45.054986Z",
     "start_time": "2023-01-14T04:37:44.068961Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings as _warnings\n",
    "\n",
    "_warnings.filterwarnings('ignore')\n",
    "import numpy as _np\n",
    "#from bgen_reader import open_bgen\n",
    "#from os.path import join\n",
    "#from pandas_plink import read_plink1_bin\n",
    "#from pandas_plink import get_data_folder\n",
    "# import matplotlib.pyplot as plt\n",
    "#from bgen_reader import open_bgen\n",
    "#from bed_reader import open_bed, sample_file\n",
    "# import pandas as pd\n",
    "# import os\n",
    "# from pysnptools.distreader import Bgen, DistMemMap\n",
    "# from pathlib import Path\n",
    "from numba import jit as _jit\n",
    "from bed_reader import open_bed as _open_bed\n",
    "# from pysnptools.snpreader import SnpData, SnpMemMap\n",
    "# from pysnptools.util import log_in_place\n",
    "# import matplotlib.pyplot as plt\n",
    "# from scipy.stats import gaussian_kde\n",
    "from KDEpy import FFTKDE as _FFTKDE\n",
    "import pandas as _pd\n",
    "from sklearn.preprocessing import RobustScaler as _scaler\n",
    "# from pybgen import PyBGEN\n",
    "import multiprocess as _mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1741add3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-14T04:37:45.230986Z",
     "start_time": "2023-01-14T04:37:45.056958Z"
    }
   },
   "outputs": [],
   "source": [
    "@_jit(forceobj=True, nogil=True, cache=True, parallel=True, fastmath=True)\n",
    "def MI_continuous_SNP(a,\n",
    "                      b,\n",
    "                      N=500,\n",
    "                      kernel=\"epa\",\n",
    "                      bw=\"silverman\",\n",
    "                      machine_err=1e-12):\n",
    "    \"\"\"\n",
    "    calculate mutual information between continuous outcome and an SNP variable of 0,1,2\n",
    "    assume no missing data\n",
    "    \"\"\"\n",
    "    # first estimate the pmf\n",
    "    p0 = _np.sum(b == 0) / len(b)\n",
    "    p1 = _np.sum(b == 1) / len(b)\n",
    "    p2 = 1. - p0 - p1\n",
    "    _a = _scaler().fit_transform(a.reshape(-1, 1)).flatten()\n",
    "    # this step is just to get the boundary width for the joint density grid\n",
    "    # the three conditional density estimates need to be evaluated on the joint density grid\n",
    "    a_temp, _ = _FFTKDE(kernel=kernel, bw=bw).fit(data=_a).evaluate(N)\n",
    "    # estimate cond density\n",
    "    _b0 = (b == 0)\n",
    "    if _np.sum(_b0) > 2:\n",
    "        # here proceed to kde only if there are more than 5 data points\n",
    "        y_cond_p0 = _FFTKDE(kernel=kernel, bw=bw).fit(data=_a[_b0])\n",
    "#         y_cond_p0 = gaussian_kde(_a[_b0])\n",
    "    else:\n",
    "        y_cond_p0 = _np.zeros_like\n",
    "    _b1 = (b == 1)\n",
    "    if _np.sum(_b1) > 2:\n",
    "        y_cond_p1 = _FFTKDE(kernel=kernel, bw=bw).fit(data=_a[_b1])\n",
    "#         y_cond_p1 = gaussian_kde(_a[_b1]) # this thing uses Scott's rule instead of Silverman defaulted by FFTKDE and R density\n",
    "    else:\n",
    "        y_cond_p1 = _np.zeros_like\n",
    "    _b2 = (b == 2)\n",
    "    if _np.sum(_b2) > 2:\n",
    "        y_cond_p2 = _FFTKDE(kernel=kernel, bw=bw).fit(data=_a[_b2])\n",
    "\n",
    "\n",
    "#         y_cond_p2 = gaussian_kde(_a[_b2])\n",
    "    else:\n",
    "        y_cond_p2 = _np.zeros_like\n",
    "    joint = _np.empty((N, 3))\n",
    "    joint[:, 0] = y_cond_p0(a_temp) * p0\n",
    "    joint[:, 1] = y_cond_p1(a_temp) * p1\n",
    "    joint[:, 2] = y_cond_p2(a_temp) * p2\n",
    "    mask = joint < machine_err\n",
    "    forward_euler_step = a_temp[1] - a_temp[0]\n",
    "    joint[mask] = 0.\n",
    "    # to scale the cdf to 1.\n",
    "    joint /= _np.sum(joint) * forward_euler_step\n",
    "    #     print(\"total measure:\",  _np.sum(joint)*forward_euler_step)\n",
    "    temp_log = _np.log(joint)\n",
    "    temp_log = _np.nan_to_num(temp_log, nan=0.)\n",
    "    temp1 = _np.log(_np.sum(joint, 1))\n",
    "    temp1 = _np.nan_to_num(temp1, nan=0.)\n",
    "    temp_log = temp_log - temp1.reshape(-1, 1)\n",
    "    temp2 = _np.log(_np.sum(joint, 0)) + _np.log(forward_euler_step)\n",
    "    temp2 = _np.nan_to_num(temp2, nan=0.)\n",
    "    temp_log = temp_log - temp2.reshape(1, -1)\n",
    "    # print(fhat_mat * temp_log)\n",
    "    temp_mat = joint * temp_log\n",
    "    #     temp_mat =  _np.nan_to_num(temp_mat, nan=0.) # numerical fix\n",
    "    mi_temp = _np.sum(temp_mat) * forward_euler_step\n",
    "\n",
    "    # this is to ensure that estimated MI is positive, to solve an numerical issue\n",
    "    if mi_temp < machine_err:\n",
    "        mi_temp = machine_err\n",
    "\n",
    "    return mi_temp\n",
    "\n",
    "\n",
    "@_jit(nopython=True, nogil=True, cache=True, parallel=True, fastmath=True)\n",
    "def _nan_to_0(x):\n",
    "    \"\"\"\n",
    "    To convert NaN to 0 in nopython mode.\n",
    "    \"\"\"\n",
    "    return _np.where(_np.isnan(x), 0., x)\n",
    "\n",
    "\n",
    "@_jit(nopython=True, nogil=True, cache=True, parallel=True, fastmath=True)\n",
    "def MI_binary_SNP(a, b, machine_err=1e-12):\n",
    "    \"\"\"\n",
    "    calculate mutual information between binary outcome and an SNP variable of 0,1,2\n",
    "    assume no missing data\n",
    "    \"\"\"\n",
    "    # first estimate the pmf of SNP\n",
    "    p0 = _np.sum(b == 0) / len(b)\n",
    "    p1 = _np.sum(b == 1) / len(b)\n",
    "    p2 = 1. - p0 - p1\n",
    "    b_marginal = _np.array([p0, p1, p2])\n",
    "    # estimate pmf of the binary outcome\n",
    "    a_p0 = _np.sum(a == 0) / len(a)\n",
    "    a_p1 = _np.sum(a == 1) / len(a)\n",
    "    a_marginal = _np.array([a_p0, a_p1]).reshape(-1, 1)\n",
    "    # estimate the cond density\n",
    "    joint = _np.zeros((2, 3))\n",
    "    _b0 = (b == 0)\n",
    "    joint[0, 0] = _np.sum(a[_b0] == 0) / len(a)\n",
    "    joint[1, 0] = _np.sum(a[_b0] == 1) / len(a)\n",
    "    _b1 = (b == 1)\n",
    "    joint[0, 1] = _np.sum(a[_b1] == 0) / len(a)\n",
    "    joint[1, 1] = _np.sum(a[_b1] == 1) / len(a)\n",
    "    _b2 = (b == 2)\n",
    "    joint[0, 2] = _np.sum(a[_b2] == 0) / len(a)\n",
    "    joint[1, 2] = _np.sum(a[_b2] == 1) / len(a)\n",
    "\n",
    "    _temp = a_marginal * b_marginal\n",
    "    _temp = joint / _temp\n",
    "    _temp = joint * _np.log(_temp)\n",
    "    _temp = _nan_to_0(_temp)  # for possible nuemrical issues\n",
    "\n",
    "    mi_temp = _np.sum(_temp)\n",
    "\n",
    "    # this is to ensure that estimated MI is positive, to solve an numerical issue\n",
    "    if mi_temp < machine_err:\n",
    "        mi_temp = machine_err\n",
    "\n",
    "    return mi_temp\n",
    "\n",
    "\n",
    "# make this function available\n",
    "@_jit(forceobj=True, nogil=True, cache=True, parallel=True, fastmath=True)\n",
    "def MI_bivariate_continuous(a,\n",
    "                            b,\n",
    "                            a_N=300,\n",
    "                            b_N=300,\n",
    "                            kernel=\"epa\",\n",
    "                            bw=\"silverman\",\n",
    "                            norm=2,\n",
    "                            machine_err=1e-12):\n",
    "    \"\"\"\n",
    "    (Single Core version) calculate mutual information on bivariate continuous r.v..\n",
    "    \"\"\"\n",
    "    _temp = _np.argsort(a)\n",
    "    data = _np.hstack((a[_temp].reshape(-1, 1), b[_temp].reshape(-1, 1)))\n",
    "    _data = _scaler().fit_transform(data)\n",
    "    grid, joint = _FFTKDE(kernel=kernel, norm=norm).fit(_data).evaluate(\n",
    "        (a_N, b_N))\n",
    "    joint = joint.reshape(b_N, -1).T\n",
    "    # this gives joint as a (a_N, b_N) array, following example: https://kdepy.readthedocs.io/en/latest/examples.html#the-effect-of-norms-in-2d\n",
    "    a_forward_euler_step = grid[b_N, 0] - grid[0, 0]\n",
    "    b_forward_euler_step = grid[1, 1] - grid[0, 1]\n",
    "    mask = joint < machine_err\n",
    "    joint[mask] = 0.\n",
    "    # to scale the cdf to 1.\n",
    "    joint /= _np.sum(joint) * a_forward_euler_step * b_forward_euler_step\n",
    "    log_a_marginal = _np.log(_np.sum(joint, 1)) + _np.log(b_forward_euler_step)\n",
    "    log_a_marginal = _np.nan_to_num(log_a_marginal, nan=0.)\n",
    "    log_b_marginal = _np.log(_np.sum(joint, 0)) + _np.log(a_forward_euler_step)\n",
    "    log_b_marginal = _np.nan_to_num(log_b_marginal, nan=0.)\n",
    "    log_joint = _np.log(joint)\n",
    "    log_joint = _np.nan_to_num(log_joint, nan=0.)\n",
    "    mi_temp = _np.sum(\n",
    "        joint *\n",
    "        (log_joint - log_a_marginal.reshape(-1, 1) - log_b_marginal.reshape(\n",
    "            1, -1))) * a_forward_euler_step * b_forward_euler_step\n",
    "\n",
    "    # this is to ensure that estimated MI is positive, to solve an numerical issue\n",
    "    if mi_temp < machine_err:\n",
    "        mi_temp = machine_err\n",
    "\n",
    "    return mi_temp\n",
    "\n",
    "\n",
    "# make this function available\n",
    "@_jit(forceobj=True, nogil=True, cache=True, parallel=True, fastmath=True)\n",
    "def MI_binary_continuous(a,\n",
    "                         b,\n",
    "                         N=500,\n",
    "                         kernel=\"epa\",\n",
    "                         bw=\"silverman\",\n",
    "                         machine_err=1e-12):\n",
    "    return MI_continuous_SNP(a=b,\n",
    "                             b=a,\n",
    "                             N=N,\n",
    "                             kernel=kernel,\n",
    "                             bw=bw,\n",
    "                             machine_err=machine_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ce8b66d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-14T04:37:45.277955Z",
     "start_time": "2023-01-14T04:37:45.233957Z"
    }
   },
   "outputs": [],
   "source": [
    "# outcome_iid should be a  list of strings for identifiers\n",
    "def continuous_filter_plink(bed_file,\n",
    "                            bim_file,\n",
    "                            fam_file,\n",
    "                            outcome,\n",
    "                            outcome_iid,\n",
    "                            N=500,\n",
    "                            kernel=\"epa\",\n",
    "                            bw=\"silverman\",\n",
    "                            machine_err=1e-12):\n",
    "    \"\"\"\n",
    "    (Single Core version) take plink files to calculate the mutual information between the continuous outcome and many SNP variables.\n",
    "    \"\"\"\n",
    "    bed1 = _open_bed(filepath=bed_file,\n",
    "                     fam_filepath=fam_file,\n",
    "                     bim_filepath=bim_file)\n",
    "    gene_iid = _np.array(list(bed1.iid))\n",
    "    bed1_sid = _np.array(list(bed1.sid))\n",
    "    outcome = outcome[_np.intersect1d(outcome_iid,\n",
    "                                      gene_iid,\n",
    "                                      assume_unique=True,\n",
    "                                      return_indices=True)[1]]\n",
    "\n",
    "    # get genetic indices\n",
    "    gene_ind = _np.intersect1d(gene_iid,\n",
    "                               outcome_iid,\n",
    "                               assume_unique=True,\n",
    "                               return_indices=True)[1]\n",
    "    MI_UKBB = _np.zeros(len(bed1_sid))\n",
    "    for j in range(len(MI_UKBB)):\n",
    "        _SNP = bed1.read(_np.s_[:, j], dtype=_np.int8).flatten()\n",
    "        _SNP = _SNP[gene_ind]  # get gene iid also in outcome iid\n",
    "        _outcome = outcome[_SNP != -127]  # remove missing SNP in outcome\n",
    "        _SNP = _SNP[_SNP != -127]  # remove missing SNP\n",
    "        MI_UKBB[j] = MI_continuous_SNP(a=_outcome,\n",
    "                                       b=_SNP,\n",
    "                                       N=N,\n",
    "                                       kernel=kernel,\n",
    "                                       bw=bw,\n",
    "                                       machine_err=machine_err)\n",
    "    return MI_UKBB\n",
    "\n",
    "\n",
    "def binary_filter_plink(bed_file,\n",
    "                        bim_file,\n",
    "                        fam_file,\n",
    "                        outcome,\n",
    "                        outcome_iid,\n",
    "                        machine_err=1e-12):\n",
    "    \"\"\"\n",
    "    (Single Core version) take plink files to calculate the mutual information between the binary outcome and many SNP variables.\n",
    "    \"\"\"\n",
    "    bed1 = _open_bed(filepath=bed_file,\n",
    "                     fam_filepath=fam_file,\n",
    "                     bim_filepath=bim_file)\n",
    "    gene_iid = _np.array(list(bed1.iid))\n",
    "    bed1_sid = _np.array(list(bed1.sid))\n",
    "    outcome = outcome[_np.intersect1d(outcome_iid,\n",
    "                                      gene_iid,\n",
    "                                      assume_unique=True,\n",
    "                                      return_indices=True)[1]]\n",
    "    # get genetic indices\n",
    "    gene_ind = _np.intersect1d(gene_iid,\n",
    "                               outcome_iid,\n",
    "                               assume_unique=True,\n",
    "                               return_indices=True)[1]\n",
    "    MI_UKBB = _np.zeros(len(bed1_sid))\n",
    "    for j in range(len(MI_UKBB)):\n",
    "        _SNP = bed1.read(_np.s_[:, j], dtype=_np.int8).flatten()\n",
    "        _SNP = _SNP[gene_ind]  # get gene iid also in outcome iid\n",
    "        _outcome = outcome[_SNP != -127]  # remove missing SNP in outcome\n",
    "        _SNP = _SNP[_SNP != -127]  # remove missing SNP\n",
    "        MI_UKBB[j] = MI_binary_SNP(a=_outcome, b=_SNP, machine_err=machine_err)\n",
    "    return MI_UKBB\n",
    "\n",
    "\n",
    "def continuous_filter_plink_parallel(bed_file,\n",
    "                                     bim_file,\n",
    "                                     fam_file,\n",
    "                                     outcome,\n",
    "                                     outcome_iid,\n",
    "                                     N=500,\n",
    "                                     kernel=\"epa\",\n",
    "                                     bw=\"silverman\",\n",
    "                                     machine_err=1e-12,\n",
    "                                     core_num=\"NOT DECLARED\",\n",
    "                                     multp=1):\n",
    "    \"\"\"\n",
    "    (Multiprocessing version) take plink files to calculate the mutual information between the continuous outcome and many SNP variables.\n",
    "    \"\"\"\n",
    "    if core_num == \"NOT DECLARED\":\n",
    "        core_num = _mp.cpu_count()\n",
    "    else:\n",
    "        assert core_num <= _mp.cpu_count(\n",
    "        ), \"Declared number of cores used for multiprocessing should not exceed number of cores on this machine.\"\n",
    "    assert core_num >= 2, \"Multiprocessing should not be used on single-core machines.\"\n",
    "\n",
    "    bed1 = _open_bed(filepath=bed_file,\n",
    "                     fam_filepath=fam_file,\n",
    "                     bim_filepath=bim_file)\n",
    "    gene_iid = _np.array(list(bed1.iid))\n",
    "    bed1_sid = _np.array(list(bed1.sid))\n",
    "    outcome = outcome[_np.intersect1d(outcome_iid,\n",
    "                                      gene_iid,\n",
    "                                      assume_unique=True,\n",
    "                                      return_indices=True)[1]]\n",
    "    # get genetic indices\n",
    "    gene_ind = _np.intersect1d(gene_iid,\n",
    "                               outcome_iid,\n",
    "                               assume_unique=True,\n",
    "                               return_indices=True)[1]\n",
    "\n",
    "    def _continuous_filter_plink_slice(_slice):\n",
    "        _MI_slice = _np.zeros(len(_slice))\n",
    "        k = 0\n",
    "        for j in _slice:\n",
    "            _SNP = bed1.read(_np.s_[:, j], dtype=_np.int8).flatten()\n",
    "            _SNP = _SNP[gene_ind]  # get gene iid also in outcome iid\n",
    "            _outcome = outcome[_SNP != -127]  # remove missing SNP in outcome\n",
    "            _SNP = _SNP[_SNP != -127]  # remove missing SNP\n",
    "            _MI_slice[k] = MI_continuous_SNP(a=_outcome,\n",
    "                                             b=_SNP,\n",
    "                                             N=N,\n",
    "                                             kernel=kernel,\n",
    "                                             bw=bw,\n",
    "                                             machine_err=machine_err)\n",
    "            k += 1\n",
    "        return _MI_slice\n",
    "\n",
    "    # multiprocessing starts here\n",
    "    ind = _np.arange(len(bed1_sid))\n",
    "    with _mp.Pool(core_num) as pl:\n",
    "        MI_UKBB = pl.map(_continuous_filter_plink_slice,\n",
    "                         _np.array_split(ind, core_num * multp))\n",
    "    MI_UKBB = _np.hstack(MI_UKBB)\n",
    "    return MI_UKBB\n",
    "\n",
    "\n",
    "def binary_filter_plink_parallel(bed_file,\n",
    "                                 bim_file,\n",
    "                                 fam_file,\n",
    "                                 outcome,\n",
    "                                 outcome_iid,\n",
    "                                 core_num=\"NOT DECLARED\",\n",
    "                                 multp=1,\n",
    "                                 machine_err=1e-12):\n",
    "    \"\"\"\n",
    "    (Multiprocessing version) take plink files to calculate the mutual information between the binary outcome and many SNP variables.\n",
    "    \"\"\"\n",
    "    if core_num == \"NOT DECLARED\":\n",
    "        core_num = _mp.cpu_count()\n",
    "    else:\n",
    "        assert core_num <= _mp.cpu_count(\n",
    "        ), \"Declared number of cores used for multiprocessing should not exceed number of cores on this machine.\"\n",
    "    assert core_num >= 2, \"Multiprocessing should not be used on single-core machines.\"\n",
    "\n",
    "    bed1 = _open_bed(filepath=bed_file,\n",
    "                     fam_filepath=fam_file,\n",
    "                     bim_filepath=bim_file)\n",
    "    gene_iid = _np.array(list(bed1.iid))\n",
    "    bed1_sid = _np.array(list(bed1.sid))\n",
    "    outcome = outcome[_np.intersect1d(outcome_iid,\n",
    "                                      gene_iid,\n",
    "                                      assume_unique=True,\n",
    "                                      return_indices=True)[1]]\n",
    "    # get genetic indices\n",
    "    gene_ind = _np.intersect1d(gene_iid,\n",
    "                               outcome_iid,\n",
    "                               assume_unique=True,\n",
    "                               return_indices=True)[1]\n",
    "\n",
    "    def _binary_filter_plink_slice(_slice):\n",
    "        _MI_slice = _np.zeros(len(_slice))\n",
    "        k = 0\n",
    "        for j in _slice:\n",
    "            _SNP = bed1.read(_np.s_[:, j], dtype=_np.int8).flatten()\n",
    "            _SNP = _SNP[gene_ind]  # get gene iid also in outcome iid\n",
    "            _outcome = outcome[_SNP != -127]  # remove missing SNP in outcome\n",
    "            _SNP = _SNP[_SNP != -127]  # remove missing SNP\n",
    "            _MI_slice[k] = MI_binary_SNP(a=_outcome,\n",
    "                                         b=_SNP,\n",
    "                                         machine_err=machine_err)\n",
    "            k += 1\n",
    "        return _MI_slice\n",
    "\n",
    "    # multiprocessing starts here\n",
    "    ind = _np.arange(len(bed1_sid))\n",
    "    with _mp.Pool(core_num) as pl:\n",
    "        MI_UKBB = pl.map(_binary_filter_plink_slice,\n",
    "                         _np.array_split(ind, core_num * multp))\n",
    "    MI_UKBB = _np.hstack(MI_UKBB)\n",
    "    return MI_UKBB\n",
    "\n",
    "\n",
    "def binary_filter_csv(csv_file,\n",
    "                      _usecols=[],\n",
    "                      N=500,\n",
    "                      kernel=\"epa\",\n",
    "                      bw=\"silverman\",\n",
    "                      machine_err=1e-12):\n",
    "    \"\"\"\n",
    "    Take a (potentionally large) csv file to calculate the mutual information between outcome and covariates.\n",
    "    The outcome should be binary and the covariates be continuous. \n",
    "    If _usecols is given, the returned mutual information will match _usecols. \n",
    "    By default, the left first covariate should be the outcome -- use _usecols to adjust if not the case.\n",
    "    \"\"\"\n",
    "    # outcome is the first variable by default; if other specifications are needed, put it the first item in _usecols\n",
    "    if _np.array(_usecols).size == 0:\n",
    "        print(\n",
    "            \"Variable names not provided -- start reading variable names from csv file now, might take some time, depending on the csv file size.\"\n",
    "        )\n",
    "        _usecols = _pd.read_csv(csv_file, index_col=0,\n",
    "                                nrows=0).columns.tolist()\n",
    "        print(\"Reading variable names from csv file finished.\")\n",
    "    else:\n",
    "        _usecols = _np.array(_usecols)\n",
    "    MI_csv = _np.empty(len(_usecols) - 1)\n",
    "    for j in _np.arange(len(_usecols) - 1):\n",
    "        __ = [\n",
    "            _usecols[0], _usecols[j + 1]\n",
    "        ]  # here using _usecol[j + 1] because the left first column is the outcome\n",
    "        _ = _pd.read_csv(csv_file,\n",
    "                         skipinitialspace=True,\n",
    "                         usecols=__,\n",
    "                         encoding='unicode_escape').dropna()\n",
    "        _a = _[_usecols[0]].to_numpy()\n",
    "        _b = _[_usecols[j + 1]].to_numpy()\n",
    "        MI_csv[j] = MI_binary_continuous(a=_a,\n",
    "                                         b=_b,\n",
    "                                         N=N,\n",
    "                                         kernel=kernel,\n",
    "                                         bw=bw,\n",
    "                                         machine_err=machine_err)\n",
    "    return MI_csv\n",
    "\n",
    "\n",
    "def continuous_filter_csv(csv_file,\n",
    "                          _usecols=[],\n",
    "                          a_N=300,\n",
    "                          b_N=300,\n",
    "                          kernel=\"epa\",\n",
    "                          bw=\"silverman\",\n",
    "                          norm=2,\n",
    "                          machine_err=1e-12):\n",
    "    \"\"\"\n",
    "    Take a (potentionally large) csv file to calculate the mutual information between outcome and covariates.\n",
    "    Both the outcome and the covariates should be continuous. \n",
    "    If _usecols is given, the returned mutual information will match _usecols. \n",
    "    By default, the left first covariate should be the outcome -- use _usecols to adjust if not the case.\n",
    "    \"\"\"\n",
    "    if _np.array(_usecols).size == 0:\n",
    "        print(\n",
    "            \"Variable names not provided -- start reading variable names from csv file now, might take some time, depending on the csv file size.\"\n",
    "        )\n",
    "        _usecols = _pd.read_csv(csv_file, index_col=0,\n",
    "                                nrows=0).columns.tolist()\n",
    "        print(\"Reading variable names from csv file finished.\")\n",
    "    else:\n",
    "        _usecols = _np.array(_usecols)\n",
    "    MI_csv = _np.empty(len(_usecols) - 1)\n",
    "    for j in _np.arange(len(_usecols) - 1):\n",
    "        __ = [\n",
    "            _usecols[0], _usecols[j + 1]\n",
    "        ]  # here using _usecol[j + 1] because the left first column is the outcome\n",
    "        _ = _pd.read_csv(csv_file,\n",
    "                         skipinitialspace=True,\n",
    "                         usecols=__,\n",
    "                         encoding='unicode_escape').dropna()\n",
    "        _a = _[_usecols[0]].to_numpy()\n",
    "        _b = _[_usecols[j + 1]].to_numpy()\n",
    "        MI_csv[j] = MI_bivariate_continuous(a=_a,\n",
    "                                            b=_b,\n",
    "                                            a_N=a_N,\n",
    "                                            b_N=b_N,\n",
    "                                            kernel=kernel,\n",
    "                                            bw=bw,\n",
    "                                            norm=norm,\n",
    "                                            machine_err=machine_err)\n",
    "    return MI_csv\n",
    "\n",
    "\n",
    "def binary_filter_csv_parallel(csv_file,\n",
    "                               _usecols=[],\n",
    "                               N=500,\n",
    "                               kernel=\"epa\",\n",
    "                               bw=\"silverman\",\n",
    "                               machine_err=1e-12,\n",
    "                               core_num=\"NOT DECLARED\",\n",
    "                               multp=1):\n",
    "    \"\"\"\n",
    "    (Multiprocessing version) Take a (potentionally large) csv file to calculate the mutual information between outcome and covariates.\n",
    "    The outcome should be binary and the covariates be continuous. \n",
    "    If _usecols is given, the returned mutual information will match _usecols. \n",
    "    By default, the left first covariate should be the outcome -- use _usecols to adjust if not the case.\n",
    "    \"\"\"\n",
    "    if core_num == \"NOT DECLARED\":\n",
    "        core_num = _mp.cpu_count()\n",
    "    else:\n",
    "        assert core_num <= _mp.cpu_count(\n",
    "        ), \"Declared number of cores used for multiprocessing should not exceed number of cores on this machine.\"\n",
    "    assert core_num >= 2, \"Multiprocessing should not be used on single-core machines.\"\n",
    "\n",
    "    if _np.array(_usecols).size == 0:\n",
    "        print(\n",
    "            \"Variable names not provided -- start reading variable names from csv file now, might take some time, depending on the csv file size.\"\n",
    "        )\n",
    "        _usecols = _pd.read_csv(csv_file, index_col=0,\n",
    "                                nrows=0).columns.tolist()\n",
    "        print(\"Reading variable names from csv file finished.\")\n",
    "    else:\n",
    "        _usecols = _np.array(_usecols)\n",
    "\n",
    "    def _binary_filter_csv_slice(_slice):\n",
    "        _MI_slice = _np.zeros(\n",
    "            len(_slice))  # returned MI should be of the same length as slice\n",
    "        k = 0\n",
    "        for j in _slice:\n",
    "            __ = [\n",
    "                _usecols[0], _usecols[j]\n",
    "            ]  # here using _usecol[j] because only input variables indices were splitted\n",
    "            _ = _pd.read_csv(csv_file,\n",
    "                             skipinitialspace=True,\n",
    "                             usecols=__,\n",
    "                             encoding='unicode_escape').dropna()\n",
    "            _a = _[_usecols[0]].to_numpy()\n",
    "            _b = _[_usecols[j]].to_numpy()\n",
    "            _MI_slice[k] = MI_binary_continuous(a=_a,\n",
    "                                                b=_b,\n",
    "                                                N=N,\n",
    "                                                kernel=kernel,\n",
    "                                                bw=bw,\n",
    "                                                machine_err=machine_err)\n",
    "            k += 1\n",
    "        return _MI_slice\n",
    "\n",
    "    # multiprocessing starts here\n",
    "    ind = _np.arange(\n",
    "        1, len(_usecols)\n",
    "    )  # starting from 1 because the first left column should be the outcome\n",
    "    with _mp.Pool(core_num) as pl:\n",
    "        MI_csv = pl.map(_binary_filter_csv_slice,\n",
    "                        _np.array_split(ind, core_num * multp))\n",
    "    MI_csv = _np.hstack(MI_csv)\n",
    "    return MI_csv\n",
    "\n",
    "\n",
    "def continuous_filter_csv_parallel(csv_file,\n",
    "                                   _usecols=[],\n",
    "                                   a_N=300,\n",
    "                                   b_N=300,\n",
    "                                   kernel=\"epa\",\n",
    "                                   bw=\"silverman\",\n",
    "                                   norm=2,\n",
    "                                   machine_err=1e-12,\n",
    "                                   core_num=\"NOT DECLARED\",\n",
    "                                   multp=1):\n",
    "    \"\"\"\n",
    "    (Multiprocessing version) Take a (potentionally large) csv file to calculate the mutual information between outcome and covariates.\n",
    "    Both the outcome and the covariates should be continuous. \n",
    "    If _usecols is given, the returned mutual information will match _usecols. \n",
    "    By default, the left first covariate should be the outcome -- use _usecols to adjust if not the case.\n",
    "    \"\"\"\n",
    "    if core_num == \"NOT DECLARED\":\n",
    "        core_num = _mp.cpu_count()\n",
    "    else:\n",
    "        assert core_num <= _mp.cpu_count(\n",
    "        ), \"Declared number of cores used for multiprocessing should not exceed number of cores on this machine.\"\n",
    "    assert core_num >= 2, \"Multiprocessing should not be used on single-core machines.\"\n",
    "\n",
    "    if _np.array(_usecols).size == 0:\n",
    "        print(\n",
    "            \"Variable names not provided -- start reading variable names from csv file now, might take some time, depending on the csv file size.\"\n",
    "        )\n",
    "        _usecols = _pd.read_csv(csv_file, index_col=0,\n",
    "                                nrows=0).columns.tolist()\n",
    "        print(\"Reading variable names from csv file finished.\")\n",
    "    else:\n",
    "        _usecols = _np.array(_usecols)\n",
    "\n",
    "    def _continuous_filter_csv_slice(_slice):\n",
    "        _MI_slice = _np.zeros(\n",
    "            len(_slice))  # returned MI should be of the same length as slice\n",
    "        k = 0\n",
    "        for j in _slice:\n",
    "            __ = [\n",
    "                _usecols[0], _usecols[j]\n",
    "            ]  # here using _usecol[j] because only input variables indices were splitted\n",
    "            _ = _pd.read_csv(csv_file,\n",
    "                             skipinitialspace=True,\n",
    "                             usecols=__,\n",
    "                             encoding='unicode_escape').dropna()\n",
    "            _a = _[_usecols[0]].to_numpy()\n",
    "            _b = _[_usecols[j]].to_numpy()\n",
    "            _MI_slice[k] = MI_bivariate_continuous(a=_a,\n",
    "                                                   b=_b,\n",
    "                                                   a_N=a_N,\n",
    "                                                   b_N=b_N,\n",
    "                                                   kernel=kernel,\n",
    "                                                   bw=bw,\n",
    "                                                   norm=norm,\n",
    "                                                   machine_err=machine_err)\n",
    "            k += 1\n",
    "        return _MI_slice\n",
    "\n",
    "    # multiprocessing starts here\n",
    "    ind = _np.arange(\n",
    "        1, len(_usecols)\n",
    "    )  # starting from 1 because the first left column should be the outcome\n",
    "    with _mp.Pool(core_num) as pl:\n",
    "        MI_csv = pl.map(_continuous_filter_csv_slice,\n",
    "                        _np.array_split(ind, core_num * multp))\n",
    "    MI_csv = _np.hstack(MI_csv)\n",
    "    return MI_csv\n",
    "\n",
    "\n",
    "def Pearson_filter_csv_parallel(csv_file,\n",
    "                                _usecols=[],\n",
    "                                core_num=\"NOT DECLARED\",\n",
    "                                multp=1):\n",
    "    \"\"\"\n",
    "    (Multiprocessing version) Take a (potentionally large) csv file to calculate the Pearson's correlation between outcome and covariates.\n",
    "    If _usecols is given, the returned Pearson correlation will match _usecols. \n",
    "    By default, the left first covariate should be the outcome -- use _usecols to adjust if not the case.\n",
    "    This function accounts for missing data better than the Pearson's correlation matrix function provided by numpy.\n",
    "    \"\"\"\n",
    "    if core_num == \"NOT DECLARED\":\n",
    "        core_num = _mp.cpu_count()\n",
    "    else:\n",
    "        assert core_num <= _mp.cpu_count(\n",
    "        ), \"Declared number of cores used for multiprocessing should not exceed number of cores on this machine.\"\n",
    "    assert core_num >= 2, \"Multiprocessing should not be used on single-core machines.\"\n",
    "\n",
    "    if _np.array(_usecols).size == 0:\n",
    "        print(\n",
    "            \"Variable names not provided -- start reading variable names from csv file now, might take some time, depending on the csv file size.\"\n",
    "        )\n",
    "        _usecols = _pd.read_csv(csv_file, index_col=0,\n",
    "                                nrows=0).columns.tolist()\n",
    "        print(\"Reading variable names from csv file finished.\")\n",
    "    else:\n",
    "        _usecols = _np.array(_usecols)\n",
    "\n",
    "    def _Pearson_filter_csv_slice(_slice):\n",
    "        _pearson_slice = _np.zeros(\n",
    "            len(_slice))  # returned MI should be of the same length as slice\n",
    "        k = 0\n",
    "        for j in _slice:\n",
    "            __ = [\n",
    "                _usecols[0], _usecols[j]\n",
    "            ]  # here using _usecol[j] because only input variables indices were splitted\n",
    "            _ = _pd.read_csv(csv_file,\n",
    "                             skipinitialspace=True,\n",
    "                             usecols=__,\n",
    "                             encoding='unicode_escape').dropna()\n",
    "            _a = _[_usecols[0]].to_numpy()\n",
    "            _b = _[_usecols[j]].to_numpy()\n",
    "            # returned Pearson correlation is a symmetric matrix\n",
    "            _pearson_slice[k] = _np.corrcoef(_a, _b)[0, 1]\n",
    "            k += 1\n",
    "        return _pearson_slice\n",
    "\n",
    "    # multiprocessing starts here\n",
    "    ind = _np.arange(\n",
    "        1, len(_usecols)\n",
    "    )  # starting from 1 because the first left column should be the outcome\n",
    "    with _mp.Pool(core_num) as pl:\n",
    "        Pearson_csv = pl.map(_Pearson_filter_csv_slice,\n",
    "                             _np.array_split(ind, core_num * multp))\n",
    "    Pearson_csv = _np.hstack(Pearson_csv)\n",
    "    return Pearson_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a032b1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-14T04:38:15.032956Z",
     "start_time": "2023-01-14T04:37:45.278957Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two independant generalized Bern. r.v. realisations has MI estimate of  0.007404254352263242\n",
      "two independant generalized Gaussian r.v. realisations has MI estimate of  0.0022542407372067234\n",
      "MI estimate between a,a and a,b:  0.11484453582991862 0.001894221878096142\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.random.binomial(2, .3, 200)\n",
    "b = np.random.binomial(2, .3, 200)\n",
    "print(\n",
    "    \"two independant generalized Bern. r.v. realisations has MI estimate of \",\n",
    "    MI_binary_SNP(a, b))\n",
    "\n",
    "a = np.random.normal(size=2000)\n",
    "b = np.random.normal(size=2000)\n",
    "\n",
    "print(\n",
    "    \"two independant generalized Gaussian r.v. realisations has MI estimate of \",\n",
    "    MI_bivariate_continuous(a, b))\n",
    "a_MI = np.zeros(500)\n",
    "b_MI = np.zeros(500)\n",
    "for i in np.arange(500):\n",
    "    np.random.seed(i)\n",
    "    a = np.random.normal(size=2000)\n",
    "    b = np.random.normal(size=2000)\n",
    "    a_MI[i] = MI_bivariate_continuous(a, a)\n",
    "    b_MI[i] = MI_bivariate_continuous(a, b)\n",
    "print(\"MI estimate between a,a and a,b: \", np.mean(a_MI), np.mean(b_MI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b189ee8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-14T04:38:16.440983Z",
     "start_time": "2023-01-14T04:38:15.033959Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 1. ... 2. 1. 1.]\n",
      "[1. 2. 0. ... 0. 2. 1.]\n",
      "[2. 1. 2. ... 2. 2. 2.]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name '_np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\yang_\\anaconda3\\lib\\site-packages\\multiprocess\\pool.py\", line 121, in worker\n    result = (True, func(*args, **kwds))\n  File \"C:\\Users\\yang_\\anaconda3\\lib\\site-packages\\multiprocess\\pool.py\", line 44, in mapstar\n    return list(map(*args))\n  File \"C:\\Users\\yang_\\AppData\\Local\\Temp\\ipykernel_18884\\833411518.py\", line 114, in _continuous_filter_plink_slice\nNameError: name '_np' is not defined\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18884\\4122119183.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m                                                  \u001b[0moutcome\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutcome\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m                                                  \u001b[0moutcome_iid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutcome_iid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m                                                  machine_err=1e-8)\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMI_continuous\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18884\\833411518.py\u001b[0m in \u001b[0;36mcontinuous_filter_plink_parallel\u001b[1;34m(bed_file, bim_file, fam_file, outcome, outcome_iid, N, kernel, bw, machine_err, core_num, multp)\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0m_mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore_num\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpl\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m         MI_UKBB = pl.map(_continuous_filter_plink_slice,\n\u001b[1;32m--> 134\u001b[1;33m                          _np.array_split(ind, core_num * multp))\n\u001b[0m\u001b[0;32m    135\u001b[0m     \u001b[0mMI_UKBB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_np\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMI_UKBB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mMI_UKBB\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\multiprocess\\pool.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[1;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         '''\n\u001b[1;32m--> 268\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\multiprocess\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    655\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    656\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 657\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    658\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    659\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name '_np' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from bed_reader import open_bed\n",
    "\n",
    "# test for continuous_filter_parallel\n",
    "bed_file = r\"./fastHDMI/tests/sim/sim1.bed\"\n",
    "bim_file = r\"./fastHDMI/tests/sim/sim1.bim\"\n",
    "fam_file = r\"./fastHDMI/tests/sim/sim1.fam\"\n",
    "\n",
    "_bed = open_bed(filepath=bed_file,\n",
    "                fam_filepath=fam_file,\n",
    "                bim_filepath=bim_file)\n",
    "outcome = np.random.rand(_bed.iid_count)\n",
    "outcome_iid = _bed.iid\n",
    "\n",
    "true_beta = np.array([4.2, -2.5, 2.6])\n",
    "for j in np.arange(3):\n",
    "    outcome += true_beta[j] * _bed.read(np.s_[:, j], dtype=np.int8).flatten()\n",
    "    print(_bed.read(np.s_[:, j], dtype=np.float64).flatten())\n",
    "\n",
    "iid_ind = np.random.permutation(np.arange(_bed.iid_count))\n",
    "outcome = outcome[iid_ind]\n",
    "outcome_iid = outcome_iid[iid_ind]\n",
    "\n",
    "MI_continuous = continuous_filter_plink_parallel(bed_file=bed_file,\n",
    "                                                 bim_file=bim_file,\n",
    "                                                 fam_file=fam_file,\n",
    "                                                 outcome=outcome,\n",
    "                                                 outcome_iid=outcome_iid,\n",
    "                                                 machine_err=1e-8)\n",
    "\n",
    "assert np.all(MI_continuous > 0)\n",
    "print(MI_continuous[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2875be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-14T04:38:16.443985Z",
     "start_time": "2023-01-14T04:38:16.443985Z"
    }
   },
   "outputs": [],
   "source": [
    "bed_file = r\"./fastHDMI/tests/sim/sim1.bed\"\n",
    "bim_file = r\"./fastHDMI/tests/sim/sim1.bim\"\n",
    "fam_file = r\"./fastHDMI/tests/sim/sim1.fam\"\n",
    "\n",
    "_bed = open_bed(filepath=bed_file,\n",
    "                fam_filepath=fam_file,\n",
    "                bim_filepath=bim_file)\n",
    "outcome = np.random.rand(_bed.iid_count)\n",
    "outcome_iid = _bed.iid\n",
    "\n",
    "true_beta = np.array([4.2, -2.5, 2.6])\n",
    "for j in np.arange(3):\n",
    "    outcome += true_beta[j] * _bed.read(np.s_[:, j], dtype=np.int8).flatten()\n",
    "    print(_bed.read(np.s_[:, j], dtype=np.float64).flatten())\n",
    "\n",
    "outcome = np.random.binomial(1, np.tanh(outcome / 2) / 2 + .5)\n",
    "\n",
    "iid_ind = np.random.permutation(np.arange(_bed.iid_count))\n",
    "outcome = outcome[iid_ind]\n",
    "outcome_iid = outcome_iid[iid_ind]\n",
    "\n",
    "MI_binary = binary_filter_plink_parallel(bed_file=bed_file,\n",
    "                                         bim_file=bim_file,\n",
    "                                         fam_file=fam_file,\n",
    "                                         outcome=outcome,\n",
    "                                         outcome_iid=outcome_iid)\n",
    "\n",
    "assert np.all(MI_binary > 0)\n",
    "print(MI_binary[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0764c527",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-14T04:38:16.444958Z",
     "start_time": "2023-01-14T04:38:16.444958Z"
    }
   },
   "outputs": [],
   "source": [
    "# ## This is to generate the test csv files\n",
    "# import numpy as np\n",
    "# from scipy.linalg import toeplitz, block_diag\n",
    "# import pandas as pd\n",
    "\n",
    "# np.random.seed(123)\n",
    "# np.random.seed(np.around(np.random.rand(1) * 1e6, 3).astype(int))\n",
    "# N = 1000\n",
    "# SNR = 5.\n",
    "# true_beta = np.array([2, -2, 8, -8] + [0] * 1000)\n",
    "# X_cov = toeplitz(.6**np.arange(true_beta.shape[0]))\n",
    "# X_cov = np.asarray(X_cov)\n",
    "# mean = (np.random.rand(true_beta.shape[0]) - .5) * 100\n",
    "# X = np.random.multivariate_normal(mean, X_cov, N)\n",
    "# X /= np.std(X, 0)\n",
    "# X[:, [0, 169]] = X[:, [169, 0]]\n",
    "# X.ravel()[np.random.choice(X.size, int(X.size * .1), replace=False)] = np.nan\n",
    "\n",
    "# pd.DataFrame(X,\n",
    "#              columns=map(lambda x: \"Var \" + str(x), np.arange(\n",
    "#                  1, X.shape[1] +\n",
    "#                  1))).to_csv(r\"./fastHDMI/tests/sim/sim_continuous.csv\")\n",
    "\n",
    "# # this cell is for profiling the function\n",
    "# np.random.seed(321)\n",
    "# np.random.seed(np.around(np.random.rand(1) * 1e6, 3).astype(int))\n",
    "# N = 1000\n",
    "# SNR = 5.\n",
    "# true_beta = np.array([.5, -.5, .8, -.8] + [0] * 2000)\n",
    "# X_cov = toeplitz(.6**np.arange(true_beta.shape[0]))\n",
    "# X_cov = np.asarray(X_cov)\n",
    "# mean = (np.random.rand(true_beta.shape[0]) - .5) * 100\n",
    "# X = np.random.multivariate_normal(mean, X_cov, N)\n",
    "# X -= np.mean(X, 0)\n",
    "# X /= np.std(X, 0)\n",
    "# intercept_design_column = np.ones(N).reshape(N, 1)\n",
    "# X_sim = np.concatenate((intercept_design_column, X), 1)\n",
    "# true_sigma_sim = np.sqrt(true_beta.T @ X_cov @ true_beta / SNR)\n",
    "# true_beta_intercept = np.concatenate((np.array([0]), true_beta))\n",
    "# signal = X_sim @ true_beta_intercept + np.random.normal(0, true_sigma_sim, N)\n",
    "# y_sim = np.random.binomial(1, np.tanh(signal / 2) / 2 + .5)\n",
    "# X[:, [0, 231]] = X[:, [231, 0]]\n",
    "# X = np.concatenate((y_sim.reshape(-1, 1), X), 1)\n",
    "# X.ravel()[np.random.choice(X.size, int(X.size * .1), replace=False)] = np.nan\n",
    "\n",
    "# pd.DataFrame(X,\n",
    "#              columns=map(lambda x: \"Var \" + str(x), np.arange(\n",
    "#                  1, X.shape[1] +\n",
    "#                  1))).to_csv(r\"./fastHDMI/tests/sim/sim_binary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbd58c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-14T04:38:16.445958Z",
     "start_time": "2023-01-14T04:38:16.445958Z"
    }
   },
   "outputs": [],
   "source": [
    "# # single-thread continuous version\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# a = continuous_filter_csv(r\"./fastHDMI/tests/sim/sim_continuous.csv\")\n",
    "# plt.plot(a)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177a3dfc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-14T04:38:16.445958Z",
     "start_time": "2023-01-14T04:38:16.445958Z"
    }
   },
   "outputs": [],
   "source": [
    "# parallel continuous version\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "a = continuous_filter_csv_parallel(r\"./fastHDMI/tests/sim/sim_continuous.csv\")\n",
    "b = Pearson_filter_csv_parallel(r\"./fastHDMI/tests/sim/sim_continuous.csv\")\n",
    "plt.plot(a)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a42999",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-14T04:38:16.446956Z",
     "start_time": "2023-01-14T04:38:16.446956Z"
    }
   },
   "outputs": [],
   "source": [
    "# # single-thread binary version\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# a = binary_filter_csv(r\"./fastHDMI/tests/sim/sim_binary.csv\")\n",
    "# plt.plot(a)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9634a110",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-14T04:38:16.447957Z",
     "start_time": "2023-01-14T04:38:16.447957Z"
    }
   },
   "outputs": [],
   "source": [
    "# parallel binary version\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "a = binary_filter_csv_parallel(r\"./fastHDMI/tests/sim/sim_binary.csv\")\n",
    "plt.plot(a)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145f480f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
