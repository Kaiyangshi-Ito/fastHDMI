{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cc105c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-18T04:58:20.269413Z",
     "start_time": "2023-04-18T04:58:17.160126Z"
    }
   },
   "outputs": [],
   "source": [
    "import multiprocess as _mp\n",
    "import ctypes as _ctypes\n",
    "from sklearn.preprocessing import RobustScaler as _scaler\n",
    "from sklearn.feature_selection import mutual_info_regression as _mutual_info_regression\n",
    "from sklearn.feature_selection import mutual_info_classif as _mutual_info_classif\n",
    "from dask import dataframe as _dd\n",
    "import pandas as _pd\n",
    "from KDEpy import FFTKDE as _FFTKDE\n",
    "# from bed_reader import open_bed as _open_bed\n",
    "from numba import njit as _njit\n",
    "from numba import jit as _jit\n",
    "import numpy as _np\n",
    "from tqdm import tqdm as _tqdm\n",
    "import warnings as _warnings\n",
    "\n",
    "_warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d6146a",
   "metadata": {},
   "source": [
    "# basic functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0debf136",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-18T04:58:20.271270Z",
     "start_time": "2023-04-18T04:58:20.271263Z"
    }
   },
   "outputs": [],
   "source": [
    "def _open_bed():\n",
    "    print(\"bed_reader might have some bugs for this version, causing it not running\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2326f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-18T04:58:20.271937Z",
     "start_time": "2023-04-18T04:58:20.271929Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe3a614",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-18T04:58:20.272467Z",
     "start_time": "2023-04-18T04:58:20.272460Z"
    }
   },
   "outputs": [],
   "source": [
    "%%cython -a\n",
    "from libc.math cimport log, isfinite\n",
    "from libc.stdlib cimport calloc, free\n",
    "cimport cython\n",
    "from cython cimport floating\n",
    "\n",
    "ctypedef fused floating_float_double:\n",
    "    float\n",
    "    double\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "@cython.wraparound(False)\n",
    "def joint_to_mi_cython(floating_float_double[:, ::1] joint, floating_float_double forward_euler_a=1., floating_float_double forward_euler_b=1.):\n",
    "    cdef int i, j\n",
    "    cdef int joint_shape0 = joint.shape[0]\n",
    "    cdef int joint_shape1 = joint.shape[1]\n",
    "    cdef floating_float_double *log_a_marginal = <floating_float_double*>calloc(joint_shape0, sizeof(floating_float_double))\n",
    "    cdef floating_float_double *log_b_marginal = <floating_float_double*>calloc(joint_shape1, sizeof(floating_float_double))\n",
    "    cdef floating_float_double temp_sum, log_temp_sum, log_forward_euler_a, log_forward_euler_b, log_joint, output\n",
    "    \n",
    "    temp_sum = 0.0\n",
    "    for i in range(joint_shape0):\n",
    "        for j in range(joint_shape1):\n",
    "            log_a_marginal[i] += joint[i, j]\n",
    "            log_b_marginal[j] += joint[i, j]\n",
    "        temp_sum += log_a_marginal[i]\n",
    "    temp_sum *= forward_euler_a * forward_euler_b\n",
    "    log_temp_sum = log(temp_sum)\n",
    "    log_forward_euler_a = log(forward_euler_a)\n",
    "    log_forward_euler_b = log(forward_euler_b)\n",
    "    \n",
    "    for i in range(joint_shape0):\n",
    "        log_a_marginal[i] = log(log_a_marginal[i]) + log_forward_euler_b if isfinite(log(log_a_marginal[i])) else 0.0\n",
    "    for j in range(joint_shape1):\n",
    "        log_b_marginal[j] = log(log_b_marginal[j]) + log_forward_euler_a if isfinite(log(log_b_marginal[j])) else 0.0\n",
    "    \n",
    "    output = 0.0\n",
    "    for i in range(joint_shape0):\n",
    "        for j in range(joint_shape1):\n",
    "            log_joint = log(joint[i, j]) if isfinite(log(joint[i, j])) else 0.0\n",
    "            output += joint[i, j] * (log_joint - log_a_marginal[i] - log_b_marginal[j]) * forward_euler_a * forward_euler_b\n",
    "\n",
    "    output = max(output, 0.0)\n",
    "\n",
    "    free(log_a_marginal)\n",
    "    free(log_b_marginal)\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1741add3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-18T04:58:20.273318Z",
     "start_time": "2023-04-18T04:58:20.273311Z"
    }
   },
   "outputs": [],
   "source": [
    "# @_njit(cache=True)\n",
    "def _nan_inf_to_0(x):\n",
    "    \"\"\"\n",
    "    To convert NaN to 0 in nopython mode.\n",
    "    \"\"\"\n",
    "    return _np.where(_np.isfinite(x), x, 0.)\n",
    "\n",
    "\n",
    "# @_njit(cache=True)\n",
    "def _joint_to_mi(joint, forward_euler_a=1., forward_euler_b=1.):\n",
    "    # assume that joint likelihood is of shape (len(a), len(b))\n",
    "    # forward_euler step being 1. means discrete r.v.\n",
    "    # to scale the cdf to 1.\n",
    "    joint /= _np.sum(joint) * forward_euler_a * forward_euler_b\n",
    "    log_a_marginal = _np.log(_np.sum(joint, 1)) + _np.log(forward_euler_b)\n",
    "    log_a_marginal = _nan_inf_to_0(log_a_marginal)\n",
    "    log_b_marginal = _np.log(_np.sum(joint, 0)) + _np.log(forward_euler_a)\n",
    "    log_b_marginal = _nan_inf_to_0(log_b_marginal)\n",
    "    log_joint = _np.log(joint)\n",
    "    log_joint = _nan_inf_to_0(log_joint)\n",
    "    mi_temp = _np.sum(\n",
    "        joint *\n",
    "        (log_joint - log_a_marginal.reshape(-1, 1) -\n",
    "         log_b_marginal.reshape(1, -1))) * forward_euler_a * forward_euler_b\n",
    "\n",
    "    # this is to ensure that estimated MI is positive, to solve an numerical issue\n",
    "    mi_temp = _np.max(_np.array([mi_temp, 0.]))\n",
    "\n",
    "    return mi_temp\n",
    "\n",
    "\n",
    "def MI_continuous_012(a, b, N=500, kernel=\"epa\", bw=\"silverman\", **kwarg):\n",
    "    \"\"\"\n",
    "    calculate mutual information between continuous outcome and an SNP variable of 0,1,2\n",
    "    assume no missing data\n",
    "    \"\"\"\n",
    "    # first estimate the pmf\n",
    "    p0 = _np.count_nonzero(b == 0) / len(b)\n",
    "    p1 = _np.count_nonzero(b == 1) / len(b)\n",
    "    p2 = 1. - p0 - p1\n",
    "    _a = _scaler().fit_transform(a.reshape(-1, 1)).flatten()\n",
    "    # this step is just to get the boundary width for the joint density grid\n",
    "    # the three conditional density estimates need to be evaluated on the joint density grid\n",
    "    a_temp, _ = _FFTKDE(kernel=kernel, bw=bw, **kwarg).fit(data=_a).evaluate(N)\n",
    "    # estimate cond density\n",
    "    _b0 = (b == 0)\n",
    "    if _np.count_nonzero(_b0) > 2:\n",
    "        # here proceed to kde only if there are more than 5 data points\n",
    "        y_cond_p0 = _FFTKDE(kernel=kernel, bw=bw, **kwarg).fit(data=_a[_b0])\n",
    "    else:\n",
    "        y_cond_p0 = _np.zeros_like\n",
    "    _b1 = (b == 1)\n",
    "    if _np.count_nonzero(_b1) > 2:\n",
    "        y_cond_p1 = _FFTKDE(kernel=kernel, bw=bw, **kwarg).fit(data=_a[_b1])\n",
    "    else:\n",
    "        y_cond_p1 = _np.zeros_like\n",
    "    _b2 = (b == 2)\n",
    "    if _np.count_nonzero(_b2) > 2:\n",
    "        y_cond_p2 = _FFTKDE(kernel=kernel, bw=bw, **kwarg).fit(data=_a[_b2])\n",
    "    else:\n",
    "        y_cond_p2 = _np.zeros_like\n",
    "    joint = _np.zeros((N, 3))\n",
    "    joint[:, 0] = y_cond_p0(a_temp) * p0\n",
    "    joint[:, 1] = y_cond_p1(a_temp) * p1\n",
    "    joint[:, 2] = y_cond_p2(a_temp) * p2\n",
    "    forward_euler_step = a_temp[1] - a_temp[0]\n",
    "    mask = joint < 0.\n",
    "    joint[mask] = 0.\n",
    "\n",
    "    joint = _np.ascontiguousarray(joint)\n",
    "\n",
    "    mi_temp = joint_to_mi_cython(joint=joint,\n",
    "                                 forward_euler_a=forward_euler_step)\n",
    "\n",
    "    return mi_temp\n",
    "\n",
    "\n",
    "# @_njit(cache=True)\n",
    "def MI_binary_012(a, b):\n",
    "    \"\"\"\n",
    "    calculate mutual information between binary outcome and an SNP variable of 0,1,2\n",
    "    assume no missing data\n",
    "    \"\"\"\n",
    "    return MI_012_012(a, b)\n",
    "\n",
    "\n",
    "# @_njit(cache=True)\n",
    "def MI_012_012(a, b):\n",
    "    \"\"\"\n",
    "    calculate mutual information between two SNPs\n",
    "    assume no missing data\n",
    "    could be very useful for a MI-based clumping\n",
    "    \"\"\"\n",
    "    # estimate the cond pmf\n",
    "    joint = _np.zeros((3, 3))\n",
    "    _b0 = (b == 0)\n",
    "    joint[0, 0] = _np.count_nonzero(_np.logical_and(a == 0, _b0)) / len(a)\n",
    "    joint[1, 0] = _np.count_nonzero(_np.logical_and(a == 1, _b0)) / len(a)\n",
    "    joint[2, 0] = _np.count_nonzero(_np.logical_and(a == 2, _b0)) / len(a)\n",
    "    _b1 = (b == 1)\n",
    "    joint[0, 1] = _np.count_nonzero(_np.logical_and(a == 0, _b1)) / len(a)\n",
    "    joint[1, 1] = _np.count_nonzero(_np.logical_and(a == 1, _b1)) / len(a)\n",
    "    joint[2, 1] = _np.count_nonzero(_np.logical_and(a == 2, _b1)) / len(a)\n",
    "    _b2 = (b == 2)\n",
    "    joint[0, 2] = _np.count_nonzero(_np.logical_and(a == 0, _b2)) / len(a)\n",
    "    joint[1, 2] = _np.count_nonzero(_np.logical_and(a == 1, _b2)) / len(a)\n",
    "    joint[2, 2] = _np.count_nonzero(_np.logical_and(a == 2, _b2)) / len(a)\n",
    "\n",
    "    joint = _np.ascontiguousarray(joint)\n",
    "\n",
    "    mi_temp = joint_to_mi_cython(joint=joint)\n",
    "\n",
    "    return mi_temp\n",
    "\n",
    "\n",
    "def MI_continuous_continuous(a,\n",
    "                             b,\n",
    "                             a_N=300,\n",
    "                             b_N=300,\n",
    "                             kernel=\"epa\",\n",
    "                             bw=\"silverman\",\n",
    "                             norm=2,\n",
    "                             **kwarg):\n",
    "    \"\"\"\n",
    "    (Single Core version) calculate mutual information on bivariate continuous r.v..\n",
    "    \"\"\"\n",
    "    _temp = _np.argsort(a)\n",
    "    data = _np.hstack((a[_temp].reshape(-1, 1), b[_temp].reshape(-1, 1)))\n",
    "    _data = _scaler().fit_transform(data)\n",
    "    grid, joint = _FFTKDE(kernel=kernel, norm=norm,\n",
    "                          **kwarg).fit(_data).evaluate((a_N, b_N))\n",
    "    joint = joint.reshape(b_N, -1).T\n",
    "    # this gives joint as a (a_N, b_N) array, following example: https://kdepy.readthedocs.io/en/latest/examples.html#the-effect-of-norms-in-2d\n",
    "    a_forward_euler_step = grid[b_N, 0] - grid[0, 0]\n",
    "    b_forward_euler_step = grid[1, 1] - grid[0, 1]\n",
    "    mask = joint < 0.\n",
    "    joint[mask] = 0.\n",
    "\n",
    "    joint = _np.ascontiguousarray(joint)\n",
    "\n",
    "    mi_temp = joint_to_mi_cython(joint=joint,\n",
    "                                 forward_euler_a=a_forward_euler_step,\n",
    "                                 forward_euler_b=b_forward_euler_step)\n",
    "\n",
    "    return mi_temp\n",
    "\n",
    "\n",
    "def MI_binary_continuous(a, b, N=500, kernel=\"epa\", bw=\"silverman\", **kwarg):\n",
    "    return MI_continuous_012(a=b, b=a, N=N, kernel=kernel, bw=bw, **kwarg)\n",
    "\n",
    "\n",
    "@_njit(cache=True)\n",
    "def Pearson_to_MI_Gaussian(corr):\n",
    "    \"\"\"\n",
    "    Assuming the input variables are bivariate Gaussian, convert their Pearson correlatin to mutual information.\n",
    "    \"\"\"\n",
    "    return -.5 * (_np.log(1 + corr) + _np.log(1 - corr))\n",
    "\n",
    "\n",
    "@_njit(cache=True)\n",
    "def MI_to_Linfoot(mi):\n",
    "    \"\"\"\n",
    "    Convert calcualted mutual information estimator to Linfoot's measure of association.\n",
    "    \"\"\"\n",
    "    return (1. - _np.exp(-2. * mi))**.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec539665",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-18T04:58:20.274017Z",
     "start_time": "2023-04-18T04:58:20.274011Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cython is faster than Python implementation, even in our case of massive vectorization\n",
    "a, b = _np.random.rand(10000), _np.random.rand(3000)\n",
    "joint = a * b.reshape(-1, 1)\n",
    "%timeit joint_to_mi_cython(joint)\n",
    "%timeit _joint_to_mi(joint)\n",
    "\n",
    "joint_to_mi_cython(joint) == _joint_to_mi(joint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd816b0e",
   "metadata": {},
   "source": [
    "# For `plink` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce8b66d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-18T04:58:20.274620Z",
     "start_time": "2023-04-18T04:58:20.274613Z"
    }
   },
   "outputs": [],
   "source": [
    "# outcome_iid should be a  list of strings for identifiers\n",
    "def continuous_screening_plink(bed_file,\n",
    "                               bim_file,\n",
    "                               fam_file,\n",
    "                               outcome,\n",
    "                               outcome_iid,\n",
    "                               N=500,\n",
    "                               kernel=\"epa\",\n",
    "                               bw=\"silverman\",\n",
    "                               verbose=1,\n",
    "                               **kwarg):\n",
    "    \"\"\"\n",
    "    (Single Core version) take plink files to calculate the mutual information between the continuous outcome and many SNP variables.\n",
    "    \"\"\"\n",
    "    bed1 = _open_bed(filepath=bed_file,\n",
    "                     fam_filepath=fam_file,\n",
    "                     bim_filepath=bim_file)\n",
    "    gene_iid = _np.array(list(bed1.iid))\n",
    "    bed1_sid = _np.array(list(bed1.sid))\n",
    "    outcome = outcome[_np.intersect1d(outcome_iid,\n",
    "                                      gene_iid,\n",
    "                                      assume_unique=True,\n",
    "                                      return_indices=True)[1]]\n",
    "\n",
    "    # get genetic indices\n",
    "    gene_ind = _np.intersect1d(gene_iid,\n",
    "                               outcome_iid,\n",
    "                               assume_unique=True,\n",
    "                               return_indices=True)[1]\n",
    "\n",
    "    def _map_foo(j):\n",
    "        _SNP = bed1.read(_np.s_[:, j], dtype=_np.int8).flatten()\n",
    "        _SNP = _SNP[gene_ind]  # get gene iid also in outcome iid\n",
    "        _outcome = outcome[_SNP != -127]  # remove missing SNP in outcome\n",
    "        _SNP = _SNP[_SNP != -127]  # remove missing SNP\n",
    "        return MI_continuous_012(a=_outcome,\n",
    "                                 b=_SNP,\n",
    "                                 N=N,\n",
    "                                 kernel=kernel,\n",
    "                                 bw=bw,\n",
    "                                 **kwarg)\n",
    "\n",
    "    _iter = range(len(bed1_sid))\n",
    "    if verbose > 1:\n",
    "        _iter = _tqdm(iter)\n",
    "    MI_UKBB = _np.array(list(map(_map_foo, _iter)))\n",
    "    return MI_UKBB\n",
    "\n",
    "\n",
    "def binary_screening_plink(bed_file,\n",
    "                           bim_file,\n",
    "                           fam_file,\n",
    "                           outcome,\n",
    "                           outcome_iid,\n",
    "                           verbose=1,\n",
    "                           **kwarg):\n",
    "    \"\"\"\n",
    "    (Single Core version) take plink files to calculate the mutual information between the binary outcome and many SNP variables.\n",
    "    \"\"\"\n",
    "    bed1 = _open_bed(filepath=bed_file,\n",
    "                     fam_filepath=fam_file,\n",
    "                     bim_filepath=bim_file)\n",
    "    gene_iid = _np.array(list(bed1.iid))\n",
    "    bed1_sid = _np.array(list(bed1.sid))\n",
    "    outcome = outcome[_np.intersect1d(outcome_iid,\n",
    "                                      gene_iid,\n",
    "                                      assume_unique=True,\n",
    "                                      return_indices=True)[1]]\n",
    "    # get genetic indices\n",
    "    gene_ind = _np.intersect1d(gene_iid,\n",
    "                               outcome_iid,\n",
    "                               assume_unique=True,\n",
    "                               return_indices=True)[1]\n",
    "\n",
    "    def _map_foo(j):\n",
    "        _SNP = bed1.read(_np.s_[:, j], dtype=_np.int8).flatten()\n",
    "        _SNP = _SNP[gene_ind]  # get gene iid also in outcome iid\n",
    "        _outcome = outcome[_SNP != -127]  # remove missing SNP in outcome\n",
    "        _SNP = _SNP[_SNP != -127]  # remove missing SNP\n",
    "        return MI_binary_012(a=_outcome, b=_SNP, **kwarg)\n",
    "\n",
    "    _iter = range(len(bed1_sid))\n",
    "    if verbose >= 1:\n",
    "        _iter = _tqdm(_iter)\n",
    "    MI_UKBB = _np.array(list(map(_map_foo, _iter)))\n",
    "    return MI_UKBB\n",
    "\n",
    "\n",
    "def continuous_screening_plink_parallel(bed_file,\n",
    "                                        bim_file,\n",
    "                                        fam_file,\n",
    "                                        outcome,\n",
    "                                        outcome_iid,\n",
    "                                        N=500,\n",
    "                                        kernel=\"epa\",\n",
    "                                        bw=\"silverman\",\n",
    "                                        core_num=\"NOT DECLARED\",\n",
    "                                        multp=10,\n",
    "                                        verbose=1,\n",
    "                                        **kwarg):\n",
    "    \"\"\"\n",
    "    (Multiprocessing version) take plink files to calculate the mutual information between the continuous outcome and many SNP variables.\n",
    "    \"\"\"\n",
    "    # check some basic things\n",
    "    if core_num == \"NOT DECLARED\":\n",
    "        core_num = _mp.cpu_count()\n",
    "    else:\n",
    "        assert core_num <= _mp.cpu_count(\n",
    "        ), \"Declared number of cores used for multiprocessing should not exceed number of cores on this machine.\"\n",
    "    assert core_num >= 2, \"Multiprocessing should not be used on single-core machines.\"\n",
    "\n",
    "    # read some metadata\n",
    "    bed1 = _open_bed(filepath=bed_file,\n",
    "                     fam_filepath=fam_file,\n",
    "                     bim_filepath=bim_file)\n",
    "    gene_iid = _np.array(list(bed1.iid))\n",
    "    bed1_sid = _np.array(list(bed1.sid))\n",
    "    outcome = outcome[_np.intersect1d(outcome_iid,\n",
    "                                      gene_iid,\n",
    "                                      assume_unique=True,\n",
    "                                      return_indices=True)[1]]\n",
    "    # get genetic indices\n",
    "    gene_ind = _np.intersect1d(gene_iid,\n",
    "                               outcome_iid,\n",
    "                               assume_unique=True,\n",
    "                               return_indices=True)[1]\n",
    "\n",
    "    def _continuous_screening_plink_slice(_slice):\n",
    "        def _map_foo(j):\n",
    "            _SNP = bed1.read(_np.s_[:, j], dtype=_np.int8).flatten()\n",
    "            _SNP = _SNP[gene_ind]  # get gene iid also in outcome iid\n",
    "            _outcome = outcome[_SNP != -127]  # remove missing SNP in outcome\n",
    "            _SNP = _SNP[_SNP != -127]  # remove missing SNP\n",
    "            return MI_continuous_012(a=_outcome,\n",
    "                                     b=_SNP,\n",
    "                                     N=N,\n",
    "                                     kernel=kernel,\n",
    "                                     bw=bw,\n",
    "                                     **kwarg)\n",
    "\n",
    "        _MI_slice = _np.array(list(map(_map_foo, _slice)))\n",
    "        return _MI_slice\n",
    "\n",
    "    # multiprocessing starts here\n",
    "    ind = _np.arange(len(bed1_sid))\n",
    "    _iter = _np.array_split(ind, core_num * multp)\n",
    "    if verbose >= 1:\n",
    "        _iter = _tqdm(_iter)\n",
    "    with _mp.Pool(core_num) as pl:\n",
    "        MI_UKBB = pl.map(_continuous_screening_plink_slice, _iter)\n",
    "    MI_UKBB = _np.hstack(MI_UKBB)\n",
    "    return MI_UKBB\n",
    "\n",
    "\n",
    "def binary_screening_plink_parallel(bed_file,\n",
    "                                    bim_file,\n",
    "                                    fam_file,\n",
    "                                    outcome,\n",
    "                                    outcome_iid,\n",
    "                                    core_num=\"NOT DECLARED\",\n",
    "                                    multp=10,\n",
    "                                    verbose=1,\n",
    "                                    **kwarg):\n",
    "    \"\"\"\n",
    "    (Multiprocessing version) take plink files to calculate the mutual information between the binary outcome and many SNP variables.\n",
    "    \"\"\"\n",
    "    # check basic things\n",
    "    if core_num == \"NOT DECLARED\":\n",
    "        core_num = _mp.cpu_count()\n",
    "    else:\n",
    "        assert core_num <= _mp.cpu_count(\n",
    "        ), \"Declared number of cores used for multiprocessing should not exceed number of cores on this machine.\"\n",
    "    assert core_num >= 2, \"Multiprocessing should not be used on single-core machines.\"\n",
    "\n",
    "    # read some metadata\n",
    "    bed1 = _open_bed(filepath=bed_file,\n",
    "                     fam_filepath=fam_file,\n",
    "                     bim_filepath=bim_file)\n",
    "    gene_iid = _np.array(list(bed1.iid))\n",
    "    bed1_sid = _np.array(list(bed1.sid))\n",
    "    outcome = outcome[_np.intersect1d(outcome_iid,\n",
    "                                      gene_iid,\n",
    "                                      assume_unique=True,\n",
    "                                      return_indices=True)[1]]\n",
    "    # get genetic indices\n",
    "    gene_ind = _np.intersect1d(gene_iid,\n",
    "                               outcome_iid,\n",
    "                               assume_unique=True,\n",
    "                               return_indices=True)[1]\n",
    "\n",
    "    def _binary_screening_plink_slice(_slice):\n",
    "        def _map_foo(j):\n",
    "            _SNP = bed1.read(_np.s_[:, j], dtype=_np.int8).flatten()\n",
    "            _SNP = _SNP[gene_ind]  # get gene iid also in outcome iid\n",
    "            _outcome = outcome[_SNP != -127]  # remove missing SNP in outcome\n",
    "            _SNP = _SNP[_SNP != -127]  # remove missing SNP\n",
    "            return MI_binary_012(a=_outcome, b=_SNP, **kwarg)\n",
    "\n",
    "        _MI_slice = _np.array(list(map(_map_foo, _slice)))\n",
    "        return _MI_slice\n",
    "\n",
    "    # multiprocessing starts here\n",
    "    ind = _np.arange(len(bed1_sid))\n",
    "    _iter = _np.array_split(ind, core_num * multp)\n",
    "    if verbose > 1:\n",
    "        _iter = _tqdm(_iter)\n",
    "    with _mp.Pool(core_num) as pl:\n",
    "        MI_UKBB = pl.map(_binary_screening_plink_slice, _iter)\n",
    "    MI_UKBB = _np.hstack(MI_UKBB)\n",
    "    return MI_UKBB\n",
    "\n",
    "\n",
    "def clump_plink_parallel(bed_file,\n",
    "                         bim_file,\n",
    "                         fam_file,\n",
    "                         clumping_threshold=Pearson_to_MI_Gaussian(.6),\n",
    "                         num_SNPS_exam=_np.infty,\n",
    "                         core_num=\"NOT DECLARED\",\n",
    "                         multp=10,\n",
    "                         verbose=1):\n",
    "    \"\"\"\n",
    "    (Multiprocessing version) take plink files to calculate the mutual information between the binary outcome and many SNP variables.\n",
    "    \"\"\"\n",
    "    # check basic things\n",
    "    if core_num == \"NOT DECLARED\":\n",
    "        core_num = _mp.cpu_count()\n",
    "    else:\n",
    "        assert core_num <= _mp.cpu_count(\n",
    "        ), \"Declared number of cores used for multiprocessing should not exceed number of cores on this machine.\"\n",
    "    assert core_num >= 2, \"Multiprocessing should not be used on single-core machines.\"\n",
    "\n",
    "    # read some metadata\n",
    "    bed1 = _open_bed(filepath=bed_file,\n",
    "                     fam_filepath=fam_file,\n",
    "                     bim_filepath=bim_file)\n",
    "    bed1_sid = _np.array(list(bed1.sid))\n",
    "    if num_SNPS_exam == _np.infty:\n",
    "        num_SNPS_exam = len(bed1_sid) - 1\n",
    "    keep_cols = _np.arange(\n",
    "        len(bed1_sid))  # pruning by keeping all SNPS at the beginning\n",
    "    _iter = _np.arange(num_SNPS_exam)\n",
    "    if verbose >= 1:\n",
    "        _iter = _tqdm(_iter)\n",
    "    for current_var_ind in _iter:  # note that here _iter and keep_cols don't need to agree, by the break command comes later\n",
    "        if current_var_ind + 1 <= len(keep_cols):\n",
    "            outcome = bed1.read(_np.s_[:, current_var_ind],\n",
    "                                dtype=_np.int8).flatten()\n",
    "            gene_ind = _np.where(outcome != -127)\n",
    "            outcome = outcome[gene_ind]\n",
    "\n",
    "            def _012_012_plink_slice(_slice):\n",
    "                def _map_foo(j):\n",
    "                    _SNP = bed1.read(_np.s_[:, j], dtype=_np.int8).flatten()\n",
    "                    _SNP = _SNP[gene_ind]  # get gene iid also in outcome iid\n",
    "                    _outcome = outcome[_SNP !=\n",
    "                                       -127]  # remove missing SNP in outcome\n",
    "                    _SNP = _SNP[_SNP != -127]  # remove missing SNP\n",
    "                    return MI_012_012(a=_outcome, b=_SNP)\n",
    "\n",
    "                _MI_slice = _np.array(list(map(_map_foo, _slice)))\n",
    "                return _MI_slice\n",
    "\n",
    "            # multiprocessing starts here\n",
    "            ind = keep_cols[current_var_ind + 1:]\n",
    "            __iter = _np.array_split(ind, core_num * multp)\n",
    "            with _mp.Pool(core_num) as pl:\n",
    "                MI_UKBB = pl.map(_012_012_plink_slice, __iter)\n",
    "            MI_UKBB = _np.hstack(MI_UKBB)\n",
    "            keep_cols = _np.hstack(\n",
    "                (keep_cols[:current_var_ind + 1],\n",
    "                 keep_cols[current_var_ind +\n",
    "                           1:][MI_UKBB <= clumping_threshold]))\n",
    "        else:\n",
    "            break\n",
    "    return current_var_ind, bed1_sid[keep_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4508b94e",
   "metadata": {},
   "source": [
    "# For `csv` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fcb92a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-18T04:58:20.275181Z",
     "start_time": "2023-04-18T04:58:20.275174Z"
    }
   },
   "outputs": [],
   "source": [
    "def _read_csv(csv_file, _usecols, csv_engine, parquet_file, sample, verbose=1):\n",
    "    \"\"\"\n",
    "    Read a csv file using differnet engines. Use dask to read csv if low in memory.\n",
    "    \"\"\"\n",
    "    assert csv_engine in [\n",
    "        \"dask\", \"pyarrow\", \"fastparquet\", \"c\", \"python\"\n",
    "    ], \"Only dask and pandas csv engines or fastparquet are supported to read csv files.\"\n",
    "    if _np.array(_usecols).size == 0:\n",
    "        if verbose > 1:\n",
    "            print(\n",
    "                \"Variable names not provided -- start reading variable names from csv file now, might take some time, depending on the csv file size.\"\n",
    "            )\n",
    "        if csv_engine == \"dask\":\n",
    "            _df = _dd.read_csv(csv_file, sample=sample)\n",
    "            _usecols = _np.array(list(_df.columns)[1:])\n",
    "        elif csv_engine in [\"pyarrow\", \"c\",\n",
    "                            \"python\"]:  # these are pandas CSV engines\n",
    "            _df = _pd.read_csv(csv_file,\n",
    "                               encoding='unicode_escape',\n",
    "                               engine=csv_engine)\n",
    "            _usecols = _np.array(_df.columns.to_list()[1:])\n",
    "        elif csv_engine == \"fastparquet\":\n",
    "            _df = _pd.read_parquet(parquet_file, engine=\"fastparquet\")\n",
    "            _usecols = _np.array(_df.columns.to_list()[1:])\n",
    "        if verbose > 1:\n",
    "            print(\"Reading variable names from csv file finished.\")\n",
    "    else:\n",
    "        _usecols = _np.array(_usecols)\n",
    "        if csv_engine == \"dask\":\n",
    "            _df = _dd.read_csv(csv_file, names=_usecols, sample=sample)\n",
    "        elif csv_engine in [\"pyarrow\", \"c\", \"python\"]:\n",
    "            _df = _pd.read_csv(csv_file,\n",
    "                               encoding='unicode_escape',\n",
    "                               usecols=_usecols,\n",
    "                               engine=csv_engine)\n",
    "        elif csv_engine == \"fastparquet\":\n",
    "            _df = _pd.read_parquet(parquet_file,\n",
    "                                   engine=\"fastparquet\")[_usecols]\n",
    "    return _df, _usecols\n",
    "\n",
    "\n",
    "def _read_two_columns(_df, __, csv_engine):\n",
    "    \"\"\"\n",
    "    Read two columns from a dataframe object, remove NaN. Use dask to read csv if low in memory.\n",
    "    \"\"\"\n",
    "    if csv_engine == \"dask\":\n",
    "        _ = _np.asarray(_df[__].dropna().compute())\n",
    "    elif csv_engine in [\"pyarrow\", \"c\", \"python\",\n",
    "                        \"fastparquet\"]:  # these are engines using pandas\n",
    "        _ = _df[__].dropna().to_numpy()\n",
    "\n",
    "    _a = _[:, 0]\n",
    "    _b = _[:, 1]\n",
    "    return _a, _b\n",
    "\n",
    "\n",
    "def binary_screening_csv(csv_file=\"_\",\n",
    "                         _usecols=[],\n",
    "                         N=500,\n",
    "                         kernel=\"epa\",\n",
    "                         bw=\"silverman\",\n",
    "                         csv_engine=\"c\",\n",
    "                         parquet_file=\"_\",\n",
    "                         sample=256000,\n",
    "                         verbose=1,\n",
    "                         **kwarg):\n",
    "    \"\"\"\n",
    "    Take a (potentionally large) csv file to calculate the mutual information between outcome and covariates.\n",
    "    The outcome should be binary and the covariates be continuous. \n",
    "    If _usecols is given, the returned mutual information will match _usecols. \n",
    "    By default, the left first covariate should be the outcome -- use _usecols to adjust if not the case.\n",
    "    \"\"\"\n",
    "    assert csv_file != \"_\" or parquet_file != \"_\", \"CSV or parquet filepath should be declared\"\n",
    "    # outcome is the first variable by default; if other specifications are needed, put it the first item in _usecols\n",
    "    # read csv\n",
    "    _df, _usecols = _read_csv(csv_file=csv_file,\n",
    "                              _usecols=_usecols,\n",
    "                              csv_engine=csv_engine,\n",
    "                              parquet_file=parquet_file,\n",
    "                              sample=sample,\n",
    "                              verbose=verbose)\n",
    "\n",
    "    def _map_foo(j):\n",
    "        __ = [\n",
    "            _usecols[0], _usecols[j + 1]\n",
    "        ]  # here using _usecol[j + 1] because the left first column is the outcome\n",
    "        _a, _b = _read_two_columns(_df=_df, __=__, csv_engine=csv_engine)\n",
    "        return MI_binary_continuous(a=_a,\n",
    "                                    b=_b,\n",
    "                                    N=N,\n",
    "                                    kernel=kernel,\n",
    "                                    bw=bw,\n",
    "                                    **kwarg)\n",
    "\n",
    "    _iter = _np.arange(len(_usecols) - 1)\n",
    "    if verbose >= 1:\n",
    "        _iter = _tqdm(_iter)\n",
    "    MI_df = _np.array(list(map(_map_foo, _iter)))\n",
    "\n",
    "    del _df\n",
    "\n",
    "    return MI_df\n",
    "\n",
    "\n",
    "def continuous_screening_csv(csv_file=\"_\",\n",
    "                             _usecols=[],\n",
    "                             a_N=300,\n",
    "                             b_N=300,\n",
    "                             kernel=\"epa\",\n",
    "                             bw=\"silverman\",\n",
    "                             norm=2,\n",
    "                             csv_engine=\"c\",\n",
    "                             parquet_file=\"_\",\n",
    "                             sample=256000,\n",
    "                             verbose=1,\n",
    "                             **kwarg):\n",
    "    \"\"\"\n",
    "    Take a (potentionally large) csv file to calculate the mutual information between outcome and covariates.\n",
    "    Both the outcome and the covariates should be continuous. \n",
    "    If _usecols is given, the returned mutual information will match _usecols. \n",
    "    By default, the left first covariate should be the outcome -- use _usecols to adjust if not the case.\n",
    "    \"\"\"\n",
    "    assert csv_file != \"_\" or parquet_file != \"_\", \"CSV or parquet filepath should be declared\"\n",
    "    # read csv\n",
    "    _df, _usecols = _read_csv(csv_file=csv_file,\n",
    "                              _usecols=_usecols,\n",
    "                              csv_engine=csv_engine,\n",
    "                              parquet_file=parquet_file,\n",
    "                              sample=sample,\n",
    "                              verbose=verbose)\n",
    "\n",
    "    def _map_foo(j):\n",
    "        __ = [\n",
    "            _usecols[0], _usecols[j + 1]\n",
    "        ]  # here using _usecol[j + 1] because the left first column is the outcome\n",
    "        _a, _b = _read_two_columns(_df=_df, __=__, csv_engine=csv_engine)\n",
    "        return MI_continuous_continuous(a=_a,\n",
    "                                        b=_b,\n",
    "                                        a_N=a_N,\n",
    "                                        b_N=b_N,\n",
    "                                        kernel=kernel,\n",
    "                                        bw=bw,\n",
    "                                        norm=norm,\n",
    "                                        **kwarg)\n",
    "\n",
    "    _iter = _np.arange(len(_usecols) - 1)\n",
    "    if verbose >= 1:\n",
    "        _iter = _tqdm(_iter)\n",
    "    MI_df = _np.array(list(map(_map_foo, _iter)))\n",
    "\n",
    "    del _df\n",
    "\n",
    "    return MI_df\n",
    "\n",
    "\n",
    "def binary_screening_csv_parallel(csv_file=\"_\",\n",
    "                                  _usecols=[],\n",
    "                                  N=500,\n",
    "                                  kernel=\"epa\",\n",
    "                                  bw=\"silverman\",\n",
    "                                  core_num=\"NOT DECLARED\",\n",
    "                                  multp=10,\n",
    "                                  csv_engine=\"c\",\n",
    "                                  parquet_file=\"_\",\n",
    "                                  sample=256000,\n",
    "                                  verbose=1,\n",
    "                                  share_memory=True,\n",
    "                                  **kwarg):\n",
    "    \"\"\"\n",
    "    (Multiprocessing version) Take a (potentionally large) csv file to calculate the mutual information between outcome and covariates.\n",
    "    The outcome should be binary and the covariates be continuous. \n",
    "    If _usecols is given, the returned mutual information will match _usecols. \n",
    "    By default, the left first covariate should be the outcome -- use _usecols to adjust if not the case.\n",
    "    share_memory is to indicate whether to share the dataframe in memory to \n",
    "    multiple processes -- if set to False, each process will copy the entire dataframe respectively. However, \n",
    "    to read very large dataframe using dask, this option should usually be turned off.\n",
    "    \"\"\"\n",
    "    # check some basic things\n",
    "    assert csv_file != \"_\" or parquet_file != \"_\", \"CSV or parquet filepath should be declared\"\n",
    "\n",
    "    if core_num == \"NOT DECLARED\":\n",
    "        core_num = _mp.cpu_count()\n",
    "    else:\n",
    "        assert core_num <= _mp.cpu_count(\n",
    "        ), \"Declared number of cores used for multiprocessing should not exceed number of cores on this machine.\"\n",
    "    assert core_num >= 2, \"Multiprocessing should not be used on single-core machines.\"\n",
    "\n",
    "    # read csv\n",
    "    _df, _usecols = _read_csv(csv_file=csv_file,\n",
    "                              _usecols=_usecols,\n",
    "                              csv_engine=csv_engine,\n",
    "                              parquet_file=parquet_file,\n",
    "                              sample=sample,\n",
    "                              verbose=verbose)\n",
    "\n",
    "    # share_memory for multiprocess\n",
    "    if share_memory == True:\n",
    "        # the origingal dataframe is df, store the columns/dtypes pairs\n",
    "        df_dtypes_dict = dict(list(zip(_df.columns, _df.dtypes)))\n",
    "        # declare a shared Array with data from df\n",
    "        mparr = _mp.Array(_ctypes.c_double, _df.values.reshape(-1))\n",
    "        # create a new df based on the shared array\n",
    "        _df = _pd.DataFrame(_np.frombuffer(mparr.get_obj()).reshape(_df.shape),\n",
    "                            columns=_df.columns).astype(df_dtypes_dict)\n",
    "\n",
    "    def _binary_screening_csv_slice(_slice):\n",
    "        def _map_foo(j):\n",
    "            __ = [\n",
    "                _usecols[0], _usecols[j]\n",
    "            ]  # here using _usecol[j] because only input variables indices were splitted\n",
    "            _a, _b = _read_two_columns(_df=_df, __=__, csv_engine=csv_engine)\n",
    "            return MI_binary_continuous(a=_a,\n",
    "                                        b=_b,\n",
    "                                        N=N,\n",
    "                                        kernel=kernel,\n",
    "                                        bw=bw,\n",
    "                                        **kwarg)\n",
    "\n",
    "        _MI_slice = _np.array(list(map(_map_foo, _slice)))\n",
    "        return _MI_slice\n",
    "\n",
    "    # multiprocessing starts here\n",
    "\n",
    "    ind = _np.arange(\n",
    "        1, len(_usecols)\n",
    "    )  # starting from 1 because the first left column should be the outcome\n",
    "    _iter = _np.array_split(ind, core_num * multp)\n",
    "    if verbose >= 1:\n",
    "        _iter = _tqdm(_iter)\n",
    "    with _mp.Pool(core_num) as pl:\n",
    "        MI_df = pl.map(_binary_screening_csv_slice, _iter)\n",
    "    MI_df = _np.hstack(MI_df)\n",
    "\n",
    "    del _df\n",
    "\n",
    "    return MI_df\n",
    "\n",
    "\n",
    "def continuous_screening_csv_parallel(csv_file=\"_\",\n",
    "                                      _usecols=[],\n",
    "                                      a_N=300,\n",
    "                                      b_N=300,\n",
    "                                      kernel=\"epa\",\n",
    "                                      bw=\"silverman\",\n",
    "                                      norm=2,\n",
    "                                      core_num=\"NOT DECLARED\",\n",
    "                                      multp=10,\n",
    "                                      csv_engine=\"c\",\n",
    "                                      parquet_file=\"_\",\n",
    "                                      sample=256000,\n",
    "                                      verbose=1,\n",
    "                                      share_memory=True,\n",
    "                                      **kwarg):\n",
    "    \"\"\"\n",
    "    (Multiprocessing version) Take a (potentionally large) csv file to calculate the mutual information between outcome and covariates.\n",
    "    Both the outcome and the covariates should be continuous. \n",
    "    If _usecols is given, the returned mutual information will match _usecols. \n",
    "    By default, the left first covariate should be the outcome -- use _usecols to adjust if not the case.\n",
    "    share_memory is to indicate whether to share the dataframe in memory to \n",
    "    multiple processes -- if set to False, each process will copy the entire dataframe respectively. However, \n",
    "    to read very large dataframe using dask, this option should usually be turned off.\n",
    "    \"\"\"\n",
    "    # check some basic things\n",
    "    assert csv_file != \"_\" or parquet_file != \"_\", \"CSV or parquet filepath should be declared\"\n",
    "\n",
    "    if core_num == \"NOT DECLARED\":\n",
    "        core_num = _mp.cpu_count()\n",
    "    else:\n",
    "        assert core_num <= _mp.cpu_count(\n",
    "        ), \"Declared number of cores used for multiprocessing should not exceed number of cores on this machine.\"\n",
    "    assert core_num >= 2, \"Multiprocessing should not be used on single-core machines.\"\n",
    "\n",
    "    # read csv\n",
    "    _df, _usecols = _read_csv(csv_file=csv_file,\n",
    "                              _usecols=_usecols,\n",
    "                              csv_engine=csv_engine,\n",
    "                              parquet_file=parquet_file,\n",
    "                              sample=sample,\n",
    "                              verbose=verbose)\n",
    "\n",
    "    # share_memory for multiprocess\n",
    "    if share_memory == True:\n",
    "        # the origingal dataframe is df, store the columns/dtypes pairs\n",
    "        df_dtypes_dict = dict(list(zip(_df.columns, _df.dtypes)))\n",
    "        # declare a shared Array with data from df\n",
    "        mparr = _mp.Array(_ctypes.c_double, _df.values.reshape(-1))\n",
    "        # create a new df based on the shared array\n",
    "        _df = _pd.DataFrame(_np.frombuffer(mparr.get_obj()).reshape(_df.shape),\n",
    "                            columns=_df.columns).astype(df_dtypes_dict)\n",
    "\n",
    "    def _continuous_screening_csv_slice(_slice):\n",
    "        def _map_foo(j):\n",
    "            __ = [\n",
    "                _usecols[0], _usecols[j]\n",
    "            ]  # here using _usecol[j] because only input variables indices were splitted\n",
    "            _a, _b = _read_two_columns(_df=_df, __=__, csv_engine=csv_engine)\n",
    "            return MI_continuous_continuous(a=_a,\n",
    "                                            b=_b,\n",
    "                                            a_N=a_N,\n",
    "                                            b_N=b_N,\n",
    "                                            kernel=kernel,\n",
    "                                            bw=bw,\n",
    "                                            norm=norm,\n",
    "                                            **kwarg)\n",
    "\n",
    "        _MI_slice = _np.array(list(map(_map_foo, _slice)))\n",
    "        return _MI_slice\n",
    "\n",
    "    # multiprocessing starts here\n",
    "    ind = _np.arange(\n",
    "        1, len(_usecols)\n",
    "    )  # starting from 1 because the first left column should be the outcome\n",
    "\n",
    "    _iter = _np.array_split(ind, core_num * multp)\n",
    "    if verbose >= 1:\n",
    "        _iter = _tqdm(_iter)\n",
    "    with _mp.Pool(core_num) as pl:\n",
    "        MI_df = pl.map(_continuous_screening_csv_slice, _iter)\n",
    "    MI_df = _np.hstack(MI_df)\n",
    "\n",
    "    del _df\n",
    "\n",
    "    return MI_df\n",
    "\n",
    "\n",
    "def binary_skMI_screening_csv_parallel(csv_file=\"_\",\n",
    "                                       _usecols=[],\n",
    "                                       n_neighbors=3,\n",
    "                                       core_num=\"NOT DECLARED\",\n",
    "                                       multp=10,\n",
    "                                       csv_engine=\"c\",\n",
    "                                       parquet_file=\"_\",\n",
    "                                       sample=256000,\n",
    "                                       verbose=1,\n",
    "                                       share_memory=True,\n",
    "                                       **kwarg):\n",
    "    \"\"\"\n",
    "    (Multiprocessing version) Take a (potentionally large) csv file to calculate the mutual information between outcome and covariates.\n",
    "    Both the outcome and the covariates should be binary. \n",
    "    If _usecols is given, the returned mutual information will match _usecols. \n",
    "    By default, the left first covariate should be the outcome -- use _usecols to adjust if not the case.\n",
    "    share_memory is to indicate whether to share the dataframe in memory to \n",
    "    multiple processes -- if set to False, each process will copy the entire dataframe respectively. However, \n",
    "    to read very large dataframe using dask, this option should usually be turned off.\n",
    "    \"\"\"\n",
    "    # check some basic things\n",
    "    assert csv_file != \"_\" or parquet_file != \"_\", \"CSV or parquet filepath should be declared\"\n",
    "\n",
    "    if core_num == \"NOT DECLARED\":\n",
    "        core_num = _mp.cpu_count()\n",
    "    else:\n",
    "        assert core_num <= _mp.cpu_count(\n",
    "        ), \"Declared number of cores used for multiprocessing should not exceed number of cores on this machine.\"\n",
    "    assert core_num >= 2, \"Multiprocessing should not be used on single-core machines.\"\n",
    "\n",
    "    # read csv\n",
    "    _df, _usecols = _read_csv(csv_file=csv_file,\n",
    "                              _usecols=_usecols,\n",
    "                              csv_engine=csv_engine,\n",
    "                              parquet_file=parquet_file,\n",
    "                              sample=sample,\n",
    "                              verbose=verbose)\n",
    "\n",
    "    # share_memory for multiprocess\n",
    "    if share_memory == True:\n",
    "        # the origingal dataframe is df, store the columns/dtypes pairs\n",
    "        df_dtypes_dict = dict(list(zip(_df.columns, _df.dtypes)))\n",
    "        # declare a shared Array with data from df\n",
    "        mparr = _mp.Array(_ctypes.c_double, _df.values.reshape(-1))\n",
    "        # create a new df based on the shared array\n",
    "        _df = _pd.DataFrame(_np.frombuffer(mparr.get_obj()).reshape(_df.shape),\n",
    "                            columns=_df.columns).astype(df_dtypes_dict)\n",
    "\n",
    "    def _binary_skMI_df_slice(_slice):\n",
    "        def _map_foo(j):\n",
    "            __ = [\n",
    "                _usecols[0], _usecols[j]\n",
    "            ]  # here using _usecol[j] because only input variables indices were splitted\n",
    "            _a, _b = _read_two_columns(_df=_df, __=__, csv_engine=csv_engine)\n",
    "            return _mutual_info_classif(y=_a.reshape(-1, 1),\n",
    "                                        X=_b.reshape(-1, 1),\n",
    "                                        n_neighbors=n_neighbors,\n",
    "                                        discrete_features=False,\n",
    "                                        **kwarg)[0]\n",
    "\n",
    "        _MI_slice = _np.array(list(map(_map_foo, _slice)))\n",
    "        return _MI_slice\n",
    "\n",
    "    # multiprocessing starts here\n",
    "    ind = _np.arange(\n",
    "        1, len(_usecols)\n",
    "    )  # starting from 1 because the first left column should be the outcome\n",
    "\n",
    "    _iter = _np.array_split(ind, core_num * multp)\n",
    "    if verbose >= 1:\n",
    "        _iter = _tqdm(_iter)\n",
    "    with _mp.Pool(core_num) as pl:\n",
    "        MI_df = pl.map(_binary_skMI_df_slice, _iter)\n",
    "    MI_df = _np.hstack(MI_df)\n",
    "\n",
    "    del _df\n",
    "\n",
    "    return MI_df\n",
    "\n",
    "\n",
    "def continuous_skMI_screening_csv_parallel(csv_file=\"_\",\n",
    "                                           _usecols=[],\n",
    "                                           n_neighbors=3,\n",
    "                                           core_num=\"NOT DECLARED\",\n",
    "                                           multp=10,\n",
    "                                           csv_engine=\"c\",\n",
    "                                           parquet_file=\"_\",\n",
    "                                           sample=256000,\n",
    "                                           verbose=1,\n",
    "                                           share_memory=True,\n",
    "                                           **kwarg):\n",
    "    \"\"\"\n",
    "    (Multiprocessing version) Take a (potentionally large) csv file to calculate the mutual information between outcome and covariates.\n",
    "    Both the outcome and the covariates should be continuous. \n",
    "    If _usecols is given, the returned mutual information will match _usecols. \n",
    "    By default, the left first covariate should be the outcome -- use _usecols to adjust if not the case.\n",
    "    share_memory is to indicate whether to share the dataframe in memory to \n",
    "    multiple processes -- if set to False, each process will copy the entire dataframe respectively. However, \n",
    "    to read very large dataframe using dask, this option should usually be turned off.\n",
    "    \"\"\"\n",
    "    # check some basic things\n",
    "    assert csv_file != \"_\" or parquet_file != \"_\", \"CSV or parquet filepath should be declared\"\n",
    "\n",
    "    if core_num == \"NOT DECLARED\":\n",
    "        core_num = _mp.cpu_count()\n",
    "    else:\n",
    "        assert core_num <= _mp.cpu_count(\n",
    "        ), \"Declared number of cores used for multiprocessing should not exceed number of cores on this machine.\"\n",
    "    assert core_num >= 2, \"Multiprocessing should not be used on single-core machines.\"\n",
    "\n",
    "    # read csv\n",
    "    _df, _usecols = _read_csv(csv_file=csv_file,\n",
    "                              _usecols=_usecols,\n",
    "                              csv_engine=csv_engine,\n",
    "                              parquet_file=parquet_file,\n",
    "                              sample=sample,\n",
    "                              verbose=verbose)\n",
    "\n",
    "    # share_memory for multiprocess\n",
    "    if share_memory == True:\n",
    "        # the origingal dataframe is df, store the columns/dtypes pairs\n",
    "        df_dtypes_dict = dict(list(zip(_df.columns, _df.dtypes)))\n",
    "        # declare a shared Array with data from df\n",
    "        mparr = _mp.Array(_ctypes.c_double, _df.values.reshape(-1))\n",
    "        # create a new df based on the shared array\n",
    "        _df = _pd.DataFrame(_np.frombuffer(mparr.get_obj()).reshape(_df.shape),\n",
    "                            columns=_df.columns).astype(df_dtypes_dict)\n",
    "\n",
    "    def _continuous_skMI_df_slice(_slice):\n",
    "        def _map_foo(j):\n",
    "            __ = [\n",
    "                _usecols[0], _usecols[j]\n",
    "            ]  # here using _usecol[j] because only input variables indices were splitted\n",
    "            _a, _b = _read_two_columns(_df=_df, __=__, csv_engine=csv_engine)\n",
    "            return _mutual_info_regression(y=_a.reshape(-1, 1),\n",
    "                                           X=_b.reshape(-1, 1),\n",
    "                                           n_neighbors=n_neighbors,\n",
    "                                           discrete_features=False,\n",
    "                                           **kwarg)[0]\n",
    "\n",
    "        _MI_slice = _np.array(list(map(_map_foo, _slice)))\n",
    "        return _MI_slice\n",
    "\n",
    "    # multiprocessing starts here\n",
    "    ind = _np.arange(\n",
    "        1, len(_usecols)\n",
    "    )  # starting from 1 because the first left column should be the outcome\n",
    "\n",
    "    _iter = _np.array_split(ind, core_num * multp)\n",
    "    if verbose >= 1:\n",
    "        _iter = _tqdm(_iter)\n",
    "    with _mp.Pool(core_num) as pl:\n",
    "        MI_df = pl.map(_continuous_skMI_df_slice, _iter)\n",
    "    MI_df = _np.hstack(MI_df)\n",
    "\n",
    "    del _df\n",
    "\n",
    "    return MI_df\n",
    "\n",
    "\n",
    "def Pearson_screening_csv_parallel(csv_file=\"_\",\n",
    "                                   _usecols=[],\n",
    "                                   core_num=\"NOT DECLARED\",\n",
    "                                   multp=10,\n",
    "                                   csv_engine=\"c\",\n",
    "                                   parquet_file=\"_\",\n",
    "                                   sample=256000,\n",
    "                                   verbose=1,\n",
    "                                   share_memory=True):\n",
    "    \"\"\"\n",
    "    (Multiprocessing version) Take a (potentionally large) csv file to calculate the Pearson's correlation between outcome and covariates.\n",
    "    If _usecols is given, the returned Pearson correlation will match _usecols. \n",
    "    By default, the left first covariate should be the outcome -- use _usecols to adjust if not the case.\n",
    "    This function accounts for missing data better than the Pearson's correlation matrix function provided by numpy.\n",
    "    share_memory is to indicate whether to share the dataframe in memory to \n",
    "    multiple processes -- if set to False, each process will copy the entire dataframe respectively. However, \n",
    "    to read very large dataframe using dask, this option should usually be turned off.    \"\"\"\n",
    "    # check some basic things\n",
    "    assert csv_file != \"_\" or parquet_file != \"_\", \"CSV or parquet filepath should be declared\"\n",
    "\n",
    "    if core_num == \"NOT DECLARED\":\n",
    "        core_num = _mp.cpu_count()\n",
    "    else:\n",
    "        assert core_num <= _mp.cpu_count(\n",
    "        ), \"Declared number of cores used for multiprocessing should not exceed number of cores on this machine.\"\n",
    "    assert core_num >= 2, \"Multiprocessing should not be used on single-core machines.\"\n",
    "\n",
    "    # read csv\n",
    "    _df, _usecols = _read_csv(csv_file=csv_file,\n",
    "                              _usecols=_usecols,\n",
    "                              csv_engine=csv_engine,\n",
    "                              parquet_file=parquet_file,\n",
    "                              sample=sample,\n",
    "                              verbose=verbose)\n",
    "\n",
    "    # share_memory for multiprocess\n",
    "    if share_memory == True:\n",
    "        # the origingal dataframe is df, store the columns/dtypes pairs\n",
    "        df_dtypes_dict = dict(list(zip(_df.columns, _df.dtypes)))\n",
    "        # declare a shared Array with data from df\n",
    "        mparr = _mp.Array(_ctypes.c_double, _df.values.reshape(-1))\n",
    "        # create a new df based on the shared array\n",
    "        _df = _pd.DataFrame(_np.frombuffer(mparr.get_obj()).reshape(_df.shape),\n",
    "                            columns=_df.columns).astype(df_dtypes_dict)\n",
    "\n",
    "    def _Pearson_screening_df_slice(_slice):\n",
    "        def _map_foo(j):\n",
    "            __ = [\n",
    "                _usecols[0], _usecols[j]\n",
    "            ]  # here using _usecol[j] because only input variables indices were splitted\n",
    "            _a, _b = _read_two_columns(_df=_df, __=__, csv_engine=csv_engine)\n",
    "            # returned Pearson correlation is a symmetric matrix\n",
    "            _a -= _np.mean(_a)\n",
    "            _a /= _np.std(_a)\n",
    "            _b -= _np.mean(_b)\n",
    "            _b /= _np.std(_b)\n",
    "            #             return _np.corrcoef(_a, _b)[0, 1]\n",
    "            return _a @ _b / len(_a)\n",
    "\n",
    "        _pearson_slice = _np.array(list(map(_map_foo, _slice)))\n",
    "        return _pearson_slice\n",
    "\n",
    "    # multiprocessing starts here\n",
    "    ind = _np.arange(\n",
    "        1, len(_usecols)\n",
    "    )  # starting from 1 because the first left column should be the outcome\n",
    "\n",
    "    _iter = _np.array_split(ind, core_num * multp)\n",
    "    if verbose >= 1:\n",
    "        _iter = _tqdm(_iter)\n",
    "    with _mp.Pool(core_num) as pl:\n",
    "        Pearson_df = pl.map(_Pearson_screening_df_slice, _iter)\n",
    "    Pearson_df = _np.hstack(Pearson_df)\n",
    "\n",
    "    del _df\n",
    "\n",
    "    return Pearson_df\n",
    "\n",
    "\n",
    "def clump_continuous_csv_parallel(\n",
    "        csv_file=\"_\",\n",
    "        _usecols=[],\n",
    "        a_N=300,\n",
    "        b_N=300,\n",
    "        kernel=\"epa\",\n",
    "        bw=\"silverman\",\n",
    "        norm=2,\n",
    "        clumping_threshold=Pearson_to_MI_Gaussian(.6),\n",
    "        num_vars_exam=_np.infty,\n",
    "        core_num=\"NOT DECLARED\",\n",
    "        multp=10,\n",
    "        csv_engine=\"c\",\n",
    "        parquet_file=\"_\",\n",
    "        sample=256000,\n",
    "        verbose=1,\n",
    "        share_memory=True,\n",
    "        **kwarg):\n",
    "    \"\"\"\n",
    "    Perform clumping based on mutual information thresholding\n",
    "    The clumping process starts from the left to right, preserve input variables under the clumping threshold\n",
    "    share_memory is to indicate whether to share the dataframe in memory to \n",
    "    multiple processes -- if set to False, each process will copy the entire dataframe respectively. However, \n",
    "    to read very large dataframe using dask, this option should usually be turned off.    \"\"\"\n",
    "    # initialization\n",
    "    _, keep_cols = _read_csv(csv_file=csv_file,\n",
    "                             _usecols=_usecols,\n",
    "                             csv_engine=\"dask\",\n",
    "                             parquet_file=parquet_file,\n",
    "                             sample=sample,\n",
    "                             verbose=verbose)\n",
    "\n",
    "    del _\n",
    "\n",
    "    if num_vars_exam == _np.infty:\n",
    "        num_vars_exam = len(keep_cols) - 1\n",
    "    _iter = _np.arange(num_vars_exam)\n",
    "    if verbose >= 1:\n",
    "        _iter = _tqdm(_iter)\n",
    "    for current_var_ind in _iter:  # note that here _iter and keep_cols don't need to agree, by the break command comes later\n",
    "        if current_var_ind + 1 <= len(keep_cols):\n",
    "            _MI = continuous_screening_csv_parallel(\n",
    "                csv_file=csv_file,\n",
    "                _usecols=keep_cols[current_var_ind:],\n",
    "                core_num=core_num,\n",
    "                multp=multp,\n",
    "                csv_engine=csv_engine,\n",
    "                parquet_file=parquet_file,\n",
    "                sample=sample,\n",
    "                verbose=0,\n",
    "                share_memory=share_memory,\n",
    "                **kwarg)\n",
    "            # current_var_ind + 1 since the current variable will be included anyway\n",
    "            keep_cols = _np.hstack(\n",
    "                (keep_cols[:current_var_ind + 1],\n",
    "                 keep_cols[current_var_ind + 1:][_MI <= clumping_threshold]))\n",
    "        else:\n",
    "            break\n",
    "    return current_var_ind, keep_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed715174",
   "metadata": {},
   "source": [
    "## DataFrame version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa757e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-18T04:58:20.275826Z",
     "start_time": "2023-04-18T04:58:20.275820Z"
    }
   },
   "outputs": [],
   "source": [
    "def binary_screening_dataframe(dataframe=\"_\",\n",
    "                               _usecols=[],\n",
    "                               N=500,\n",
    "                               kernel=\"epa\",\n",
    "                               bw=\"silverman\",\n",
    "                               csv_engine=\"c\",\n",
    "                               verbose=1,\n",
    "                               **kwarg):\n",
    "    \"\"\"\n",
    "    Take a (potentionally large) csv file to calculate the mutual information between outcome and covariates.\n",
    "    The outcome should be binary and the covariates be continuous. \n",
    "    If _usecols is given, the returned mutual information will match _usecols. \n",
    "    By default, the left first covariate should be the outcome -- use _usecols to adjust if not the case.\n",
    "    \"\"\"\n",
    "    _df = dataframe\n",
    "    if _np.array(_usecols).size == 0:\n",
    "        _usecols = _np.array(_df.columns.to_list()[1:])\n",
    "    else:\n",
    "        _usecols = _np.array(_usecols)\n",
    "\n",
    "    def _map_foo(j):\n",
    "        __ = [\n",
    "            _usecols[0], _usecols[j + 1]\n",
    "        ]  # here using _usecol[j + 1] because the left first column is the outcome\n",
    "        _a, _b = _read_two_columns(_df=_df, __=__, csv_engine=csv_engine)\n",
    "        return MI_binary_continuous(a=_a,\n",
    "                                    b=_b,\n",
    "                                    N=N,\n",
    "                                    kernel=kernel,\n",
    "                                    bw=bw,\n",
    "                                    **kwarg)\n",
    "\n",
    "    _iter = _np.arange(len(_usecols) - 1)\n",
    "    if verbose >= 1:\n",
    "        _iter = _tqdm(_iter)\n",
    "    MI_df = _np.array(list(map(_map_foo, _iter)))\n",
    "\n",
    "    return MI_df\n",
    "\n",
    "\n",
    "def continuous_screening_dataframe(dataframe=\"_\",\n",
    "                                   _usecols=[],\n",
    "                                   a_N=300,\n",
    "                                   b_N=300,\n",
    "                                   kernel=\"epa\",\n",
    "                                   bw=\"silverman\",\n",
    "                                   norm=2,\n",
    "                                   csv_engine=\"c\",\n",
    "                                   verbose=1,\n",
    "                                   **kwarg):\n",
    "    \"\"\"\n",
    "    Take a (potentionally large) csv file to calculate the mutual information between outcome and covariates.\n",
    "    Both the outcome and the covariates should be continuous. \n",
    "    If _usecols is given, the returned mutual information will match _usecols. \n",
    "    By default, the left first covariate should be the outcome -- use _usecols to adjust if not the case.\n",
    "    \"\"\"\n",
    "    _df = dataframe\n",
    "    if _np.array(_usecols).size == 0:\n",
    "        _usecols = _np.array(_df.columns.to_list()[1:])\n",
    "    else:\n",
    "        _usecols = _np.array(_usecols)\n",
    "\n",
    "    def _map_foo(j):\n",
    "        __ = [\n",
    "            _usecols[0], _usecols[j + 1]\n",
    "        ]  # here using _usecol[j + 1] because the left first column is the outcome\n",
    "        _a, _b = _read_two_columns(_df=_df, __=__, csv_engine=csv_engine)\n",
    "        return MI_continuous_continuous(a=_a,\n",
    "                                        b=_b,\n",
    "                                        a_N=a_N,\n",
    "                                        b_N=b_N,\n",
    "                                        kernel=kernel,\n",
    "                                        bw=bw,\n",
    "                                        norm=norm,\n",
    "                                        **kwarg)\n",
    "\n",
    "    _iter = _np.arange(len(_usecols) - 1)\n",
    "    if verbose >= 1:\n",
    "        _iter = _tqdm(_iter)\n",
    "    MI_df = _np.array(list(map(_map_foo, _iter)))\n",
    "\n",
    "    return MI_df\n",
    "\n",
    "\n",
    "def binary_screening_dataframe_parallel(dataframe=\"_\",\n",
    "                                        _usecols=[],\n",
    "                                        N=500,\n",
    "                                        kernel=\"epa\",\n",
    "                                        bw=\"silverman\",\n",
    "                                        core_num=\"NOT DECLARED\",\n",
    "                                        multp=10,\n",
    "                                        csv_engine=\"c\",\n",
    "                                        verbose=1,\n",
    "                                        share_memory=True,\n",
    "                                        **kwarg):\n",
    "    \"\"\"\n",
    "    (Multiprocessing version) Take a (potentionally large) csv file to calculate the mutual information between outcome and covariates.\n",
    "    The outcome should be binary and the covariates be continuous. \n",
    "    If _usecols is given, the returned mutual information will match _usecols. \n",
    "    By default, the left first covariate should be the outcome -- use _usecols to adjust if not the case.\n",
    "    share_memory is to indicate whether to share the dataframe in memory to \n",
    "    multiple processes -- if set to False, each process will copy the entire dataframe respectively. However, \n",
    "    to read very large dataframe using dask, this option should usually be turned off.\n",
    "    \"\"\"\n",
    "    if core_num == \"NOT DECLARED\":\n",
    "        core_num = _mp.cpu_count()\n",
    "    else:\n",
    "        assert core_num <= _mp.cpu_count(\n",
    "        ), \"Declared number of cores used for multiprocessing should not exceed number of cores on this machine.\"\n",
    "    assert core_num >= 2, \"Multiprocessing should not be used on single-core machines.\"\n",
    "\n",
    "    _df = dataframe\n",
    "    if _np.array(_usecols).size == 0:\n",
    "        _usecols = _np.array(_df.columns.to_list()[1:])\n",
    "    else:\n",
    "        _usecols = _np.array(_usecols)\n",
    "\n",
    "    # share_memory for multiprocess\n",
    "    if share_memory == True:\n",
    "        # the origingal dataframe is df, store the columns/dtypes pairs\n",
    "        df_dtypes_dict = dict(list(zip(_df.columns, _df.dtypes)))\n",
    "        # declare a shared Array with data from df\n",
    "        mparr = _mp.Array(_ctypes.c_double, _df.values.reshape(-1))\n",
    "        # create a new df based on the shared array\n",
    "        _df = _pd.DataFrame(_np.frombuffer(mparr.get_obj()).reshape(_df.shape),\n",
    "                            columns=_df.columns).astype(df_dtypes_dict)\n",
    "\n",
    "    def _binary_screening_csv_slice(_slice):\n",
    "        def _map_foo(j):\n",
    "            __ = [\n",
    "                _usecols[0], _usecols[j]\n",
    "            ]  # here using _usecol[j] because only input variables indices were splitted\n",
    "            _a, _b = _read_two_columns(_df=_df, __=__, csv_engine=csv_engine)\n",
    "            return MI_binary_continuous(a=_a,\n",
    "                                        b=_b,\n",
    "                                        N=N,\n",
    "                                        kernel=kernel,\n",
    "                                        bw=bw,\n",
    "                                        **kwarg)\n",
    "\n",
    "        _MI_slice = _np.array(list(map(_map_foo, _slice)))\n",
    "        return _MI_slice\n",
    "\n",
    "    # multiprocessing starts here\n",
    "\n",
    "    ind = _np.arange(\n",
    "        1, len(_usecols)\n",
    "    )  # starting from 1 because the first left column should be the outcome\n",
    "    _iter = _np.array_split(ind, core_num * multp)\n",
    "    if verbose >= 1:\n",
    "        _iter = _tqdm(_iter)\n",
    "    with _mp.Pool(core_num) as pl:\n",
    "        MI_df = pl.map(_binary_screening_csv_slice, _iter)\n",
    "    MI_df = _np.hstack(MI_df)\n",
    "\n",
    "    return MI_df\n",
    "\n",
    "\n",
    "def continuous_screening_dataframe_parallel(dataframe=\"_\",\n",
    "                                            _usecols=[],\n",
    "                                            a_N=300,\n",
    "                                            b_N=300,\n",
    "                                            kernel=\"epa\",\n",
    "                                            bw=\"silverman\",\n",
    "                                            norm=2,\n",
    "                                            core_num=\"NOT DECLARED\",\n",
    "                                            multp=10,\n",
    "                                            csv_engine=\"c\",\n",
    "                                            verbose=1,\n",
    "                                            share_memory=True,\n",
    "                                            **kwarg):\n",
    "    \"\"\"\n",
    "    (Multiprocessing version) Take a (potentionally large) csv file to calculate the mutual information between outcome and covariates.\n",
    "    Both the outcome and the covariates should be continuous. \n",
    "    If _usecols is given, the returned mutual information will match _usecols. \n",
    "    By default, the left first covariate should be the outcome -- use _usecols to adjust if not the case.\n",
    "    share_memory is to indicate whether to share the dataframe in memory to \n",
    "    multiple processes -- if set to False, each process will copy the entire dataframe respectively. However, \n",
    "    to read very large dataframe using dask, this option should usually be turned off.\n",
    "    \"\"\"\n",
    "    if core_num == \"NOT DECLARED\":\n",
    "        core_num = _mp.cpu_count()\n",
    "    else:\n",
    "        assert core_num <= _mp.cpu_count(\n",
    "        ), \"Declared number of cores used for multiprocessing should not exceed number of cores on this machine.\"\n",
    "    assert core_num >= 2, \"Multiprocessing should not be used on single-core machines.\"\n",
    "\n",
    "    _df = dataframe\n",
    "    if _np.array(_usecols).size == 0:\n",
    "        _usecols = _np.array(_df.columns.to_list()[1:])\n",
    "    else:\n",
    "        _usecols = _np.array(_usecols)\n",
    "\n",
    "    # share_memory for multiprocess\n",
    "    if share_memory == True:\n",
    "        # the origingal dataframe is df, store the columns/dtypes pairs\n",
    "        df_dtypes_dict = dict(list(zip(_df.columns, _df.dtypes)))\n",
    "        # declare a shared Array with data from df\n",
    "        mparr = _mp.Array(_ctypes.c_double, _df.values.reshape(-1))\n",
    "        # create a new df based on the shared array\n",
    "        _df = _pd.DataFrame(_np.frombuffer(mparr.get_obj()).reshape(_df.shape),\n",
    "                            columns=_df.columns).astype(df_dtypes_dict)\n",
    "\n",
    "    def _continuous_screening_csv_slice(_slice):\n",
    "        def _map_foo(j):\n",
    "            __ = [\n",
    "                _usecols[0], _usecols[j]\n",
    "            ]  # here using _usecol[j] because only input variables indices were splitted\n",
    "            _a, _b = _read_two_columns(_df=_df, __=__, csv_engine=csv_engine)\n",
    "            return MI_continuous_continuous(a=_a,\n",
    "                                            b=_b,\n",
    "                                            a_N=a_N,\n",
    "                                            b_N=b_N,\n",
    "                                            kernel=kernel,\n",
    "                                            bw=bw,\n",
    "                                            norm=norm,\n",
    "                                            **kwarg)\n",
    "\n",
    "        _MI_slice = _np.array(list(map(_map_foo, _slice)))\n",
    "        return _MI_slice\n",
    "\n",
    "    # multiprocessing starts here\n",
    "    ind = _np.arange(\n",
    "        1, len(_usecols)\n",
    "    )  # starting from 1 because the first left column should be the outcome\n",
    "\n",
    "    _iter = _np.array_split(ind, core_num * multp)\n",
    "    if verbose >= 1:\n",
    "        _iter = _tqdm(_iter)\n",
    "    with _mp.Pool(core_num) as pl:\n",
    "        MI_df = pl.map(_continuous_screening_csv_slice, _iter)\n",
    "    MI_df = _np.hstack(MI_df)\n",
    "\n",
    "    return MI_df\n",
    "\n",
    "\n",
    "def binary_skMI_screening_dataframe_parallel(dataframe=\"_\",\n",
    "                                             _usecols=[],\n",
    "                                             n_neighbors=3,\n",
    "                                             core_num=\"NOT DECLARED\",\n",
    "                                             multp=10,\n",
    "                                             csv_engine=\"c\",\n",
    "                                             verbose=1,\n",
    "                                             share_memory=True,\n",
    "                                             **kwarg):\n",
    "    \"\"\"\n",
    "    (Multiprocessing version) Take a (potentionally large) csv file to calculate the mutual information between outcome and covariates.\n",
    "    Both the outcome and the covariates should be binary. \n",
    "    If _usecols is given, the returned mutual information will match _usecols. \n",
    "    By default, the left first covariate should be the outcome -- use _usecols to adjust if not the case.\n",
    "    share_memory is to indicate whether to share the dataframe in memory to \n",
    "    multiple processes -- if set to False, each process will copy the entire dataframe respectively. However, \n",
    "    to read very large dataframe using dask, this option should usually be turned off.\n",
    "    \"\"\"\n",
    "    if core_num == \"NOT DECLARED\":\n",
    "        core_num = _mp.cpu_count()\n",
    "    else:\n",
    "        assert core_num <= _mp.cpu_count(\n",
    "        ), \"Declared number of cores used for multiprocessing should not exceed number of cores on this machine.\"\n",
    "    assert core_num >= 2, \"Multiprocessing should not be used on single-core machines.\"\n",
    "\n",
    "    _df = dataframe\n",
    "    if _np.array(_usecols).size == 0:\n",
    "        _usecols = _np.array(_df.columns.to_list()[1:])\n",
    "    else:\n",
    "        _usecols = _np.array(_usecols)\n",
    "\n",
    "    # share_memory for multiprocess\n",
    "    if share_memory == True:\n",
    "        # the origingal dataframe is df, store the columns/dtypes pairs\n",
    "        df_dtypes_dict = dict(list(zip(_df.columns, _df.dtypes)))\n",
    "        # declare a shared Array with data from df\n",
    "        mparr = _mp.Array(_ctypes.c_double, _df.values.reshape(-1))\n",
    "        # create a new df based on the shared array\n",
    "        _df = _pd.DataFrame(_np.frombuffer(mparr.get_obj()).reshape(_df.shape),\n",
    "                            columns=_df.columns).astype(df_dtypes_dict)\n",
    "\n",
    "    def _binary_skMI_df_slice(_slice):\n",
    "        def _map_foo(j):\n",
    "            __ = [\n",
    "                _usecols[0], _usecols[j]\n",
    "            ]  # here using _usecol[j] because only input variables indices were splitted\n",
    "            _a, _b = _read_two_columns(_df=_df, __=__, csv_engine=csv_engine)\n",
    "            return _mutual_info_classif(y=_a.reshape(-1, 1),\n",
    "                                        X=_b.reshape(-1, 1),\n",
    "                                        n_neighbors=n_neighbors,\n",
    "                                        discrete_features=False,\n",
    "                                        **kwarg)[0]\n",
    "\n",
    "        _MI_slice = _np.array(list(map(_map_foo, _slice)))\n",
    "        return _MI_slice\n",
    "\n",
    "    # multiprocessing starts here\n",
    "    ind = _np.arange(\n",
    "        1, len(_usecols)\n",
    "    )  # starting from 1 because the first left column should be the outcome\n",
    "\n",
    "    _iter = _np.array_split(ind, core_num * multp)\n",
    "    if verbose >= 1:\n",
    "        _iter = _tqdm(_iter)\n",
    "    with _mp.Pool(core_num) as pl:\n",
    "        MI_df = pl.map(_binary_skMI_df_slice, _iter)\n",
    "    MI_df = _np.hstack(MI_df)\n",
    "\n",
    "    return MI_df\n",
    "\n",
    "\n",
    "def continuous_skMI_screening_dataframe_parallel(dataframe=\"_\",\n",
    "                                                 _usecols=[],\n",
    "                                                 n_neighbors=3,\n",
    "                                                 core_num=\"NOT DECLARED\",\n",
    "                                                 multp=10,\n",
    "                                                 csv_engine=\"c\",\n",
    "                                                 verbose=1,\n",
    "                                                 share_memory=True,\n",
    "                                                 **kwarg):\n",
    "    \"\"\"\n",
    "    (Multiprocessing version) Take a (potentionally large) csv file to calculate the mutual information between outcome and covariates.\n",
    "    Both the outcome and the covariates should be continuous. \n",
    "    If _usecols is given, the returned mutual information will match _usecols. \n",
    "    By default, the left first covariate should be the outcome -- use _usecols to adjust if not the case.\n",
    "    share_memory is to indicate whether to share the dataframe in memory to \n",
    "    multiple processes -- if set to False, each process will copy the entire dataframe respectively. However, \n",
    "    to read very large dataframe using dask, this option should usually be turned off.\n",
    "    \"\"\"\n",
    "    if core_num == \"NOT DECLARED\":\n",
    "        core_num = _mp.cpu_count()\n",
    "    else:\n",
    "        assert core_num <= _mp.cpu_count(\n",
    "        ), \"Declared number of cores used for multiprocessing should not exceed number of cores on this machine.\"\n",
    "    assert core_num >= 2, \"Multiprocessing should not be used on single-core machines.\"\n",
    "\n",
    "    _df = dataframe\n",
    "    if _np.array(_usecols).size == 0:\n",
    "        _usecols = _np.array(_df.columns.to_list()[1:])\n",
    "    else:\n",
    "        _usecols = _np.array(_usecols)\n",
    "\n",
    "    # share_memory for multiprocess\n",
    "    if share_memory == True:\n",
    "        # the origingal dataframe is df, store the columns/dtypes pairs\n",
    "        df_dtypes_dict = dict(list(zip(_df.columns, _df.dtypes)))\n",
    "        # declare a shared Array with data from df\n",
    "        mparr = _mp.Array(_ctypes.c_double, _df.values.reshape(-1))\n",
    "        # create a new df based on the shared array\n",
    "        _df = _pd.DataFrame(_np.frombuffer(mparr.get_obj()).reshape(_df.shape),\n",
    "                            columns=_df.columns).astype(df_dtypes_dict)\n",
    "\n",
    "    def _continuous_skMI_df_slice(_slice):\n",
    "        def _map_foo(j):\n",
    "            __ = [\n",
    "                _usecols[0], _usecols[j]\n",
    "            ]  # here using _usecol[j] because only input variables indices were splitted\n",
    "            _a, _b = _read_two_columns(_df=_df, __=__, csv_engine=csv_engine)\n",
    "            return _mutual_info_regression(y=_a.reshape(-1, 1),\n",
    "                                           X=_b.reshape(-1, 1),\n",
    "                                           n_neighbors=n_neighbors,\n",
    "                                           discrete_features=False,\n",
    "                                           **kwarg)[0]\n",
    "\n",
    "        _MI_slice = _np.array(list(map(_map_foo, _slice)))\n",
    "        return _MI_slice\n",
    "\n",
    "    # multiprocessing starts here\n",
    "    ind = _np.arange(\n",
    "        1, len(_usecols)\n",
    "    )  # starting from 1 because the first left column should be the outcome\n",
    "\n",
    "    _iter = _np.array_split(ind, core_num * multp)\n",
    "    if verbose >= 1:\n",
    "        _iter = _tqdm(_iter)\n",
    "    with _mp.Pool(core_num) as pl:\n",
    "        MI_df = pl.map(_continuous_skMI_df_slice, _iter)\n",
    "    MI_df = _np.hstack(MI_df)\n",
    "\n",
    "    return MI_df\n",
    "\n",
    "\n",
    "def Pearson_screening_dataframe_parallel(dataframe=\"_\",\n",
    "                                         _usecols=[],\n",
    "                                         core_num=\"NOT DECLARED\",\n",
    "                                         multp=10,\n",
    "                                         csv_engine=\"c\",\n",
    "                                         verbose=1,\n",
    "                                         share_memory=True):\n",
    "    \"\"\"\n",
    "    (Multiprocessing version) Take a (potentionally large) csv file to calculate the Pearson's correlation between outcome and covariates.\n",
    "    If _usecols is given, the returned Pearson correlation will match _usecols. \n",
    "    By default, the left first covariate should be the outcome -- use _usecols to adjust if not the case.\n",
    "    This function accounts for missing data better than the Pearson's correlation matrix function provided by numpy.\n",
    "    share_memory is to indicate whether to share the dataframe in memory to \n",
    "    multiple processes -- if set to False, each process will copy the entire dataframe respectively. However, \n",
    "    to read very large dataframe using dask, this option should usually be turned off.    \n",
    "    \"\"\"\n",
    "    if core_num == \"NOT DECLARED\":\n",
    "        core_num = _mp.cpu_count()\n",
    "    else:\n",
    "        assert core_num <= _mp.cpu_count(\n",
    "        ), \"Declared number of cores used for multiprocessing should not exceed number of cores on this machine.\"\n",
    "    assert core_num >= 2, \"Multiprocessing should not be used on single-core machines.\"\n",
    "\n",
    "    _df = dataframe\n",
    "    if _np.array(_usecols).size == 0:\n",
    "        _usecols = _np.array(_df.columns.to_list()[1:])\n",
    "    else:\n",
    "        _usecols = _np.array(_usecols)\n",
    "\n",
    "    # share_memory for multiprocess\n",
    "    if share_memory == True:\n",
    "        # the origingal dataframe is df, store the columns/dtypes pairs\n",
    "        df_dtypes_dict = dict(list(zip(_df.columns, _df.dtypes)))\n",
    "        # declare a shared Array with data from df\n",
    "        mparr = _mp.Array(_ctypes.c_double, _df.values.reshape(-1))\n",
    "        # create a new df based on the shared array\n",
    "        _df = _pd.DataFrame(_np.frombuffer(mparr.get_obj()).reshape(_df.shape),\n",
    "                            columns=_df.columns).astype(df_dtypes_dict)\n",
    "\n",
    "    def _Pearson_screening_df_slice(_slice):\n",
    "        def _map_foo(j):\n",
    "            __ = [\n",
    "                _usecols[0], _usecols[j]\n",
    "            ]  # here using _usecol[j] because only input variables indices were splitted\n",
    "            _a, _b = _read_two_columns(_df=_df, __=__, csv_engine=csv_engine)\n",
    "            # returned Pearson correlation is a symmetric matrix\n",
    "            _a -= _np.mean(_a)\n",
    "            _a /= _np.std(_a)\n",
    "            _b -= _np.mean(_b)\n",
    "            _b /= _np.std(_b)\n",
    "            #             return _np.corrcoef(_a, _b)[0, 1]\n",
    "            return _a @ _b / len(_a)\n",
    "\n",
    "        _pearson_slice = _np.array(list(map(_map_foo, _slice)))\n",
    "        return _pearson_slice\n",
    "\n",
    "    # multiprocessing starts here\n",
    "    ind = _np.arange(\n",
    "        1, len(_usecols)\n",
    "    )  # starting from 1 because the first left column should be the outcome\n",
    "\n",
    "    _iter = _np.array_split(ind, core_num * multp)\n",
    "    if verbose >= 1:\n",
    "        _iter = _tqdm(_iter)\n",
    "    with _mp.Pool(core_num) as pl:\n",
    "        Pearson_df = pl.map(_Pearson_screening_df_slice, _iter)\n",
    "    Pearson_df = _np.hstack(Pearson_df)\n",
    "\n",
    "    return Pearson_df\n",
    "\n",
    "\n",
    "def clump_continuous_dataframe_parallel(\n",
    "        dataframe=\"_\",\n",
    "        _usecols=[],\n",
    "        a_N=300,\n",
    "        b_N=300,\n",
    "        kernel=\"epa\",\n",
    "        bw=\"silverman\",\n",
    "        norm=2,\n",
    "        clumping_threshold=Pearson_to_MI_Gaussian(.6),\n",
    "        num_vars_exam=_np.infty,\n",
    "        core_num=\"NOT DECLARED\",\n",
    "        multp=10,\n",
    "        csv_engine=\"c\",\n",
    "        verbose=1,\n",
    "        share_memory=True,\n",
    "        **kwarg):\n",
    "    \"\"\"\n",
    "    Perform clumping based on mutual information thresholding\n",
    "    The clumping process starts from the left to right, preserve input variables under the clumping threshold\n",
    "    share_memory is to indicate whether to share the dataframe in memory to \n",
    "    multiple processes -- if set to False, each process will copy the entire dataframe respectively. However, \n",
    "    to read very large dataframe using dask, this option should usually be turned off.    \"\"\"\n",
    "    # initialization\n",
    "    _df = dataframe\n",
    "    if _np.array(_usecols).size == 0:\n",
    "        _usecols = _np.array(_df.columns.to_list()[1:])\n",
    "    else:\n",
    "        _usecols = _np.array(_usecols)\n",
    "\n",
    "    if num_vars_exam == _np.infty:\n",
    "        num_vars_exam = len(keep_cols) - 1\n",
    "    _iter = _np.arange(num_vars_exam)\n",
    "    if verbose >= 1:\n",
    "        _iter = _tqdm(_iter)\n",
    "    for current_var_ind in _iter:  # note that here _iter and keep_cols don't need to agree, by the break command comes later\n",
    "        if current_var_ind + 1 <= len(keep_cols):\n",
    "            _MI = continuous_screening_dataframe_parallel(\n",
    "                dataframe=dataframe,\n",
    "                _usecols=keep_cols[current_var_ind:],\n",
    "                core_num=core_num,\n",
    "                multp=multp,\n",
    "                csv_engine=csv_engine,\n",
    "                parquet_file=parquet_file,\n",
    "                sample=sample,\n",
    "                verbose=0,\n",
    "                share_memory=share_memory,\n",
    "                **kwarg)\n",
    "            # current_var_ind + 1 since the current variable will be included anyway\n",
    "            keep_cols = _np.hstack(\n",
    "                (keep_cols[:current_var_ind + 1],\n",
    "                 keep_cols[current_var_ind + 1:][_MI <= clumping_threshold]))\n",
    "        else:\n",
    "            break\n",
    "    return current_var_ind, keep_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9f2694",
   "metadata": {},
   "source": [
    "# For numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07043f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-18T04:58:20.276500Z",
     "start_time": "2023-04-18T04:58:20.276494Z"
    }
   },
   "outputs": [],
   "source": [
    "def continuous_skMI_array_parallel(X,\n",
    "                                   y,\n",
    "                                   drop_na=True,\n",
    "                                   n_neighbors=3,\n",
    "                                   core_num=\"NOT DECLARED\",\n",
    "                                   multp=10,\n",
    "                                   verbose=1,\n",
    "                                   **kwarg):\n",
    "    \"\"\"\n",
    "    (Multiprocessing version) Calculate the mutual information using sklearn implementation between outcome and covariates.\n",
    "    The outcome should be binary and the covariates be continuous. \n",
    "    If drop_na is set to be True, the NaN values will be dropped in a bivariate manner. \n",
    "    \"\"\"\n",
    "    # check some basic things\n",
    "    if core_num == \"NOT DECLARED\":\n",
    "        core_num = _mp.cpu_count()\n",
    "    else:\n",
    "        assert core_num <= _mp.cpu_count(\n",
    "        ), \"Declared number of cores used for multiprocessing should not exceed number of cores on this machine.\"\n",
    "    assert core_num >= 2, \"Multiprocessing should not be used on single-core machines.\"\n",
    "\n",
    "    def _continuous_skMI_array_slice(_slice):\n",
    "        def _map_foo(j):\n",
    "            _a, _b = y.copy(), X[:, j].copy()\n",
    "            if drop_na == True:\n",
    "                _keep = _np.logical_not(\n",
    "                    _np.logical_or(_np.isnan(_a), _np.isnan(_b)))\n",
    "                _a, _b = _a[_keep], _b[_keep]\n",
    "            return _mutual_info_regression(y=_a.reshape(-1, 1),\n",
    "                                           X=_b.reshape(-1, 1),\n",
    "                                           n_neighbors=n_neighbors,\n",
    "                                           discrete_features=False,\n",
    "                                           **kwarg)[0]\n",
    "\n",
    "        _MI_slice = _np.array(list(map(_map_foo, _slice)))\n",
    "        return _MI_slice\n",
    "\n",
    "    # multiprocessing starts here\n",
    "    ind = _np.arange(X.shape[1])\n",
    "    _iter = _np.array_split(ind, core_num * multp)\n",
    "    if verbose >= 1:\n",
    "        _iter = _tqdm(_iter)\n",
    "    with _mp.Pool(core_num) as pl:\n",
    "        MI_array = pl.map(_continuous_skMI_array_slice, _iter)\n",
    "    MI_array = _np.hstack(MI_array)\n",
    "    return MI_array\n",
    "\n",
    "\n",
    "def binary_screening_array(X,\n",
    "                           y,\n",
    "                           drop_na=True,\n",
    "                           N=500,\n",
    "                           kernel=\"epa\",\n",
    "                           bw=\"silverman\",\n",
    "                           verbose=1,\n",
    "                           **kwarg):\n",
    "    \"\"\"\n",
    "    Take a numpy file to calculate the mutual information between outcome and covariates.\n",
    "    The outcome should be binary and the covariates be continuous. \n",
    "    If drop_na is set to be True, the NaN values will be dropped in a bivariate manner. \n",
    "    \"\"\"\n",
    "    def _map_foo(j):\n",
    "        _a, _b = y.copy(), X[:, j].copy()\n",
    "        if drop_na == True:\n",
    "            _keep = _np.logical_not(\n",
    "                _np.logical_or(_np.isnan(_a), _np.isnan(_b)))\n",
    "            _a, _b = _a[_keep], _b[_keep]\n",
    "        return MI_binary_continuous(a=_a,\n",
    "                                    b=_b,\n",
    "                                    N=N,\n",
    "                                    kernel=kernel,\n",
    "                                    bw=bw,\n",
    "                                    **kwarg)\n",
    "\n",
    "    _iter = _np.arange(X.shape[1])\n",
    "    if verbose >= 1:\n",
    "        _iter = _tqdm(_iter)\n",
    "    MI_array = _np.array(list(map(_map_foo, _iter)))\n",
    "    return MI_array\n",
    "\n",
    "\n",
    "def continuous_screening_array(X,\n",
    "                               y,\n",
    "                               drop_na=True,\n",
    "                               a_N=300,\n",
    "                               b_N=300,\n",
    "                               kernel=\"epa\",\n",
    "                               bw=\"silverman\",\n",
    "                               norm=2,\n",
    "                               verbose=1,\n",
    "                               **kwarg):\n",
    "    \"\"\"\n",
    "    Take a numpy file to calculate the mutual information between outcome and covariates.\n",
    "    The outcome should be continuous and the covariates be continuous. \n",
    "    If drop_na is set to be True, the NaN values will be dropped in a bivariate manner. \n",
    "    \"\"\"\n",
    "    def _map_foo(j):\n",
    "        _a, _b = y.copy(), X[:, j].copy()\n",
    "        if drop_na == True:\n",
    "            _keep = _np.logical_not(\n",
    "                _np.logical_or(_np.isnan(_a), _np.isnan(_b)))\n",
    "            _a, _b = _a[_keep], _b[_keep]\n",
    "        return MI_continuous_continuous(a=_a,\n",
    "                                        b=_b,\n",
    "                                        a_N=a_N,\n",
    "                                        b_N=b_N,\n",
    "                                        kernel=kernel,\n",
    "                                        bw=bw,\n",
    "                                        norm=norm,\n",
    "                                        **kwarg)\n",
    "\n",
    "    _iter = _np.arange(X.shape[1])\n",
    "    if verbose >= 1:\n",
    "        _iter = _tqdm(_iter)\n",
    "    MI_array = _np.array(list(map(_map_foo, _iter)))\n",
    "    return MI_array\n",
    "\n",
    "\n",
    "def binary_screening_array_parallel(X,\n",
    "                                    y,\n",
    "                                    drop_na=True,\n",
    "                                    N=500,\n",
    "                                    kernel=\"epa\",\n",
    "                                    bw=\"silverman\",\n",
    "                                    core_num=\"NOT DECLARED\",\n",
    "                                    multp=10,\n",
    "                                    verbose=1,\n",
    "                                    **kwarg):\n",
    "    \"\"\"\n",
    "    (Multiprocessing version) Calculate the mutual information between outcome and covariates.\n",
    "    The outcome should be binary and the covariates be continuous. \n",
    "    If drop_na is set to be True, the NaN values will be dropped in a bivariate manner. \n",
    "    \"\"\"\n",
    "    # check some basic things\n",
    "    if core_num == \"NOT DECLARED\":\n",
    "        core_num = _mp.cpu_count()\n",
    "    else:\n",
    "        assert core_num <= _mp.cpu_count(\n",
    "        ), \"Declared number of cores used for multiprocessing should not exceed number of cores on this machine.\"\n",
    "    assert core_num >= 2, \"Multiprocessing should not be used on single-core machines.\"\n",
    "\n",
    "    def _binary_screening_array_slice(_slice):\n",
    "        def _map_foo(j):\n",
    "            _a, _b = y.copy(), X[:, j].copy()\n",
    "            if drop_na == True:\n",
    "                _keep = _np.logical_not(\n",
    "                    _np.logical_or(_np.isnan(_a), _np.isnan(_b)))\n",
    "                _a, _b = _a[_keep], _b[_keep]\n",
    "            return MI_binary_continuous(a=_a,\n",
    "                                        b=_b,\n",
    "                                        N=N,\n",
    "                                        kernel=kernel,\n",
    "                                        bw=bw,\n",
    "                                        **kwarg)\n",
    "\n",
    "        _MI_slice = _np.array(list(map(_map_foo, _slice)))\n",
    "        return _MI_slice\n",
    "\n",
    "    # multiprocessing starts here\n",
    "    ind = _np.arange(\n",
    "        X.shape[1]\n",
    "    )  # starting from 1 because the first left column should be the outcome\n",
    "    _iter = _np.array_split(ind, core_num * multp)\n",
    "    if verbose >= 1:\n",
    "        _iter = _tqdm(_iter)\n",
    "    with _mp.Pool(core_num) as pl:\n",
    "        MI_array = pl.map(_binary_screening_csv_slice, _iter)\n",
    "    MI_array = _np.hstack(MI_array)\n",
    "    return MI_array\n",
    "\n",
    "\n",
    "def continuous_screening_array_parallel(X,\n",
    "                                        y,\n",
    "                                        drop_na=True,\n",
    "                                        a_N=300,\n",
    "                                        b_N=300,\n",
    "                                        kernel=\"epa\",\n",
    "                                        bw=\"silverman\",\n",
    "                                        norm=2,\n",
    "                                        core_num=\"NOT DECLARED\",\n",
    "                                        multp=10,\n",
    "                                        verbose=1,\n",
    "                                        **kwarg):\n",
    "    \"\"\"\n",
    "    (Multiprocessing version) Calculate the mutual information between outcome and covariates.\n",
    "    The outcome should be continuous and the covariates be continuous. \n",
    "    If drop_na is set to be True, the NaN values will be dropped in a bivariate manner. \n",
    "    \"\"\"\n",
    "    # check some basic things\n",
    "    if core_num == \"NOT DECLARED\":\n",
    "        core_num = _mp.cpu_count()\n",
    "    else:\n",
    "        assert core_num <= _mp.cpu_count(\n",
    "        ), \"Declared number of cores used for multiprocessing should not exceed number of cores on this machine.\"\n",
    "    assert core_num >= 2, \"Multiprocessing should not be used on single-core machines.\"\n",
    "\n",
    "    def _continuous_screening_array_slice(_slice):\n",
    "        def _map_foo(j):\n",
    "            _a, _b = y.copy(), X[:, j].copy()\n",
    "            if drop_na == True:\n",
    "                _keep = _np.logical_not(\n",
    "                    _np.logical_or(_np.isnan(_a), _np.isnan(_b)))\n",
    "                _a, _b = _a[_keep], _b[_keep]\n",
    "            return MI_continuous_continuous(a=_a,\n",
    "                                            b=_b,\n",
    "                                            a_N=a_N,\n",
    "                                            b_N=b_N,\n",
    "                                            kernel=kernel,\n",
    "                                            bw=bw,\n",
    "                                            norm=norm,\n",
    "                                            **kwarg)\n",
    "\n",
    "        _MI_slice = _np.array(list(map(_map_foo, _slice)))\n",
    "        return _MI_slice\n",
    "\n",
    "    # multiprocessing starts here\n",
    "    ind = _np.arange(X.shape[1])\n",
    "    _iter = _np.array_split(ind, core_num * multp)\n",
    "    if verbose >= 1:\n",
    "        _iter = _tqdm(_iter)\n",
    "    with _mp.Pool(core_num) as pl:\n",
    "        MI_array = pl.map(_continuous_screening_array_slice, _iter)\n",
    "    MI_array = _np.hstack(MI_array)\n",
    "    return MI_array\n",
    "\n",
    "\n",
    "def binary_skMI_array_parallel(X,\n",
    "                               y,\n",
    "                               drop_na=True,\n",
    "                               n_neighbors=3,\n",
    "                               core_num=\"NOT DECLARED\",\n",
    "                               multp=10,\n",
    "                               verbose=1,\n",
    "                               **kwarg):\n",
    "    \"\"\"\n",
    "    (Multiprocessing version) Calculate the mutual information using sklearn implementation between outcome and covariates.\n",
    "    The outcome should be binary and the covariates be binary. \n",
    "    If drop_na is set to be True, the NaN values will be dropped in a bivariate manner. \n",
    "    \"\"\"\n",
    "    # check some basic things\n",
    "    if core_num == \"NOT DECLARED\":\n",
    "        core_num = _mp.cpu_count()\n",
    "    else:\n",
    "        assert core_num <= _mp.cpu_count(\n",
    "        ), \"Declared number of cores used for multiprocessing should not exceed number of cores on this machine.\"\n",
    "    assert core_num >= 2, \"Multiprocessing should not be used on single-core machines.\"\n",
    "\n",
    "    def _binary_skMI_array_slice(_slice):\n",
    "        def _map_foo(j):\n",
    "            _a, _b = y.copy(), X[:, j].copy()\n",
    "            if drop_na == True:\n",
    "                _keep = _np.logical_not(\n",
    "                    _np.logical_or(_np.isnan(_a), _np.isnan(_b)))\n",
    "                _a, _b = _a[_keep], _b[_keep]\n",
    "            return _mutual_info_classif(y=_a.reshape(-1, 1),\n",
    "                                        X=_b.reshape(-1, 1),\n",
    "                                        n_neighbors=n_neighbors,\n",
    "                                        discrete_features=False,\n",
    "                                        **kwarg)[0]\n",
    "\n",
    "        _MI_slice = _np.array(list(map(_map_foo, _slice)))\n",
    "        return _MI_slice\n",
    "\n",
    "    # multiprocessing starts here\n",
    "    ind = _np.arange(X.shape[1])\n",
    "    _iter = _np.array_split(ind, core_num * multp)\n",
    "    if verbose >= 1:\n",
    "        _iter = _tqdm(_iter)\n",
    "    with _mp.Pool(core_num) as pl:\n",
    "        MI_array = pl.map(_binary_skMI_array_slice, _iter)\n",
    "    MI_array = _np.hstack(MI_array)\n",
    "    return MI_array\n",
    "\n",
    "\n",
    "def continuous_skMI_array_parallel(X,\n",
    "                                   y,\n",
    "                                   drop_na=True,\n",
    "                                   n_neighbors=3,\n",
    "                                   core_num=\"NOT DECLARED\",\n",
    "                                   multp=10,\n",
    "                                   verbose=1,\n",
    "                                   **kwarg):\n",
    "    \"\"\"\n",
    "    (Multiprocessing version) Calculate the mutual information using sklearn implementation between outcome and covariates.\n",
    "    The outcome should be binary and the covariates be continuous. \n",
    "    If drop_na is set to be True, the NaN values will be dropped in a bivariate manner. \n",
    "    \"\"\"\n",
    "    # check some basic things\n",
    "    if core_num == \"NOT DECLARED\":\n",
    "        core_num = _mp.cpu_count()\n",
    "    else:\n",
    "        assert core_num <= _mp.cpu_count(\n",
    "        ), \"Declared number of cores used for multiprocessing should not exceed number of cores on this machine.\"\n",
    "    assert core_num >= 2, \"Multiprocessing should not be used on single-core machines.\"\n",
    "\n",
    "    def _continuous_skMI_array_slice(_slice):\n",
    "        def _map_foo(j):\n",
    "            _a, _b = y.copy(), X[:, j].copy()\n",
    "            if drop_na == True:\n",
    "                _keep = _np.logical_not(\n",
    "                    _np.logical_or(_np.isnan(_a), _np.isnan(_b)))\n",
    "                _a, _b = _a[_keep], _b[_keep]\n",
    "            return _mutual_info_regression(y=_a.reshape(-1, 1),\n",
    "                                           X=_b.reshape(-1, 1),\n",
    "                                           n_neighbors=n_neighbors,\n",
    "                                           discrete_features=False,\n",
    "                                           **kwarg)[0]\n",
    "\n",
    "        _MI_slice = _np.array(list(map(_map_foo, _slice)))\n",
    "        return _MI_slice\n",
    "\n",
    "    # multiprocessing starts here\n",
    "    ind = _np.arange(X.shape[1])\n",
    "    _iter = _np.array_split(ind, core_num * multp)\n",
    "    if verbose >= 1:\n",
    "        _iter = _tqdm(_iter)\n",
    "    with _mp.Pool(core_num) as pl:\n",
    "        MI_array = pl.map(_continuous_skMI_array_slice, _iter)\n",
    "    MI_array = _np.hstack(MI_array)\n",
    "    return MI_array\n",
    "\n",
    "\n",
    "def continuous_Pearson_array_parallel(X,\n",
    "                                      y,\n",
    "                                      drop_na=True,\n",
    "                                      n_neighbors=3,\n",
    "                                      core_num=\"NOT DECLARED\",\n",
    "                                      multp=10,\n",
    "                                      verbose=1):\n",
    "    \"\"\"\n",
    "    (Multiprocessing version) Calculate the mutual information using sklearn implementation between outcome and covariates.\n",
    "    The outcome should be binary and the covariates be continuous. \n",
    "    If drop_na is set to be True, the NaN values will be dropped in a bivariate manner. \n",
    "    \"\"\"\n",
    "    # check some basic things\n",
    "    if core_num == \"NOT DECLARED\":\n",
    "        core_num = _mp.cpu_count()\n",
    "    else:\n",
    "        assert core_num <= _mp.cpu_count(\n",
    "        ), \"Declared number of cores used for multiprocessing should not exceed number of cores on this machine.\"\n",
    "    assert core_num >= 2, \"Multiprocessing should not be used on single-core machines.\"\n",
    "\n",
    "    def _continuous_Pearson_array_slice(_slice):\n",
    "        def _map_foo(j):\n",
    "            _a, _b = y.copy(), X[:, j].copy()\n",
    "            if drop_na == True:\n",
    "                _keep = _np.logical_not(\n",
    "                    _np.logical_or(_np.isnan(_a), _np.isnan(_b)))\n",
    "                _a, _b = _a[_keep], _b[_keep]\n",
    "            _a -= _np.mean(_a)\n",
    "            _a /= _np.std(_a)\n",
    "            _b -= _np.mean(_b)\n",
    "            _b /= _np.std(_b)\n",
    "            return _a @ _b / len(_a)\n",
    "\n",
    "        _MI_slice = _np.array(list(map(_map_foo, _slice)))\n",
    "        return _MI_slice\n",
    "\n",
    "    # multiprocessing starts here\n",
    "    ind = _np.arange(X.shape[1])\n",
    "    _iter = _np.array_split(ind, core_num * multp)\n",
    "    if verbose >= 1:\n",
    "        _iter = _tqdm(_iter)\n",
    "    with _mp.Pool(core_num) as pl:\n",
    "        MI_array = pl.map(_continuous_Pearson_array_slice, _iter)\n",
    "    MI_array = _np.hstack(MI_array)\n",
    "    return MI_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd99543",
   "metadata": {},
   "source": [
    "# Tests\n",
    "## test for basic functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a032b1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-18T04:58:20.277242Z",
     "start_time": "2023-04-18T04:58:20.277236Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# from bed_reader import open_bed\n",
    "# import cupy as cp\n",
    "from scipy.linalg import toeplitz, block_diag\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "a = np.random.binomial(2, .3, 2000)\n",
    "b = np.random.binomial(2, .3, 2000)\n",
    "print(\n",
    "    \"two independant generalized Bern. r.v. realisations has MI estimate of \",\n",
    "    MI_binary_012(a, b))\n",
    "print(\n",
    "    \"two independant generalized Bern. r.v. realisations has MI estimate of \",\n",
    "    MI_012_012(a, b))\n",
    "\n",
    "a = np.random.normal(size=2000)\n",
    "b = np.random.normal(size=2000)\n",
    "\n",
    "print(\n",
    "    \"two independant generalized Gaussian r.v. realisations has MI estimate of \",\n",
    "    MI_continuous_continuous(a, b))\n",
    "a_MI = np.zeros(500)\n",
    "b_MI = np.zeros(500)\n",
    "for i in np.arange(500):\n",
    "    np.random.seed(i)\n",
    "    a = np.random.normal(size=2000)\n",
    "    b = np.random.normal(size=2000)\n",
    "    a_MI[i] = MI_continuous_continuous(a, a)\n",
    "    b_MI[i] = MI_continuous_continuous(a, b)\n",
    "print(\"MI estimate between a,a and a,b: \", np.mean(a_MI), np.mean(b_MI))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dc9ae9",
   "metadata": {},
   "source": [
    "## test for `plink` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca5549d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-18T04:58:20.277810Z",
     "start_time": "2023-04-18T04:58:20.277802Z"
    }
   },
   "outputs": [],
   "source": [
    "# # test for continuous screening on plink files\n",
    "# bed_file = r\"./fastHDMI/tests/sim/sim1.bed\"\n",
    "# bim_file = r\"./fastHDMI/tests/sim/sim1.bim\"\n",
    "# fam_file = r\"./fastHDMI/tests/sim/sim1.fam\"\n",
    "\n",
    "# _bed = open_bed(filepath=bed_file,\n",
    "#                 fam_filepath=fam_file,\n",
    "#                 bim_filepath=bim_file)\n",
    "# outcome = np.random.rand(_bed.iid_count)\n",
    "# outcome_iid = _bed.iid\n",
    "\n",
    "# true_beta = np.array([4.2, -2.5, 2.6])\n",
    "# for j in np.arange(3):\n",
    "#     outcome += true_beta[j] * _bed.read(np.s_[:, j], dtype=np.int8).flatten()\n",
    "#     print(_bed.read(np.s_[:, j], dtype=np.float64).flatten())\n",
    "\n",
    "# iid_ind = np.random.permutation(np.arange(_bed.iid_count))\n",
    "# outcome = outcome[iid_ind]\n",
    "# outcome_iid = outcome_iid[iid_ind]\n",
    "\n",
    "# MI_continuous = continuous_screening_plink_parallel(bed_file=bed_file,\n",
    "#                                                     bim_file=bim_file,\n",
    "#                                                     fam_file=fam_file,\n",
    "#                                                     outcome=outcome,\n",
    "#                                                     outcome_iid=outcome_iid, verbose=2)\n",
    "# assert np.all(MI_continuous >= 0)\n",
    "\n",
    "# plt.plot(MI_continuous)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5026fad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-18T04:58:20.278317Z",
     "start_time": "2023-04-18T04:58:20.278311Z"
    }
   },
   "outputs": [],
   "source": [
    "# # testing for binary plink files screening\n",
    "# bed_file = r\"./fastHDMI/tests/sim/sim1.bed\"\n",
    "# bim_file = r\"./fastHDMI/tests/sim/sim1.bim\"\n",
    "# fam_file = r\"./fastHDMI/tests/sim/sim1.fam\"\n",
    "\n",
    "# _bed = open_bed(filepath=bed_file,\n",
    "#                 fam_filepath=fam_file,\n",
    "#                 bim_filepath=bim_file)\n",
    "# outcome = np.random.rand(_bed.iid_count)\n",
    "# outcome_iid = _bed.iid\n",
    "\n",
    "# true_beta = np.array([4.2, -2.5, 2.6])\n",
    "# for j in np.arange(3):\n",
    "#     outcome += true_beta[j] * _bed.read(np.s_[:, j], dtype=np.int8).flatten()\n",
    "#     print(_bed.read(np.s_[:, j], dtype=np.float64).flatten())\n",
    "\n",
    "# outcome = np.random.binomial(1, np.tanh(outcome / 2) / 2 + .5)\n",
    "\n",
    "# iid_ind = np.random.permutation(np.arange(_bed.iid_count))\n",
    "# outcome = outcome[iid_ind]\n",
    "# outcome_iid = outcome_iid[iid_ind]\n",
    "\n",
    "# MI_binary = binary_screening_plink_parallel(bed_file=bed_file,\n",
    "#                                             bim_file=bim_file,\n",
    "#                                             fam_file=fam_file,\n",
    "#                                             outcome=outcome,\n",
    "#                                             outcome_iid=outcome_iid, verbose=2)\n",
    "# assert np.all(MI_binary >= 0)\n",
    "\n",
    "# plt.plot(MI_binary)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d657c60c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-18T04:58:20.278765Z",
     "start_time": "2023-04-18T04:58:20.278759Z"
    }
   },
   "outputs": [],
   "source": [
    "# # test for clumping for plink files\n",
    "# bed_file = r\"./fastHDMI/tests/sim/sim1.bed\"\n",
    "# bim_file = r\"./fastHDMI/tests/sim/sim1.bim\"\n",
    "# fam_file = r\"./fastHDMI/tests/sim/sim1.fam\"\n",
    "\n",
    "# clump_plink_parallel(bed_file=bed_file,\n",
    "#                      bim_file=bim_file,\n",
    "#                      fam_file=fam_file,\n",
    "#                      num_SNPS_exam=5, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dba5d6b",
   "metadata": {},
   "source": [
    "## test for `csv` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0764c527",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-18T04:58:20.279218Z",
     "start_time": "2023-04-18T04:58:20.279211Z"
    }
   },
   "outputs": [],
   "source": [
    "# # This is to generate the test csv files\n",
    "# import numpy as np\n",
    "# from scipy.linalg import toeplitz, block_diag\n",
    "# import pandas as pd\n",
    "\n",
    "# np.random.seed(123)\n",
    "# np.random.seed(np.around(np.random.rand(1) * 1e6, 3).astype(int))\n",
    "# N = 1000\n",
    "# SNR = 5.\n",
    "# true_beta = np.array([2., -2., 8., -8.] + [0] * 2000)\n",
    "# X_cov = toeplitz(.6**np.arange(true_beta.shape[0]))\n",
    "# X_cov = np.asarray(X_cov)\n",
    "# mean = (np.random.rand(true_beta.shape[0]) - .5) * 100\n",
    "# X = np.random.multivariate_normal(mean, X_cov, N)\n",
    "# X -= np.mean(X, 0).reshape(1, -1)\n",
    "# X /= np.std(X, 0).reshape(1, -1)\n",
    "# intercept_design_column = np.ones(N).reshape(N, 1)\n",
    "# X_sim = np.concatenate((intercept_design_column, X), 1)\n",
    "# true_sigma_sim = np.sqrt(true_beta.T @ X_cov @ true_beta / SNR)\n",
    "# true_beta_intercept = np.concatenate((np.array([0]), true_beta))\n",
    "# y_sim = X_sim @ true_beta_intercept + np.random.normal(0, true_sigma_sim, N)\n",
    "# X = X**2\n",
    "# X -= np.mean(X, 0).reshape(1, -1)\n",
    "# X /= np.std(X, 0).reshape(1, -1)\n",
    "# X = np.concatenate((y_sim.reshape(-1, 1), X), 1)\n",
    "# X.ravel()[np.random.choice(X.size, int(X.size * .1),\n",
    "#                            replace=False)] = np.nan  # 10% missing data\n",
    "\n",
    "# pd.DataFrame(X,\n",
    "#              columns=map(lambda x: \"Var \" + str(x), np.arange(\n",
    "#                  1, X.shape[1] +\n",
    "#                  1))).to_csv(r\"./fastHDMI/tests/sim/sim_continuous.csv\")\n",
    "\n",
    "# # this cell is for profiling the function\n",
    "# np.random.seed(321)\n",
    "# np.random.seed(np.around(np.random.rand(1) * 1e6, 3).astype(int))\n",
    "# N = 1000\n",
    "# SNR = 5.\n",
    "# true_beta = np.array([.5, -.5, .8, -.8] + [0] * 2000)\n",
    "# X_cov = toeplitz(.6**np.arange(true_beta.shape[0]))\n",
    "# X_cov = np.asarray(X_cov)\n",
    "# mean = (np.random.rand(true_beta.shape[0]) - .5) * 100\n",
    "# X = np.random.multivariate_normal(mean, X_cov, N)\n",
    "# X -= np.mean(X, 0).reshape(1, -1)\n",
    "# X /= np.std(X, 0).reshape(1, -1)\n",
    "# intercept_design_column = np.ones(N).reshape(N, 1)\n",
    "# X_sim = np.concatenate((intercept_design_column, X), 1)\n",
    "# true_sigma_sim = np.sqrt(true_beta.T @ X_cov @ true_beta / SNR)\n",
    "# true_beta_intercept = np.concatenate((np.array([0]), true_beta))\n",
    "# # + np.random.normal(0, true_sigma_sim, N)\n",
    "# signal = X_sim @ true_beta_intercept\n",
    "# y_sim = np.random.binomial(1, np.tanh(signal / 2) / 2 + .5)\n",
    "# # X = X**2\n",
    "# # X -= np.mean(X, 0).reshape(1, -1)\n",
    "# # X /= np.std(X, 0).reshape(1, -1)\n",
    "# X = np.concatenate((y_sim.reshape(-1, 1), X), 1)\n",
    "# X.ravel()[np.random.choice(X.size, int(X.size * .1),\n",
    "#                            replace=False)] = np.nan  # 10% missing data\n",
    "\n",
    "# pd.DataFrame(X,\n",
    "#              columns=map(lambda x: \"Var \" + str(x), np.arange(\n",
    "#                  1, X.shape[1] +\n",
    "#                  1))).to_csv(r\"./fastHDMI/tests/sim/sim_binary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e4c7d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-18T04:58:20.279821Z",
     "start_time": "2023-04-18T04:58:20.279814Z"
    }
   },
   "outputs": [],
   "source": [
    "# single-thread continuous version test\n",
    "a = continuous_screening_csv(\n",
    "    r\"./fastHDMI/tests/sim/sim_continuous.csv\", verbose=2)\n",
    "assert np.all(a >= 0)\n",
    "\n",
    "plt.plot(a)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d637d850",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-18T04:58:20.280582Z",
     "start_time": "2023-04-18T04:58:20.280575Z"
    }
   },
   "outputs": [],
   "source": [
    "# parallel continuous version test XXX\n",
    "a = continuous_screening_csv_parallel(\n",
    "    r\"./fastHDMI/tests/sim/sim_continuous.csv\", verbose=2)\n",
    "assert np.all(a >= 0)\n",
    "plt.plot(a)\n",
    "plt.show()\n",
    "\n",
    "b = np.absolute(\n",
    "    Pearson_screening_csv_parallel(r\"./fastHDMI/tests/sim/sim_continuous.csv\", verbose=2))\n",
    "plt.plot(b)\n",
    "plt.show()\n",
    "\n",
    "c = continuous_skMI_screening_csv_parallel(\n",
    "    r\"./fastHDMI/tests/sim/sim_continuous.csv\", random_state=0, verbose=2)\n",
    "assert np.all(c >= 0)\n",
    "plt.plot(c)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19ddd1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-18T04:58:20.281005Z",
     "start_time": "2023-04-18T04:58:20.281000Z"
    }
   },
   "outputs": [],
   "source": [
    "# single-thread binary version for csv\n",
    "a = binary_screening_csv(r\"./fastHDMI/tests/sim/sim_binary.csv\", verbose=2)\n",
    "print(a)\n",
    "assert np.all(a >= 0)\n",
    "plt.plot(a)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b51a385",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-18T04:58:20.281502Z",
     "start_time": "2023-04-18T04:58:20.281495Z"
    }
   },
   "outputs": [],
   "source": [
    "# parallel binary version test\n",
    "a = binary_screening_csv_parallel(\n",
    "    r\"./fastHDMI/tests/sim/sim_binary.csv\", verbose=2)\n",
    "assert np.all(a >= 0)\n",
    "plt.plot(a)\n",
    "plt.show()\n",
    "\n",
    "b = np.absolute(\n",
    "    Pearson_screening_csv_parallel(r\"./fastHDMI/tests/sim/sim_binary.csv\", verbose=2))\n",
    "plt.plot(b)\n",
    "plt.show()\n",
    "\n",
    "c = binary_skMI_screening_csv_parallel(r\"./fastHDMI/tests/sim/sim_binary.csv\",\n",
    "                                       random_state=0, verbose=2)\n",
    "assert np.all(c >= 0)\n",
    "plt.plot(c)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9dda67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-18T04:58:20.282338Z",
     "start_time": "2023-04-18T04:58:20.282331Z"
    }
   },
   "outputs": [],
   "source": [
    "# test for clumping for CSV files\n",
    "clump_continuous_csv_parallel(\n",
    "    csv_file=r\"./fastHDMI/tests/sim/sim_continuous.csv\", num_vars_exam=5, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d68690",
   "metadata": {},
   "source": [
    "## test for `np.array`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174c8ae7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-18T04:58:20.282848Z",
     "start_time": "2023-04-18T04:58:20.282841Z"
    }
   },
   "outputs": [],
   "source": [
    "# parallel continuous version but using numpy array\n",
    "csv = pd.read_csv(r\"./fastHDMI/tests/sim/sim_continuous.csv\",\n",
    "                  encoding='unicode_escape',\n",
    "                  engine=\"c\")\n",
    "# here it is because pandas reads the first column as the index\n",
    "X, y = csv.iloc[:, 2:].to_numpy(), csv.iloc[:, 1].to_numpy()\n",
    "\n",
    "MI = continuous_screening_array_parallel(X, y, verbose=2)\n",
    "assert np.all(MI >= 0)\n",
    "plt.plot(MI)\n",
    "plt.show()\n",
    "\n",
    "skMI = continuous_skMI_array_parallel(\n",
    "    X, y, n_neighbors=3, random_state=0, verbose=2)\n",
    "assert np.all(skMI >= 0)\n",
    "plt.plot(skMI)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8be973",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "893.844px",
    "left": "2190px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
