{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the class fundementals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-20T22:33:02.739726Z",
     "start_time": "2022-07-20T22:33:02.574050Z"
    }
   },
   "outputs": [],
   "source": [
    "# import os, sys\n",
    "# import collections\n",
    "import numpy as _np\n",
    "# import matplotlib.markers as markers\n",
    "# import matplotlib.pyplot as plt\n",
    "# import timeit\n",
    "# import collections\n",
    "# from scipy.stats import median_abs_deviation as mad\n",
    "# import multiprocessing\n",
    "# import cProfile\n",
    "# import itertools\n",
    "from numba import jit as _jit\n",
    "from numba import njit as _njit\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore') # this is just to hide all the warnings\n",
    "# import rpy2.robjects as robjects\n",
    "# import matplotlib.pyplot as plt # change font globally to Times\n",
    "# plt.style.use('ggplot')\n",
    "# plt.rcParams.update({\n",
    "#     \"text.usetex\": True,\n",
    "#     \"font.family\": \"Times New Roman\",\n",
    "#     \"font.sans-serif\": [\"Times New Roman\"],\n",
    "#     \"font.size\": 12})\n",
    "\n",
    "# os.chdir(sys.path[0]) # ensure working direcotry is set same as the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-20T22:33:02.888938Z",
     "start_time": "2022-07-20T22:33:02.741010Z"
    }
   },
   "outputs": [],
   "source": [
    "######################################  some SCAD and MCP things  #######################################\n",
    "@_jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def soft_thresholding(x, lambda_):\n",
    "    '''\n",
    "    To calculate soft-thresholding mapping of a given ONE-DIMENSIONAL tensor, BESIDES THE FIRST TERM (so beta_0 will not be penalized). \n",
    "    This function is to be used for calculation involving L1 penalty term later. \n",
    "    '''\n",
    "    return _np.hstack((_np.array([x[0]]),\n",
    "                       _np.where(\n",
    "                           _np.abs(x[1:]) > lambda_,\n",
    "                           x[1:] - _np.sign(x[1:]) * lambda_, 0)))\n",
    "\n",
    "\n",
    "soft_thresholding(_np.random.rand(20), 3.1)\n",
    "\n",
    "\n",
    "@_jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def SCAD(x, lambda_, a=3.7):\n",
    "    '''\n",
    "    To calculate SCAD penalty value;\n",
    "    #x can be a multi-dimensional tensor;\n",
    "    lambda_, a are scalars;\n",
    "    Fan and Li suggests to take a as 3.7 \n",
    "    '''\n",
    "    # here I notice the function is de facto a function of absolute value of x, therefore take absolute value first to simplify calculation\n",
    "    x = _np.abs(x)\n",
    "    temp = _np.where(\n",
    "        x <= lambda_, lambda_ * x,\n",
    "        _np.where(x < a * lambda_,\n",
    "                  (2 * a * lambda_ * x - x**2 - lambda_**2) / (2 * (a - 1)),\n",
    "                  lambda_**2 * (a + 1) / 2))\n",
    "    temp[0] = 0.  # this is to NOT penalize intercept beta later\n",
    "    return temp\n",
    "\n",
    "\n",
    "@_jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def SCAD_grad(x, lambda_, a=3.7):\n",
    "    '''\n",
    "    To calculate the gradient of SCAD wrt. input x; \n",
    "    #x can be a multi-dimensional tensor. \n",
    "    '''\n",
    "    # here decompose x to sign and its absolute value for easier calculation\n",
    "    sgn = _np.sign(x)\n",
    "    x = _np.abs(x)\n",
    "    temp = _np.where(\n",
    "        x <= lambda_, lambda_ * sgn,\n",
    "        _np.where(x < a * lambda_, (a * lambda_ * sgn - sgn * x) / (a - 1), 0))\n",
    "    temp[0] = 0.  # this is to NOT penalize intercept beta later\n",
    "    return temp\n",
    "\n",
    "\n",
    "@_jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def MCP(x, lambda_, gamma):\n",
    "    '''\n",
    "    To calculate MCP penalty value; \n",
    "    #x can be a multi-dimensional tensor. \n",
    "    '''\n",
    "    # the function is a function of absolute value of x\n",
    "    x = _np.abs(x)\n",
    "    temp = _np.where(x <= gamma * lambda_, lambda_ * x - x**2 / (2 * gamma),\n",
    "                     .5 * gamma * lambda_**2)\n",
    "    temp[0] = 0.  # this is to NOT penalize intercept beta later\n",
    "    return temp\n",
    "\n",
    "\n",
    "@_jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def MCP_grad(x, lambda_, gamma):\n",
    "    '''\n",
    "    To calculate MCP gradient wrt. input x; \n",
    "    #x can be a multi-dimensional tensor. \n",
    "    '''\n",
    "    temp = _np.where(\n",
    "        _np.abs(x) < gamma * lambda_,\n",
    "        lambda_ * _np.sign(x) - x / gamma, _np.zeros_like(x))\n",
    "    temp[0] = 0.  # this is to NOT penalize intercept beta later\n",
    "    return temp\n",
    "\n",
    "\n",
    "@_jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def SCAD_concave(x, lambda_, a=3.7):\n",
    "    '''\n",
    "    The value of concave part of SCAD penalty; \n",
    "    #x can be a multi-dimensional tensor. \n",
    "    '''\n",
    "    x = _np.abs(x)\n",
    "    temp = _np.where(\n",
    "        x <= lambda_, 0.,\n",
    "        _np.where(x < a * lambda_,\n",
    "                  (lambda_ * x - (x**2 + lambda_**2) / 2) / (a - 1),\n",
    "                  (a + 1) / 2 * lambda_**2 - lambda_ * x))\n",
    "    temp[0] = 0.  # this is to NOT penalize intercept beta later\n",
    "    return temp\n",
    "\n",
    "\n",
    "@_jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def SCAD_concave_grad(x, lambda_, a=3.7):\n",
    "    '''\n",
    "    The gradient of concave part of SCAD penalty wrt. input x; \n",
    "    #x can be a multi-dimensional tensor. \n",
    "    '''\n",
    "    sgn = _np.sign(x)\n",
    "    x = _np.abs(x)\n",
    "    temp = _np.where(\n",
    "        x <= lambda_, 0.,\n",
    "        _np.where(x < a * lambda_, (lambda_ * sgn - sgn * x) / (a - 1),\n",
    "                  -lambda_ * sgn))\n",
    "    temp[0] = 0.  # this is to NOT penalize intercept beta later\n",
    "    return temp\n",
    "\n",
    "\n",
    "@_jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def MCP_concave(x, lambda_, gamma):\n",
    "    '''\n",
    "    The value of concave part of MCP penalty; \n",
    "    #x can be a multi-dimensional tensor. \n",
    "    '''\n",
    "    # similiar as in MCP\n",
    "    x = _np.abs(x)\n",
    "    temp = _np.where(x <= gamma * lambda_, -(x**2) / (2 * gamma),\n",
    "                     (gamma * lambda_**2) / 2 - lambda_ * x)\n",
    "    temp[0] = 0.  # this is to NOT penalize intercept beta later\n",
    "    return temp\n",
    "\n",
    "\n",
    "@_jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def MCP_concave_grad(x, lambda_, gamma):\n",
    "    '''\n",
    "    The gradient of concave part of MCP penalty wrt. input x; \n",
    "    #x can be a multi-dimensional tensor. \n",
    "    '''\n",
    "    temp = _np.where(\n",
    "        _np.abs(x) < gamma * lambda_, -x / gamma, -lambda_ * _np.sign(x))\n",
    "    temp[0] = 0.  # this is to NOT penalize intercept beta later\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-20T22:33:02.934214Z",
     "start_time": "2022-07-20T22:33:02.890169Z"
    }
   },
   "outputs": [],
   "source": [
    "@_jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def _update_smooth_grad_convex_LM(N, X, beta_md, y):\n",
    "    '''\n",
    "    Update the gradient of the smooth convex objective component.\n",
    "    '''\n",
    "    return 1 / N * X.T @ (X @ beta_md - y)\n",
    "\n",
    "\n",
    "@_jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def _update_smooth_grad_SCAD_LM(N, X, beta_md, y, _lambda, a):\n",
    "    '''\n",
    "    Update the gradient of the smooth objective component for SCAD penalty.\n",
    "    '''\n",
    "    return _update_smooth_grad_convex_LM(N=N, X=X, beta_md=beta_md,\n",
    "                                         y=y) + SCAD_concave_grad(\n",
    "                                             x=beta_md, lambda_=_lambda, a=a)\n",
    "\n",
    "\n",
    "@_jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def _update_smooth_grad_MCP_LM(N, X, beta_md, y, _lambda, gamma):\n",
    "    '''\n",
    "    Update the gradient of the smooth objective component for MCP penalty.\n",
    "    '''\n",
    "    return _update_smooth_grad_convex_LM(\n",
    "        N=N, X=X, beta_md=beta_md, y=y) + MCP_concave_grad(\n",
    "            x=beta_md, lambda_=_lambda, gamma=gamma)\n",
    "\n",
    "\n",
    "@_jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def _eval_obj_SCAD_LM(N, X, beta_md, y, _lambda, a, x_temp):\n",
    "    '''\n",
    "    evaluate value of the objective function.\n",
    "    '''\n",
    "    error = y - X @ x_temp\n",
    "    return (error.T @ error) / (2. * N) + _np.sum(\n",
    "        SCAD(x_temp, lambda_=_lambda, a=a))\n",
    "\n",
    "\n",
    "@_jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def _eval_obj_MCP_LM(N, X, beta_md, y, _lambda, gamma, x_temp):\n",
    "    '''\n",
    "    evaluate value of the objective function.\n",
    "    '''\n",
    "    error = y - X @ x_temp\n",
    "    return (error.T @ error) / (2 * N) + _np.sum(\n",
    "        SCAD(x_temp, lambda_=_lambda, gamma=gamma))\n",
    "\n",
    "\n",
    "@_jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def lambda_max_LM(X, y):\n",
    "    \"\"\"\n",
    "    Calculate the lambda_max, i.e., the minimum lambda to nullify all penalized betas.\n",
    "    \"\"\"\n",
    "    #     X_temp = X.copy()\n",
    "    #     X_temp = X_temp[:,1:]\n",
    "    #     X_temp -= _np.mean(X_temp,0).reshape(1,-1)\n",
    "    #     X_temp /= _np.std(X_temp,0)\n",
    "    #     y_temp = y.copy()\n",
    "    #     y_temp -= _np.mean(y)\n",
    "    #     y_temp /= _np.std(y)\n",
    "    grad_at_0 = y @ X[:, 1:] / len(y)\n",
    "    lambda_max = _np.linalg.norm(grad_at_0, ord=_np.infty)\n",
    "    return lambda_max\n",
    "\n",
    "\n",
    "@_jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def _strong_rule_seq_LM(X, y, beta_old, lambda_new, lambda_old):\n",
    "    \"\"\"\n",
    "    Use sequential strong to determine which betas to be nullified next.\n",
    "    \"\"\"\n",
    "    #     X_temp = X.copy()\n",
    "    #     X_temp -= _np.mean(X_temp,0).reshape(1,-1)\n",
    "    #     X_temp /= _np.std(X_temp,0)\n",
    "    #     y_temp = y.copy()\n",
    "    #     y_temp -= _np.mean(y)\n",
    "    #     y_temp /= _np.std(y)\n",
    "    grad = _np.abs((y - X[:, 1:] @ beta_old[1:]) @ X[:, 1:] / (2 * len(y)))\n",
    "    eliminated = (grad < 2 * lambda_new - lambda_old\n",
    "                  )  # True means the value gets eliminated\n",
    "    eliminated = _np.hstack(\n",
    "        (_np.array([False]),\n",
    "         eliminated))  # because intercept coefficient is not penalized\n",
    "    return eliminated\n",
    "\n",
    "\n",
    "@_jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def UAG_LM_SCAD_MCP(design_matrix,\n",
    "                    outcome,\n",
    "                    beta_0=_np.ones(1),\n",
    "                    tol=1e-2,\n",
    "                    maxit=500,\n",
    "                    _lambda=.5,\n",
    "                    penalty=\"SCAD\",\n",
    "                    a=3.7,\n",
    "                    gamma=2.,\n",
    "                    L_convex=1.1,\n",
    "                    add_intercept_column=True):\n",
    "    '''\n",
    "    Carry out the optimization for penalized LM for a fixed lambda.\n",
    "    '''\n",
    "    X = design_matrix.copy()\n",
    "    y = outcome.copy()\n",
    "    N = X.shape[0]\n",
    "    if _np.all(beta_0 == _np.ones(1)):\n",
    "        cov = (y - _np.mean(y)) @ (X - 1 / N * _np.sum(X, 0).reshape(1, -1))\n",
    "        beta = _np.sign(cov)\n",
    "    else:\n",
    "        beta = beta_0\n",
    "#     add design matrix column for the intercept, if it's not there already\n",
    "    if add_intercept_column == True:\n",
    "        if _np.any(\n",
    "                X[:, 0] != X[0, 0]\n",
    "        ):  # check if design matrix has included a column for intercept or not\n",
    "            intercept_design = _np.ones(N).reshape(-1, 1)\n",
    "            X = _np.hstack((intercept_design, X))\n",
    "            beta = _np.hstack((_np.array([0.]), beta))\n",
    "    # passing other parameters\n",
    "    p = X.shape[1]  # so here p includes the intercept design matrix column\n",
    "    smooth_grad = _np.ones(p)\n",
    "    beta_ag = beta.copy()\n",
    "    beta_md = beta.copy()\n",
    "    k = 0\n",
    "    converged = False\n",
    "    opt_alpha = 1.\n",
    "    #     L_convex = 1/N*_np.max(_np.linalg.eigvalsh(X@X.T)[-1]).item()\n",
    "    if L_convex == 1.1:\n",
    "        L_convex = 1 / N * (_np.linalg.eigvalsh(X @ X.T)[-1])\n",
    "    else:\n",
    "        pass\n",
    "    old_speed_norm = 1.\n",
    "    speed_norm = 1.\n",
    "    restart_k = 0\n",
    "\n",
    "    if penalty == \"SCAD\":\n",
    "        #         L = _np.max(_np.array([L_convex, 1./(a-1)]))\n",
    "        L = _np.linalg.norm(_np.array([L_convex, 1. / (a - 1)]), ord=_np.infty)\n",
    "        opt_beta = .99 / L\n",
    "        while ((not converged) or (k < 3)) and k <= maxit:\n",
    "            k += 1\n",
    "            if old_speed_norm > speed_norm and k - restart_k >= 3:  # in this case, restart\n",
    "                opt_alpha = 1.  # restarting\n",
    "                restart_k = k  # restarting\n",
    "            else:  # restarting\n",
    "                opt_alpha = 2 / (\n",
    "                    1 + (1 + 4. / opt_alpha**2)**.5\n",
    "                )  #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound\n",
    "            opt_lambda = opt_beta / opt_alpha  #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound\n",
    "            beta_md_old = beta_md.copy()  # restarting\n",
    "            beta_md = (1 - opt_alpha) * beta_ag + opt_alpha * beta\n",
    "            old_speed_norm = speed_norm  # restarting\n",
    "            speed_norm = _np.linalg.norm(beta_md - beta_md_old,\n",
    "                                         ord=2)  # restarting\n",
    "            converged = (_np.linalg.norm(beta_md - beta_md_old, ord=_np.infty)\n",
    "                         < tol)\n",
    "            smooth_grad = _update_smooth_grad_SCAD_LM(N=N,\n",
    "                                                      X=X,\n",
    "                                                      beta_md=beta_md,\n",
    "                                                      y=y,\n",
    "                                                      _lambda=_lambda,\n",
    "                                                      a=a)\n",
    "            beta = soft_thresholding(x=beta - opt_lambda * smooth_grad,\n",
    "                                     lambda_=opt_lambda * _lambda)\n",
    "            beta_ag = soft_thresholding(x=beta_md - opt_beta * smooth_grad,\n",
    "                                        lambda_=opt_beta * _lambda)\n",
    "#             converged = _np.all(_np.max(_np.abs(beta_md - beta_ag)/opt_beta) < tol).item()\n",
    "#             converged = (_np.linalg.norm(beta_md - beta_ag, ord=_np.infty) < (tol*opt_beta))\n",
    "    else:\n",
    "        #         L = _np.max(_np.array([L_convex, 1./(gamma)]))\n",
    "        L = _np.linalg.norm(_np.array([L_convex, 1. / (gamma)]), ord=_np.infty)\n",
    "        opt_beta = .99 / L\n",
    "        while ((not converged) or (k < 3)) and k <= maxit:\n",
    "            k += 1\n",
    "            if old_speed_norm > speed_norm and k - restart_k >= 3:  # in this case, restart\n",
    "                opt_alpha = 1.  # restarting\n",
    "                restart_k = k  # restarting\n",
    "            else:  # restarting\n",
    "                opt_alpha = 2 / (\n",
    "                    1 + (1 + 4. / opt_alpha**2)**.5\n",
    "                )  #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound\n",
    "            opt_lambda = opt_beta / opt_alpha  #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound\n",
    "            beta_md_old = beta_md.copy()  # restarting\n",
    "            beta_md = (1 - opt_alpha) * beta_ag + opt_alpha * beta\n",
    "            old_speed_norm = speed_norm  # restarting\n",
    "            speed_norm = _np.linalg.norm(beta_md - beta_md_old,\n",
    "                                         ord=2)  # restarting\n",
    "            converged = (_np.linalg.norm(beta_md - beta_md_old, ord=_np.infty)\n",
    "                         < tol)\n",
    "            smooth_grad = _update_smooth_grad_MCP_LM(N=N,\n",
    "                                                     X=X,\n",
    "                                                     beta_md=beta_md,\n",
    "                                                     y=y,\n",
    "                                                     _lambda=_lambda,\n",
    "                                                     gamma=gamma)\n",
    "            beta = soft_thresholding(x=beta - opt_lambda * smooth_grad,\n",
    "                                     lambda_=opt_lambda * _lambda)\n",
    "            beta_ag = soft_thresholding(x=beta_md - opt_beta * smooth_grad,\n",
    "                                        lambda_=opt_beta * _lambda)\n",
    "#             converged = _np.all(_np.max(_np.abs(beta_md - beta_ag)/opt_beta) < tol).item()\n",
    "#             converged = (_np.linalg.norm(beta_md - beta_ag, ord=_np.infty) < (tol*opt_beta))\n",
    "    return k, beta_md\n",
    "\n",
    "\n",
    "# def vanilla_proximal(self):\n",
    "#     '''\n",
    "#     Carry out optimization using vanilla gradient descent.\n",
    "#     '''\n",
    "#     if self.penalty == \"SCAD\":\n",
    "#         L = max([self.L_convex, 1/(self.a-1)])\n",
    "#         self.vanilla_stepsize = 1/L\n",
    "#         self._eval_obj_SCAD_LM(self.beta_md, self.obj_value)\n",
    "#         self._eval_obj_SCAD_LM(self.beta, self.obj_value_ORIGINAL)\n",
    "#         self._eval_obj_SCAD_LM(self.beta_ag, self.obj_value_AG)\n",
    "#         self.old_beta = self.beta_md - 10.\n",
    "#         while not self.converged:\n",
    "#             self.k += 1\n",
    "#             if self.k <= self.maxit:\n",
    "#                 self._update_smooth_grad_SCAD_LM()\n",
    "#                 self.beta_md = self.soft_thresholding(self.beta_md - self.vanilla_stepsize*self.smooth_grad, self.vanilla_stepsize*self._lambda)\n",
    "#                 self.converged = _np.all(_np.max(_np.abs(self.beta_md - self.old_beta)) < self.tol).item()\n",
    "#                 self.old_beta = self.beta_md.copy()\n",
    "#                 self._eval_obj_SCAD_LM(self.beta_md, self.obj_value)\n",
    "#                 self._eval_obj_SCAD_LM(self.beta, self.obj_value_ORIGINAL)\n",
    "#                 self._eval_obj_SCAD_LM(self.beta_ag, self.obj_value_AG)\n",
    "#             else:\n",
    "#                 break\n",
    "#     else:\n",
    "#         L = max([self.L_convex, 1/self.gamma])\n",
    "#         self.vanilla_stepsize = 1/L\n",
    "#         self._eval_obj_MCP_LM(self.beta_md, self.obj_value)\n",
    "#         self._eval_obj_MCP_LM(self.beta, self.obj_value_ORIGINAL)\n",
    "#         self._eval_obj_MCP_LM(self.beta_ag, self.obj_value_AG)\n",
    "#         self.old_beta = self.beta_md - 10.\n",
    "#         while not self.converged:\n",
    "#             self.k += 1\n",
    "#             if self.k <= self.maxit:\n",
    "#                 self._update_smooth_grad_MCP_LM()\n",
    "#                 self.beta_md = self.soft_thresholding(self.beta_md - self.vanilla_stepsize*self.smooth_grad, self.vanilla_stepsize*self._lambda)\n",
    "#                 self.converged = _np.all(_np.max(_np.abs(self.beta_md - self.old_beta)) < self.tol).item()\n",
    "#                 self.old_beta = self.beta_md.copy()\n",
    "#                 self._eval_obj_MCP_LM(self.beta_md, self.obj_value)\n",
    "#                 self._eval_obj_MCP_LM(self.beta, self.obj_value_ORIGINAL)\n",
    "#                 self._eval_obj_MCP_LM(self.beta_ag, self.obj_value_AG)\n",
    "#             else:\n",
    "#                 break\n",
    "#     return self.report_results()\n",
    "\n",
    "\n",
    "@_jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def solution_path_LM(design_matrix,\n",
    "                     outcome,\n",
    "                     lambda_,\n",
    "                     beta_0=_np.ones(1),\n",
    "                     tol=1e-2,\n",
    "                     maxit=500,\n",
    "                     penalty=\"SCAD\",\n",
    "                     a=3.7,\n",
    "                     gamma=2.,\n",
    "                     add_intercept_column=True):\n",
    "    '''\n",
    "    Carry out the optimization for the solution path without the strong rule.\n",
    "    '''\n",
    "    #     add design matrix column for the intercept, if it's not there already\n",
    "    if add_intercept_column == True:\n",
    "        if _np.any(\n",
    "                design_matrix[:, 0] != design_matrix[0, 0]\n",
    "        ):  # check if design matrix has included a column for intercept or not\n",
    "            intercept_design = _np.ones(N).reshape(-1, 1)\n",
    "            _design_matrix = design_matrix.copy()\n",
    "            _design_matrix = _np.hstack((intercept_design, _design_matrix))\n",
    "        else:\n",
    "            _design_matrix = design_matrix\n",
    "    else:\n",
    "        _design_matrix = design_matrix\n",
    "    beta_mat = _np.zeros((len(lambda_) + 1, _design_matrix.shape[1]))\n",
    "    for j in range(len(lambda_)):\n",
    "        beta_mat[j + 1, :] = UAG_LM_SCAD_MCP(design_matrix=_design_matrix,\n",
    "                                             outcome=outcome,\n",
    "                                             beta_0=beta_mat[j, :],\n",
    "                                             tol=tol,\n",
    "                                             maxit=maxit,\n",
    "                                             _lambda=lambda_[j],\n",
    "                                             penalty=penalty,\n",
    "                                             a=a,\n",
    "                                             gamma=gamma,\n",
    "                                             add_intercept_column=False)[1]\n",
    "    return beta_mat[1:, :]\n",
    "\n",
    "\n",
    "# with strong rule\n",
    "\n",
    "\n",
    "@_jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def _UAG_LM_SCAD_MCP_strongrule(design_matrix,\n",
    "                                outcome,\n",
    "                                beta_0=_np.ones(1),\n",
    "                                tol=1e-2,\n",
    "                                maxit=500,\n",
    "                                _lambda=.5,\n",
    "                                penalty=\"SCAD\",\n",
    "                                a=3.7,\n",
    "                                gamma=2.,\n",
    "                                L_convex=1.1,\n",
    "                                add_intercept_column=True,\n",
    "                                strongrule=True):\n",
    "    '''\n",
    "    Carry out the optimization for a fixed lambda with strong rule.\n",
    "    '''\n",
    "    X = design_matrix.copy()\n",
    "    y = outcome.copy()\n",
    "    N = X.shape[0]\n",
    "    if _np.all(beta_0 == _np.ones(1)):\n",
    "        cov = (y - _np.mean(y)) @ (X - 1 / N * _np.sum(X, 0).reshape(1, -1))\n",
    "        beta = _np.sign(cov)\n",
    "    else:\n",
    "        beta = beta_0\n",
    "#     add design matrix column for the intercept, if it's not there already\n",
    "    if add_intercept_column == True:\n",
    "        if _np.any(\n",
    "                X[:, 0] != X[0, 0]\n",
    "        ):  # check if design matrix has included a column for intercept or not\n",
    "            intercept_design = _np.ones(N).reshape(-1, 1)\n",
    "            X = _np.hstack((intercept_design, X))\n",
    "            beta = _np.hstack((_np.array([0.]), beta))\n",
    "    if strongrule == True:\n",
    "        _lambda_max = lambda_max_LM(X, y)\n",
    "        p_original = X.shape[1]\n",
    "        elim = _strong_rule_seq_LM(X,\n",
    "                                   y,\n",
    "                                   beta_old=_np.zeros(p_original),\n",
    "                                   lambda_new=_lambda,\n",
    "                                   lambda_old=_lambda_max)\n",
    "        X = X[:, _np.logical_not(elim)]\n",
    "        beta = beta[_np.logical_not(elim)]\n",
    "\n",
    "    # passing other parameters\n",
    "    p = X.shape[1]  # so here p includes the intercept design matrix column\n",
    "    smooth_grad = _np.ones(p)\n",
    "    beta_ag = beta.copy()\n",
    "    beta_md = beta.copy()\n",
    "    k = 0\n",
    "    converged = False\n",
    "    opt_alpha = 1.\n",
    "    #     L_convex = 1/N*_np.max(_np.linalg.eigvalsh(X@X.T)[-1]).item()\n",
    "    if L_convex == 1.1:\n",
    "        L_convex = 1 / N * (_np.linalg.eigvalsh(X @ X.T)[-1])\n",
    "    else:\n",
    "        pass\n",
    "    old_speed_norm = 1.\n",
    "    speed_norm = 1.\n",
    "    restart_k = 0\n",
    "\n",
    "    if penalty == \"SCAD\":\n",
    "        #         L = _np.max(_np.array([L_convex, 1./(a-1)]))\n",
    "        L = _np.linalg.norm(_np.array([L_convex, 1. / (a - 1)]), ord=_np.infty)\n",
    "        opt_beta = .99 / L\n",
    "        while ((not converged) or (k < 3)) and k <= maxit:\n",
    "            k += 1\n",
    "            if old_speed_norm > speed_norm and k - restart_k >= 3:  # in this case, restart\n",
    "                opt_alpha = 1.  # restarting\n",
    "                restart_k = k  # restarting\n",
    "            else:  # restarting\n",
    "                opt_alpha = 2. / (\n",
    "                    1. + (1. + 4. / opt_alpha**2)**.5\n",
    "                )  #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound\n",
    "            opt_lambda = opt_beta / opt_alpha  #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound\n",
    "            beta_md_old = beta_md.copy()  # restarting\n",
    "            beta_md = (1. - opt_alpha) * beta_ag + opt_alpha * beta\n",
    "            old_speed_norm = speed_norm  # restarting\n",
    "            speed_norm = _np.linalg.norm(beta_md - beta_md_old,\n",
    "                                         ord=2)  # restarting\n",
    "            converged = (_np.linalg.norm(beta_md - beta_md_old, ord=_np.infty)\n",
    "                         < tol)\n",
    "            smooth_grad = _update_smooth_grad_SCAD_LM(N=N,\n",
    "                                                      X=X,\n",
    "                                                      beta_md=beta_md,\n",
    "                                                      y=y,\n",
    "                                                      _lambda=_lambda,\n",
    "                                                      a=a)\n",
    "            beta = soft_thresholding(x=beta - opt_lambda * smooth_grad,\n",
    "                                     lambda_=opt_lambda * _lambda)\n",
    "            beta_ag = soft_thresholding(x=beta_md - opt_beta * smooth_grad,\n",
    "                                        lambda_=opt_beta * _lambda)\n",
    "#             converged = _np.all(_np.max(_np.abs(beta_md - beta_ag)/opt_beta) < tol).item()\n",
    "#             converged = (_np.linalg.norm(beta_md - beta_ag, ord=_np.infty) < (tol*opt_beta))\n",
    "    else:\n",
    "        #         L = _np.max(_np.array([L_convex, 1./(gamma)]))\n",
    "        L = _np.linalg.norm(_np.array([L_convex, 1. / (gamma)]), ord=_np.infty)\n",
    "        opt_beta = .99 / L\n",
    "        while ((not converged) or (k < 3)) and k <= maxit:\n",
    "            k += 1\n",
    "            if old_speed_norm > speed_norm and k - restart_k >= 3:  # in this case, restart\n",
    "                opt_alpha = 1.  # restarting\n",
    "                restart_k = k  # restarting\n",
    "            else:  # restarting\n",
    "                opt_alpha = 2 / (\n",
    "                    1. + (1. + 4. / opt_alpha**2)**.5\n",
    "                )  #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound\n",
    "            opt_lambda = opt_beta / opt_alpha  #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound\n",
    "            beta_md_old = beta_md.copy()  # restarting\n",
    "            beta_md = (1. - opt_alpha) * beta_ag + opt_alpha * beta\n",
    "            old_speed_norm = speed_norm  # restarting\n",
    "            speed_norm = _np.linalg.norm(beta_md - beta_md_old,\n",
    "                                         ord=2)  # restarting\n",
    "            converged = (_np.linalg.norm(beta_md - beta_md_old, ord=_np.infty)\n",
    "                         < tol)\n",
    "            smooth_grad = _update_smooth_grad_MCP_LM(N=N,\n",
    "                                                     X=X,\n",
    "                                                     beta_md=beta_md,\n",
    "                                                     y=y,\n",
    "                                                     _lambda=_lambda,\n",
    "                                                     gamma=gamma)\n",
    "            beta = soft_thresholding(x=beta - opt_lambda * smooth_grad,\n",
    "                                     lambda_=opt_lambda * _lambda)\n",
    "            beta_ag = soft_thresholding(x=beta_md - opt_beta * smooth_grad,\n",
    "                                        lambda_=opt_beta * _lambda)\n",
    "\n",
    "\n",
    "#             converged = _np.all(_np.max(_np.abs(beta_md - beta_ag)/opt_beta) < tol).item()\n",
    "#             converged = (_np.linalg.norm(beta_md - beta_ag, ord=_np.infty) < (tol*opt_beta))\n",
    "#     if strongrule == True:\n",
    "#         _beta_output = _np.zeros((p_original))\n",
    "# #         _ = _np.argwhere(_np.logical_not(elim)).flatten()\n",
    "# #         print(_)\n",
    "# #         for j in range(len(_)):\n",
    "# #             if j<10:\n",
    "# #                 print(j)\n",
    "# #                 print(_[j])\n",
    "# #             _beta_output[_[j]] = beta_md[j]\n",
    "# #             if j<10:\n",
    "# #                 print(_beta_output[_[j]])\n",
    "#         _beta_output[~elim] = beta_md  # this line of code can't compile\n",
    "#     else:\n",
    "#         _beta_output = beta_md\n",
    "    return k, beta_md, elim\n",
    "\n",
    "\n",
    "@_jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def UAG_LM_SCAD_MCP_strongrule(design_matrix,\n",
    "                               outcome,\n",
    "                               beta_0=_np.ones(1),\n",
    "                               tol=1e-2,\n",
    "                               maxit=500,\n",
    "                               _lambda=.5,\n",
    "                               penalty=\"SCAD\",\n",
    "                               a=3.7,\n",
    "                               gamma=2.,\n",
    "                               L_convex=1.1,\n",
    "                               add_intercept_column=True,\n",
    "                               strongrule=True):\n",
    "    \"\"\"\n",
    "    Carry out the optimization for a fixed lambda for penanlized LM with strong rule.\n",
    "    \"\"\"\n",
    "    _k, _beta_md, _elim = _UAG_LM_SCAD_MCP_strongrule(\n",
    "        design_matrix=design_matrix,\n",
    "        outcome=outcome,\n",
    "        beta_0=beta_0,\n",
    "        tol=tol,\n",
    "        maxit=maxit,\n",
    "        _lambda=_lambda,\n",
    "        penalty=penalty,\n",
    "        a=a,\n",
    "        gamma=gamma,\n",
    "        L_convex=L_convex,\n",
    "        add_intercept_column=add_intercept_column,\n",
    "        strongrule=strongrule)\n",
    "    output_beta = _np.zeros(len(_elim))\n",
    "    output_beta[_np.logical_not(_elim)] = _beta_md\n",
    "    return _k, output_beta\n",
    "\n",
    "\n",
    "@_jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def solution_path_LM_strongrule(design_matrix,\n",
    "                                outcome,\n",
    "                                lambda_,\n",
    "                                beta_0=_np.ones(1),\n",
    "                                tol=1e-2,\n",
    "                                maxit=500,\n",
    "                                penalty=\"SCAD\",\n",
    "                                a=3.7,\n",
    "                                gamma=2.,\n",
    "                                add_intercept_column=True):\n",
    "    '''\n",
    "    Carry out the optimization for the solution path of a penalized LM with strong rule.\n",
    "    '''\n",
    "    #     add design matrix column for the intercept, if it's not there already\n",
    "    _design_matrix = design_matrix.copy()\n",
    "    if add_intercept_column == True:\n",
    "        if _np.any(\n",
    "                design_matrix[:, 0] != design_matrix[0, 0]\n",
    "        ):  # check if design matrix has included a column for intercept or not\n",
    "            intercept_design = _np.ones(N).reshape(-1, 1)\n",
    "            _design_matrix = _np.hstack((intercept_design, _design_matrix))\n",
    "    beta_mat = _np.empty((len(lambda_) + 1, _design_matrix.shape[1]))\n",
    "    beta_mat[0, :] = 0.\n",
    "    _lambda_max = lambda_max_LM(_design_matrix, outcome)\n",
    "    lambda_ = _np.hstack((_np.array([_lambda_max]), lambda_))\n",
    "    elim = _np.array([False] * _design_matrix.shape[1])\n",
    "    for j in range(len(lambda_) - 1):\n",
    "        _elim = _strong_rule_seq_LM(X=_design_matrix,\n",
    "                                    y=outcome,\n",
    "                                    beta_old=beta_mat[j, :],\n",
    "                                    lambda_new=lambda_[j + 1],\n",
    "                                    lambda_old=lambda_[j])\n",
    "        elim = _np.logical_and(elim, _elim)\n",
    "        _beta_0 = beta_mat[j, :]\n",
    "        _new_beta = _np.zeros(_design_matrix.shape[1])\n",
    "        _new_beta[_np.logical_not(elim)] = UAG_LM_SCAD_MCP(\n",
    "            design_matrix=_design_matrix[:, _np.logical_not(elim)],\n",
    "            outcome=outcome,\n",
    "            beta_0=_beta_0[_np.logical_not(elim)],\n",
    "            tol=tol,\n",
    "            maxit=maxit,\n",
    "            _lambda=lambda_[j],\n",
    "            penalty=penalty,\n",
    "            a=a,\n",
    "            gamma=gamma,\n",
    "            add_intercept_column=False)[1]\n",
    "        beta_mat[j + 1, :] = _new_beta\n",
    "    return beta_mat[1:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-20T22:33:31.457657Z",
     "start_time": "2022-07-20T22:33:03.020159Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_107423/1324807545.py:349: NumbaPerformanceWarning: \u001b[1m\u001b[1m\u001b[1m'@' is faster on contiguous arrays, called on (array(float64, 1d, C), array(float64, 2d, A))\u001b[0m\u001b[0m\u001b[0m\n",
      "  _lambda_max = lambda_max_LM(_design_matrix, outcome)\n",
      "/tmp/ipykernel_107423/1324807545.py:349: NumbaPerformanceWarning: \u001b[1m\u001b[1m'@' is faster on contiguous arrays, called on (array(float64, 1d, C), array(float64, 2d, A))\u001b[0m\u001b[0m\n",
      "  _lambda_max = lambda_max_LM(_design_matrix, outcome)\n",
      "/tmp/ipykernel_107423/1324807545.py:353: NumbaPerformanceWarning: \u001b[1m\u001b[1m\u001b[1m'@' is faster on contiguous arrays, called on (array(float64, 1d, C), array(float64, 2d, A))\u001b[0m\u001b[0m\u001b[0m\n",
      "  _elim = _strong_rule_seq_LM(X=_design_matrix, y=outcome, beta_old=beta_mat[j,:], lambda_new=lambda_[j+1], lambda_old=lambda_[j])\n",
      "/tmp/ipykernel_107423/1324807545.py:353: NumbaPerformanceWarning: \u001b[1m\u001b[1m\u001b[1m'@' is faster on contiguous arrays, called on (array(float64, 2d, A), array(float64, 1d, C))\u001b[0m\u001b[0m\u001b[0m\n",
      "  _elim = _strong_rule_seq_LM(X=_design_matrix, y=outcome, beta_old=beta_mat[j,:], lambda_new=lambda_[j+1], lambda_old=lambda_[j])\n",
      "/tmp/ipykernel_107423/1324807545.py:353: NumbaPerformanceWarning: \u001b[1m\u001b[1m'@' is faster on contiguous arrays, called on (array(float64, 1d, C), array(float64, 2d, A))\u001b[0m\u001b[0m\n",
      "  _elim = _strong_rule_seq_LM(X=_design_matrix, y=outcome, beta_old=beta_mat[j,:], lambda_new=lambda_[j+1], lambda_old=lambda_[j])\n",
      "/tmp/ipykernel_107423/1324807545.py:353: NumbaPerformanceWarning: \u001b[1m\u001b[1m'@' is faster on contiguous arrays, called on (array(float64, 2d, A), array(float64, 1d, C))\u001b[0m\u001b[0m\n",
      "  _elim = _strong_rule_seq_LM(X=_design_matrix, y=outcome, beta_old=beta_mat[j,:], lambda_new=lambda_[j+1], lambda_old=lambda_[j])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.25145030e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 1.25529911e+00  1.93834934e+00 -1.87491707e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 1.25529773e+00  1.94263335e+00 -1.88578904e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " ...\n",
      " [ 1.25529526e+00  1.87673754e+00 -1.91218195e+00 ...  7.69226132e-02\n",
      "  -8.24977017e-02  0.00000000e+00]\n",
      " [ 1.25529526e+00  1.87214777e+00 -1.90930101e+00 ...  8.90060269e-02\n",
      "  -8.95442224e-02  0.00000000e+00]\n",
      " [ 1.25529526e+00  1.86767007e+00 -1.90691276e+00 ...  1.00662533e-01\n",
      "  -9.61300285e-02  1.36140337e-04]]\n",
      "1.74 s ± 77.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.markers as markers\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit\n",
    "from scipy.linalg import toeplitz, block_diag\n",
    "\n",
    "np.random.seed(1)\n",
    "N = 1000\n",
    "SNR = 5.\n",
    "true_beta = np.array([2, -2, 8, -8] + [0] * 1000)\n",
    "X_cov = toeplitz(.6**np.arange(true_beta.shape[0]))\n",
    "mean = np.zeros(true_beta.shape[0])\n",
    "X = np.random.multivariate_normal(mean, X_cov, N)\n",
    "X -= np.mean(X, 0).reshape(1, -1)\n",
    "X /= np.std(X, 0)\n",
    "intercept_design_column = np.ones(N).reshape(N, 1)\n",
    "X_sim = np.concatenate((intercept_design_column, X), 1)\n",
    "true_sigma_sim = np.sqrt(true_beta.T @ X_cov @ true_beta / SNR)\n",
    "true_beta_intercept = np.concatenate((np.array([\n",
    "    1.23\n",
    "]), true_beta))  # here just define the intercept to be 1.23 for simulated data\n",
    "epsilon = np.random.normal(0, true_sigma_sim, N)\n",
    "y_sim = X_sim @ true_beta_intercept + epsilon\n",
    "\n",
    "lambda_seq = np.arange(40) / 400\n",
    "lambda_seq = lambda_seq[1:]\n",
    "lambda_seq = lambda_seq[::-1]\n",
    "\n",
    "# do NOT include the design matrix intercept column\n",
    "LM_beta = solution_path_LM_strongrule(design_matrix=X_sim,\n",
    "                                      outcome=y_sim,\n",
    "                                      lambda_=lambda_seq,\n",
    "                                      beta_0=np.ones(1),\n",
    "                                      tol=1e-2,\n",
    "                                      maxit=500,\n",
    "                                      penalty=\"SCAD\",\n",
    "                                      a=3.7,\n",
    "                                      gamma=2.,\n",
    "                                      add_intercept_column=True)\n",
    "\n",
    "print(LM_beta)  # to make sure strong rule runs correctly\n",
    "\n",
    "%timeit solution_path_LM_strongrule(design_matrix=X_sim, outcome=y_sim, lambda_=lambda_seq, beta_0 = np.ones(1), tol=1e-2, maxit=500, penalty=\"SCAD\", a=3.7, gamma=2., add_intercept_column=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-20T22:33:47.789574Z",
     "start_time": "2022-07-20T22:33:31.459299Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# this cell is for profiling the function\n",
    "np.random.seed(1)\n",
    "N = 1000\n",
    "p_zeros = 2000\n",
    "SNR = 5.\n",
    "true_beta = np.array([2, -2, 8, -8] + [0] * p_zeros)\n",
    "X_cov = toeplitz(0.6**np.arange(len(true_beta)))\n",
    "mean = np.zeros(len(true_beta))\n",
    "X = np.random.multivariate_normal(mean, X_cov, N)\n",
    "X -= np.mean(X, 0).reshape(1, -1)\n",
    "X /= np.std(X, 0)\n",
    "intercept_design_column = np.ones(N).reshape(N, 1)\n",
    "X_sim = np.concatenate((intercept_design_column, X), 1)\n",
    "true_sigma_sim = np.sqrt(true_beta.T @ X_cov @ true_beta / SNR)\n",
    "true_beta_intercept = np.concatenate((np.array([\n",
    "    1.23\n",
    "]), true_beta))  # here just define the intercept to be 1.23 for simulated data\n",
    "epsilon = np.random.normal(0, true_sigma_sim, N)\n",
    "y_sim = X_sim @ true_beta_intercept + epsilon\n",
    "\n",
    "fit1 = UAG_LM_SCAD_MCP_strongrule(design_matrix=X_sim,\n",
    "                                  outcome=y_sim,\n",
    "                                  tol=1e-2,\n",
    "                                  maxit=500,\n",
    "                                  _lambda=.2,\n",
    "                                  penalty=\"SCAD\",\n",
    "                                  a=3.7,\n",
    "                                  gamma=2.)\n",
    "\n",
    "fit2 = solution_path_LM(design_matrix=X_sim,\n",
    "                        outcome=y_sim,\n",
    "                        tol=1e-2,\n",
    "                        maxit=500,\n",
    "                        lambda_=np.linspace(.1, 1, 100)[::-1],\n",
    "                        penalty=\"SCAD\",\n",
    "                        a=3.7,\n",
    "                        gamma=2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-20T22:33:50.115558Z",
     "start_time": "2022-07-20T22:33:47.790559Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "287 ms ± 9.47 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit UAG_LM_SCAD_MCP_strongrule(design_matrix=X_sim, outcome=y_sim, tol=1e-2, maxit=500, _lambda=.5, penalty=\"MCP\", a=3.7, gamma=2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-20T22:33:50.120694Z",
     "start_time": "2022-07-20T22:33:50.116802Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64,\n",
       " array([ 1.42549458,  2.15140761, -2.03064098, ...,  0.        ,\n",
       "         0.        ,  0.        ]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-20T22:33:50.230569Z",
     "start_time": "2022-07-20T22:33:50.121688Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(len(fit1[1])), fit1[1], 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "877.844px",
    "left": "2188px",
    "right": "20px",
    "top": "120px",
    "width": "352px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
