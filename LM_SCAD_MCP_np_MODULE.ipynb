{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the class fundementals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-24T20:48:44.545215Z",
     "start_time": "2022-05-24T20:48:43.529873Z"
    }
   },
   "outputs": [],
   "source": [
    "# import os, sys\n",
    "# import collections\n",
    "import numpy as np\n",
    "import matplotlib.markers as markers\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit\n",
    "# import collections\n",
    "from scipy.linalg import toeplitz, block_diag\n",
    "from scipy.stats import median_abs_deviation as mad\n",
    "# import multiprocessing\n",
    "# import cProfile\n",
    "# import itertools\n",
    "from numba import jit, njit\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # this is just to hide all the warnings\n",
    "# import rpy2.robjects as robjects\n",
    "# import matplotlib.pyplot as plt # change font globally to Times \n",
    "# plt.style.use('ggplot')\n",
    "# plt.rcParams.update({\n",
    "#     \"text.usetex\": True,\n",
    "#     \"font.family\": \"Times New Roman\",\n",
    "#     \"font.sans-serif\": [\"Times New Roman\"],\n",
    "#     \"font.size\": 12})\n",
    "\n",
    "# os.chdir(sys.path[0]) # ensure working direcotry is set same as the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-24T20:48:44.623642Z",
     "start_time": "2022-05-24T20:48:44.545215Z"
    }
   },
   "outputs": [],
   "source": [
    "######################################  some SCAD and MCP things  #######################################\n",
    "@jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def soft_thresholding(x, lambda_):\n",
    "    '''\n",
    "    To calculate soft-thresholding mapping of a given ONE-DIMENSIONAL tensor, BESIDES THE FIRST TERM (so beta_0 will not be penalized). \n",
    "    This function is to be used for calculation involving L1 penalty term later. \n",
    "    '''\n",
    "    return np.concatenate((np.array([x[0]]), np.where(np.abs(x[1:])>lambda_, x[1:] - np.sign(x[1:])*lambda_, 0)))\n",
    "\n",
    "@jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def SCAD(x, lambda_, a=3.7):\n",
    "    '''\n",
    "    To calculate SCAD penalty value;\n",
    "    #x can be a multi-dimensional tensor;\n",
    "    lambda_, a are scalars;\n",
    "    Fan and Li suggests to take a as 3.7 \n",
    "    '''\n",
    "    # here I notice the function is de facto a function of absolute value of x, therefore take absolute value first to simplify calculation \n",
    "    x = np.abs(x)\n",
    "    temp = np.where(x<=lambda_, lambda_*x, np.where(x<a*lambda_, (2*a*lambda_*x - x**2 - lambda_**2)/(2*(a - 1)), lambda_**2 * (a+1)/2))\n",
    "    temp[0] = 0. # this is to NOT penalize intercept beta later \n",
    "    return temp \n",
    "\n",
    "@jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def SCAD_grad(x, lambda_, a=3.7):\n",
    "    '''\n",
    "    To calculate the gradient of SCAD wrt. input x; \n",
    "    #x can be a multi-dimensional tensor. \n",
    "    '''\n",
    "    # here decompose x to sign and its absolute value for easier calculation\n",
    "    sgn = np.sign(x)\n",
    "    x = np.abs(x)\n",
    "    temp = np.where(x<=lambda_, lambda_*sgn, np.where(x<a*lambda_, (a*lambda_*sgn-sgn*x)/(a-1), 0))\n",
    "    temp[0] = 0. # this is to NOT penalize intercept beta later \n",
    "    return temp \n",
    "\n",
    "@jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def MCP(x, lambda_, gamma):\n",
    "    '''\n",
    "    To calculate MCP penalty value; \n",
    "    #x can be a multi-dimensional tensor. \n",
    "    '''\n",
    "    # the function is a function of absolute value of x \n",
    "    x = np.abs(x)\n",
    "    temp = np.where(x<=gamma*lambda_, lambda_*x - x**2/(2*gamma), .5*gamma*lambda_**2)\n",
    "    temp[0] = 0. # this is to NOT penalize intercept beta later \n",
    "    return temp \n",
    "\n",
    "@jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def MCP_grad(x, lambda_, gamma):\n",
    "    '''\n",
    "    To calculate MCP gradient wrt. input x; \n",
    "    #x can be a multi-dimensional tensor. \n",
    "    '''\n",
    "    temp = np.where(np.abs(x)<gamma*lambda_, lambda_*np.sign(x)-x/gamma, np.zeros_like(x))\n",
    "    temp[0] = 0. # this is to NOT penalize intercept beta later \n",
    "    return temp \n",
    "\n",
    "@jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def SCAD_concave(x, lambda_, a=3.7):\n",
    "    '''\n",
    "    The value of concave part of SCAD penalty; \n",
    "    #x can be a multi-dimensional tensor. \n",
    "    '''\n",
    "    x = np.abs(x)\n",
    "    temp = np.where(x<=lambda_, 0., np.where(x<a*lambda_, (lambda_*x - (x**2 + lambda_**2)/2)/(a-1), (a+1)/2*lambda_**2 - lambda_*x))\n",
    "    temp[0] = 0. # this is to NOT penalize intercept beta later \n",
    "    return temp \n",
    "\n",
    "@jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def SCAD_concave_grad(x, lambda_, a=3.7):\n",
    "    '''\n",
    "    The gradient of concave part of SCAD penalty wrt. input x; \n",
    "    #x can be a multi-dimensional tensor. \n",
    "    '''\n",
    "    sgn = np.sign(x)\n",
    "    x = np.abs(x)\n",
    "    temp = np.where(x<=lambda_, 0., np.where(x<a*lambda_, (lambda_*sgn-sgn*x)/(a-1), -lambda_*sgn))\n",
    "    temp[0] = 0. # this is to NOT penalize intercept beta later \n",
    "    return temp \n",
    "\n",
    "@jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def MCP_concave(x, lambda_, gamma):\n",
    "    '''\n",
    "    The value of concave part of MCP penalty; \n",
    "    #x can be a multi-dimensional tensor. \n",
    "    '''\n",
    "    # similiar as in MCP\n",
    "    x = np.abs(x)\n",
    "    temp = np.where(x<=gamma*lambda_, -(x**2)/(2*gamma), (gamma*lambda_**2)/2 - lambda_*x)\n",
    "    temp[0] = 0. # this is to NOT penalize intercept beta later \n",
    "    return temp \n",
    "\n",
    "@jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def MCP_concave_grad(x, lambda_, gamma):\n",
    "    '''\n",
    "    The gradient of concave part of MCP penalty wrt. input x; \n",
    "    #x can be a multi-dimensional tensor. \n",
    "    '''\n",
    "    temp = np.where(np.abs(x) < gamma*lambda_, -x/gamma, -lambda_*np.sign(x))\n",
    "    temp[0] = 0. # this is to NOT penalize intercept beta later \n",
    "    return temp \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-24T20:48:44.732624Z",
     "start_time": "2022-05-24T20:48:44.625531Z"
    }
   },
   "outputs": [],
   "source": [
    "@jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def update_smooth_grad_convex_LM(N, X, beta_md, y):\n",
    "    '''\n",
    "    Update the gradient of the smooth convex objective component.\n",
    "    '''\n",
    "    return 1/N*X.T@(X@beta_md - y)\n",
    "\n",
    "@jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def update_smooth_grad_SCAD(N, X, beta_md, y, _lambda, a):\n",
    "    '''\n",
    "    Update the gradient of the smooth objective component for SCAD penalty.\n",
    "    '''\n",
    "    return update_smooth_grad_convex_LM(N=N, X=X, beta_md=beta_md, y=y) + SCAD_concave_grad(x=beta_md, lambda_=_lambda, a=a)\n",
    "\n",
    "@jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def update_smooth_grad_MCP(N, X, beta_md, y, _lambda, gamma):\n",
    "    '''\n",
    "    Update the gradient of the smooth objective component for MCP penalty.\n",
    "    '''\n",
    "    return update_smooth_grad_convex_LM(N=N, X=X, beta_md=beta_md, y=y) + MCP_concave_grad(x=beta_md, lambda_=_lambda, gamma=gamma)\n",
    "\n",
    "@jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def eval_obj_SCAD(N, X, beta_md, y, _lambda, a, x_temp):\n",
    "    '''\n",
    "    evaluate value of the objective function.\n",
    "    '''\n",
    "    error = y - X@x_temp\n",
    "    return (error.T@error)/(2.*N) + np.sum(SCAD(x_temp, lambda_=_lambda, a=a))\n",
    "\n",
    "@jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def eval_obj_MCP(N, X, beta_md, y, _lambda, gamma, x_temp):\n",
    "    '''\n",
    "    evaluate value of the objective function.\n",
    "    '''\n",
    "    error = y - X@x_temp\n",
    "    return (error.T@error)/(2*N) + np.sum(SCAD(x_temp, lambda_=_lambda, gamma=gamma))\n",
    "\n",
    "\n",
    "@jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def lambda_max_LM(X, y):\n",
    "#     X_temp = X.copy()\n",
    "#     X_temp = X_temp[:,1:]\n",
    "#     X_temp -= np.mean(X_temp,0).reshape(1,-1)\n",
    "#     X_temp /= np.std(X_temp,0)\n",
    "#     y_temp = y.copy()\n",
    "#     y_temp -= np.mean(y)\n",
    "#     y_temp /= np.std(y)\n",
    "    grad_at_0 = y@X[:,1:]/len(y)\n",
    "    lambda_max = np.linalg.norm(grad_at_0, ord=np.infty)\n",
    "    return lambda_max\n",
    "\n",
    "@jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def strong_rule_seq_LM(X, y, beta_old, lambda_new, lambda_old):\n",
    "#     X_temp = X.copy()\n",
    "#     X_temp -= np.mean(X_temp,0).reshape(1,-1)\n",
    "#     X_temp /= np.std(X_temp,0)\n",
    "#     y_temp = y.copy()\n",
    "#     y_temp -= np.mean(y)\n",
    "#     y_temp /= np.std(y)\n",
    "    grad = np.abs((y-X[:,1:]@beta_old[1:])@X[:,1:]/(2*len(y)))\n",
    "    eliminated = (grad < 2*lambda_new - lambda_old) # True means the value gets eliminated\n",
    "    eliminated = np.hstack((np.array([False]), eliminated)) # because intercept coefficient is not penalized\n",
    "    return eliminated\n",
    "\n",
    "@jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def UAG_LM_SCAD_MCP(design_matrix, outcome, beta_0 = np.ones(1), tol=1e-2, maxit=500, _lambda=.5, penalty=\"SCAD\", a=3.7, gamma=2., L_convex=1.1, add_intercept_column = True):\n",
    "    '''\n",
    "    Carry out the optimization.\n",
    "    '''\n",
    "    X = design_matrix.copy()\n",
    "    y = outcome.copy()\n",
    "    N = X.shape[0]\n",
    "    if np.all(beta_0==np.ones(1)):\n",
    "        cov = (y - np.mean(y))@(X - 1/N*np.sum(X, 0).reshape(1,-1))\n",
    "        beta = np.sign(cov)\n",
    "    else:\n",
    "        beta = beta_0\n",
    "#     add design matrix column for the intercept, if it's not there already\n",
    "    if add_intercept_column == True:\n",
    "        if np.any(X[:,0] != X[0,0]): # check if design matrix has included a column for intercept or not\n",
    "            intercept_design = np.ones(N).reshape(-1, 1)\n",
    "            X = np.hstack((intercept_design, X))\n",
    "            beta = np.hstack((np.array([0.]), beta))\n",
    "    # passing other parameters\n",
    "    p = X.shape[1] # so here p includes the intercept design matrix column \n",
    "    smooth_grad = np.ones(p)\n",
    "    beta_ag = beta.copy()\n",
    "    beta_md = beta.copy()\n",
    "    k = 0\n",
    "    converged = False\n",
    "    opt_alpha = 1.\n",
    "#     L_convex = 1/N*np.max(np.linalg.eigvalsh(X@X.T)[-1]).item()\n",
    "    if L_convex == 1.1:\n",
    "        L_convex = 1/N*(np.linalg.eigvalsh(X@X.T)[-1])\n",
    "    else:\n",
    "        pass\n",
    "    old_speed_norm = 1.\n",
    "    speed_norm = 1.\n",
    "    restart_k = 0\n",
    "    \n",
    "    if penalty == \"SCAD\":\n",
    "#         L = np.max(np.array([L_convex, 1./(a-1)]))\n",
    "        L = np.linalg.norm(np.array([L_convex, 1./(a-1)]), ord=np.infty)\n",
    "        opt_beta = .99/L\n",
    "        while ((not converged) or (k<3)) and k <= maxit:\n",
    "            k += 1\n",
    "            if old_speed_norm > speed_norm and k - restart_k>=3: # in this case, restart\n",
    "                opt_alpha = 1. # restarting\n",
    "                restart_k = k # restarting\n",
    "            else: # restarting\n",
    "                opt_alpha = 2/(1+(1+4./opt_alpha**2)**.5) #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound \n",
    "            opt_lambda = opt_beta/opt_alpha #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound\n",
    "            beta_md_old = beta_md.copy() # restarting\n",
    "            beta_md = (1-opt_alpha)*beta_ag + opt_alpha*beta\n",
    "            old_speed_norm = speed_norm # restarting\n",
    "            speed_norm = np.linalg.norm(beta_md - beta_md_old, ord=2) # restarting\n",
    "            converged = (np.linalg.norm(beta_md - beta_md_old, ord=np.infty) < tol)\n",
    "            smooth_grad = update_smooth_grad_SCAD(N=N, X=X, beta_md=beta_md, y=y, _lambda=_lambda, a=a)\n",
    "            beta = soft_thresholding(x=beta - opt_lambda*smooth_grad, lambda_=opt_lambda*_lambda)\n",
    "            beta_ag = soft_thresholding(x=beta_md - opt_beta*smooth_grad, lambda_=opt_beta*_lambda)\n",
    "#             converged = np.all(np.max(np.abs(beta_md - beta_ag)/opt_beta) < tol).item()\n",
    "#             converged = (np.linalg.norm(beta_md - beta_ag, ord=np.infty) < (tol*opt_beta))\n",
    "    else:\n",
    "#         L = np.max(np.array([L_convex, 1./(gamma)]))\n",
    "        L = np.linalg.norm(np.array([L_convex, 1./(gamma)]), ord=np.infty)\n",
    "        opt_beta = .99/L\n",
    "        while ((not converged) or (k<3)) and k <= maxit:\n",
    "            k += 1\n",
    "            if old_speed_norm > speed_norm and k - restart_k>=3: # in this case, restart\n",
    "                opt_alpha = 1. # restarting\n",
    "                restart_k = k # restarting\n",
    "            else: # restarting\n",
    "                opt_alpha = 2/(1+(1+4./opt_alpha**2)**.5) #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound \n",
    "            opt_lambda = opt_beta/opt_alpha #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound\n",
    "            beta_md_old = beta_md.copy() # restarting\n",
    "            beta_md = (1-opt_alpha)*beta_ag + opt_alpha*beta\n",
    "            old_speed_norm = speed_norm # restarting\n",
    "            speed_norm = np.linalg.norm(beta_md - beta_md_old, ord=2) # restarting\n",
    "            converged = (np.linalg.norm(beta_md - beta_md_old, ord=np.infty) < tol)\n",
    "            smooth_grad = update_smooth_grad_MCP(N=N, X=X, beta_md=beta_md, y=y, _lambda=_lambda, gamma=gamma)\n",
    "            beta = soft_thresholding(x=beta - opt_lambda*smooth_grad, lambda_=opt_lambda*_lambda)\n",
    "            beta_ag = soft_thresholding(x=beta_md - opt_beta*smooth_grad, lambda_=opt_beta*_lambda)\n",
    "#             converged = np.all(np.max(np.abs(beta_md - beta_ag)/opt_beta) < tol).item()\n",
    "#             converged = (np.linalg.norm(beta_md - beta_ag, ord=np.infty) < (tol*opt_beta))\n",
    "    return k, beta_md\n",
    "\n",
    "# def vanilla_proximal(self):\n",
    "#     '''\n",
    "#     Carry out optimization using vanilla gradient descent.\n",
    "#     '''\n",
    "#     if self.penalty == \"SCAD\":\n",
    "#         L = max([self.L_convex, 1/(self.a-1)])\n",
    "#         self.vanilla_stepsize = 1/L\n",
    "#         self.eval_obj_SCAD(self.beta_md, self.obj_value)\n",
    "#         self.eval_obj_SCAD(self.beta, self.obj_value_ORIGINAL)\n",
    "#         self.eval_obj_SCAD(self.beta_ag, self.obj_value_AG)\n",
    "#         self.old_beta = self.beta_md - 10.\n",
    "#         while not self.converged:\n",
    "#             self.k += 1\n",
    "#             if self.k <= self.maxit:\n",
    "#                 self.update_smooth_grad_SCAD()\n",
    "#                 self.beta_md = self.soft_thresholding(self.beta_md - self.vanilla_stepsize*self.smooth_grad, self.vanilla_stepsize*self._lambda)\n",
    "#                 self.converged = np.all(np.max(np.abs(self.beta_md - self.old_beta)) < self.tol).item()\n",
    "#                 self.old_beta = self.beta_md.copy()\n",
    "#                 self.eval_obj_SCAD(self.beta_md, self.obj_value)\n",
    "#                 self.eval_obj_SCAD(self.beta, self.obj_value_ORIGINAL)\n",
    "#                 self.eval_obj_SCAD(self.beta_ag, self.obj_value_AG)\n",
    "#             else:\n",
    "#                 break\n",
    "#     else:\n",
    "#         L = max([self.L_convex, 1/self.gamma])\n",
    "#         self.vanilla_stepsize = 1/L\n",
    "#         self.eval_obj_MCP(self.beta_md, self.obj_value)\n",
    "#         self.eval_obj_MCP(self.beta, self.obj_value_ORIGINAL)\n",
    "#         self.eval_obj_MCP(self.beta_ag, self.obj_value_AG)\n",
    "#         self.old_beta = self.beta_md - 10.\n",
    "#         while not self.converged:\n",
    "#             self.k += 1\n",
    "#             if self.k <= self.maxit:\n",
    "#                 self.update_smooth_grad_MCP()\n",
    "#                 self.beta_md = self.soft_thresholding(self.beta_md - self.vanilla_stepsize*self.smooth_grad, self.vanilla_stepsize*self._lambda)\n",
    "#                 self.converged = np.all(np.max(np.abs(self.beta_md - self.old_beta)) < self.tol).item()\n",
    "#                 self.old_beta = self.beta_md.copy()\n",
    "#                 self.eval_obj_MCP(self.beta_md, self.obj_value)\n",
    "#                 self.eval_obj_MCP(self.beta, self.obj_value_ORIGINAL)\n",
    "#                 self.eval_obj_MCP(self.beta_ag, self.obj_value_AG)\n",
    "#             else:\n",
    "#                 break\n",
    "#     return self.report_results()\n",
    "\n",
    "\n",
    "@jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def solution_path_LM(design_matrix, outcome, lambda_, beta_0 = np.ones(1), tol=1e-2, maxit=500, penalty=\"SCAD\", a=3.7, gamma=2., add_intercept_column=True):\n",
    "    '''\n",
    "    Carry out the optimization.\n",
    "    '''\n",
    "    #     add design matrix column for the intercept, if it's not there already\n",
    "    if add_intercept_column == True:\n",
    "        if np.any(X[:,0] != X[0,0]): # check if design matrix has included a column for intercept or not\n",
    "            intercept_design = np.ones(N).reshape(-1, 1)\n",
    "            _design_matrix = design_matrix.copy()\n",
    "            _design_matrix = np.hstack((intercept_design, _design_matrix))\n",
    "    beta_mat = np.zeros((len(lambda_)+1, _design_matrix.shape[1]))\n",
    "    for j in range(len(lambda_)):\n",
    "        beta_mat[j+1,:] = UAG_LM_SCAD_MCP(design_matrix=_design_matrix, outcome=outcome, beta_0 = beta_mat[j,:], tol=tol, maxit=maxit, _lambda=lambda_[j], penalty=penalty, a=a, gamma=gamma, add_intercept_column=False)[1]\n",
    "    return beta_mat[1:,:]\n",
    "\n",
    "\n",
    "\n",
    "# with strong rule \n",
    "\n",
    "# @jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def UAG_LM_SCAD_MCP_strongrule(design_matrix, outcome, beta_0 = np.ones(1), tol=1e-2, maxit=500, _lambda=.5, penalty=\"SCAD\", a=3.7, gamma=2., L_convex=1.1, add_intercept_column = True, strongrule=True):\n",
    "    '''\n",
    "    Carry out the optimization.\n",
    "    '''\n",
    "    X = design_matrix.copy()\n",
    "    y = outcome.copy()\n",
    "    N = X.shape[0]\n",
    "    if np.all(beta_0==np.ones(1)):\n",
    "        cov = (y - np.mean(y))@(X - 1/N*np.sum(X, 0).reshape(1,-1))\n",
    "        beta = np.sign(cov)\n",
    "    else:\n",
    "        beta = beta_0\n",
    "#     add design matrix column for the intercept, if it's not there already\n",
    "    if add_intercept_column == True:\n",
    "        if np.any(X[:,0] != X[0,0]): # check if design matrix has included a column for intercept or not\n",
    "            intercept_design = np.ones(N).reshape(-1, 1)\n",
    "            X = np.hstack((intercept_design, X))\n",
    "            beta = np.hstack((np.array([0.]), beta))\n",
    "    if strongrule == True:\n",
    "        _lambda_max = lambda_max_LM(X, y)\n",
    "        p_original = X.shape[1]\n",
    "        elim = strong_rule_seq_LM(X, y, beta_old=np.zeros(p_original), lambda_new=_lambda, lambda_old=_lambda_max)\n",
    "        X = X[:, np.logical_not(elim)]\n",
    "        beta = beta[np.logical_not(elim)]\n",
    "        \n",
    "    # passing other parameters\n",
    "    p = X.shape[1] # so here p includes the intercept design matrix column \n",
    "    smooth_grad = np.ones(p)\n",
    "    beta_ag = beta.copy()\n",
    "    beta_md = beta.copy()\n",
    "    k = 0\n",
    "    converged = False\n",
    "    opt_alpha = 1.\n",
    "#     L_convex = 1/N*np.max(np.linalg.eigvalsh(X@X.T)[-1]).item()\n",
    "    if L_convex == 1.1:\n",
    "        L_convex = 1/N*(np.linalg.eigvalsh(X@X.T)[-1])\n",
    "    else:\n",
    "        pass\n",
    "    old_speed_norm = 1.\n",
    "    speed_norm = 1.\n",
    "    restart_k = 0\n",
    "    \n",
    "    if penalty == \"SCAD\":\n",
    "#         L = np.max(np.array([L_convex, 1./(a-1)]))\n",
    "        L = np.linalg.norm(np.array([L_convex, 1./(a-1)]), ord=np.infty)\n",
    "        opt_beta = .99/L\n",
    "        while ((not converged) or (k<3)) and k <= maxit:\n",
    "            k += 1\n",
    "            if old_speed_norm > speed_norm and k - restart_k>=3: # in this case, restart\n",
    "                opt_alpha = 1. # restarting\n",
    "                restart_k = k # restarting\n",
    "            else: # restarting\n",
    "                opt_alpha = 2/(1+(1+4./opt_alpha**2)**.5) #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound \n",
    "            opt_lambda = opt_beta/opt_alpha #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound\n",
    "            beta_md_old = beta_md.copy() # restarting\n",
    "            beta_md = (1-opt_alpha)*beta_ag + opt_alpha*beta\n",
    "            old_speed_norm = speed_norm # restarting\n",
    "            speed_norm = np.linalg.norm(beta_md - beta_md_old, ord=2) # restarting\n",
    "            converged = (np.linalg.norm(beta_md - beta_md_old, ord=np.infty) < tol)\n",
    "            smooth_grad = update_smooth_grad_SCAD(N=N, X=X, beta_md=beta_md, y=y, _lambda=_lambda, a=a)\n",
    "            beta = soft_thresholding(x=beta - opt_lambda*smooth_grad, lambda_=opt_lambda*_lambda)\n",
    "            beta_ag = soft_thresholding(x=beta_md - opt_beta*smooth_grad, lambda_=opt_beta*_lambda)\n",
    "#             converged = np.all(np.max(np.abs(beta_md - beta_ag)/opt_beta) < tol).item()\n",
    "#             converged = (np.linalg.norm(beta_md - beta_ag, ord=np.infty) < (tol*opt_beta))\n",
    "    else:\n",
    "#         L = np.max(np.array([L_convex, 1./(gamma)]))\n",
    "        L = np.linalg.norm(np.array([L_convex, 1./(gamma)]), ord=np.infty)\n",
    "        opt_beta = .99/L\n",
    "        while ((not converged) or (k<3)) and k <= maxit:\n",
    "            k += 1\n",
    "            if old_speed_norm > speed_norm and k - restart_k>=3: # in this case, restart\n",
    "                opt_alpha = 1. # restarting\n",
    "                restart_k = k # restarting\n",
    "            else: # restarting\n",
    "                opt_alpha = 2/(1+(1+4./opt_alpha**2)**.5) #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound \n",
    "            opt_lambda = opt_beta/opt_alpha #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound\n",
    "            beta_md_old = beta_md.copy() # restarting\n",
    "            beta_md = (1-opt_alpha)*beta_ag + opt_alpha*beta\n",
    "            old_speed_norm = speed_norm # restarting\n",
    "            speed_norm = np.linalg.norm(beta_md - beta_md_old, ord=2) # restarting\n",
    "            converged = (np.linalg.norm(beta_md - beta_md_old, ord=np.infty) < tol)\n",
    "            smooth_grad = update_smooth_grad_MCP(N=N, X=X, beta_md=beta_md, y=y, _lambda=_lambda, gamma=gamma)\n",
    "            beta = soft_thresholding(x=beta - opt_lambda*smooth_grad, lambda_=opt_lambda*_lambda)\n",
    "            beta_ag = soft_thresholding(x=beta_md - opt_beta*smooth_grad, lambda_=opt_beta*_lambda)\n",
    "#             converged = np.all(np.max(np.abs(beta_md - beta_ag)/opt_beta) < tol).item()\n",
    "#             converged = (np.linalg.norm(beta_md - beta_ag, ord=np.infty) < (tol*opt_beta))\n",
    "    if strongrule == True: # this part of code can't compile\n",
    "        _beta_output = np.zeros(p_original)  # this part of code can't compile\n",
    "        _beta_output[np.logical_not(elim)] = beta_md  # this part of code can't compile\n",
    "    else:  # this part of code can't compile\n",
    "        _beta_output = beta_md  # this part of code can't compile\n",
    "    return k, _beta_output\n",
    "\n",
    "@jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def solution_path_LM_strongrule(design_matrix, outcome, lambda_, beta_0 = np.ones(1), tol=1e-2, maxit=500, penalty=\"SCAD\", a=3.7, gamma=2., add_intercept_column=True):\n",
    "    '''\n",
    "    Carry out the optimization.\n",
    "    '''\n",
    "    #     add design matrix column for the intercept, if it's not there already\n",
    "    _design_matrix = design_matrix.copy()\n",
    "    if add_intercept_column == True:\n",
    "        if np.any(design_matrix[:,0] != design_matrix[0,0]): # check if design matrix has included a column for intercept or not\n",
    "            intercept_design = np.ones(N).reshape(-1, 1)\n",
    "            _design_matrix = np.hstack((intercept_design, _design_matrix))\n",
    "    beta_mat = np.empty((len(lambda_)+1, _design_matrix.shape[1]))\n",
    "    beta_mat[0,:] = 0.\n",
    "    _lambda_max = lambda_max_LM(_design_matrix, outcome)\n",
    "    lambda_ = np.hstack((np.array([_lambda_max]), lambda_))\n",
    "    elim = np.array([False]*_design_matrix.shape[1])\n",
    "    for j in range(len(lambda_)-1):\n",
    "        _elim = strong_rule_seq_LM(X=_design_matrix, y=outcome, beta_old=beta_mat[j,:], lambda_new=lambda_[j+1], lambda_old=lambda_[j])\n",
    "        elim = np.logical_and(elim, _elim)\n",
    "        _beta_0 = beta_mat[j,:]\n",
    "        _new_beta = np.zeros(_design_matrix.shape[1])\n",
    "        _new_beta[np.logical_not(elim)] = UAG_LM_SCAD_MCP(design_matrix=_design_matrix[:, np.logical_not(elim)], outcome=outcome, beta_0 = _beta_0[np.logical_not(elim)], tol=tol, maxit=maxit, _lambda=lambda_[j], penalty=penalty, a=a, gamma=gamma, add_intercept_column=False)[1]\n",
    "        beta_mat[j+1,:] = _new_beta\n",
    "    return beta_mat[1:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-24T20:49:45.551381Z",
     "start_time": "2022-05-24T20:48:44.734530Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.25145030e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 1.25529911e+00  1.93834934e+00 -1.87491707e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 1.25529773e+00  1.94263335e+00 -1.88578904e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " ...\n",
      " [ 1.25529526e+00  1.87673754e+00 -1.91218195e+00 ...  7.69226132e-02\n",
      "  -8.24977017e-02  0.00000000e+00]\n",
      " [ 1.25529526e+00  1.87214777e+00 -1.90930101e+00 ...  8.90060269e-02\n",
      "  -8.95442224e-02  0.00000000e+00]\n",
      " [ 1.25529526e+00  1.86767007e+00 -1.90691276e+00 ...  1.00662533e-01\n",
      "  -9.61300285e-02  1.36140337e-04]]\n",
      "4.45 s ± 79.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "N = 1000\n",
    "SNR = 5.\n",
    "true_beta = np.array([2,-2,8,-8]+[0]*1000)\n",
    "X_cov = toeplitz(.6**np.arange(true_beta.shape[0]))\n",
    "mean = np.zeros(true_beta.shape[0])\n",
    "X = np.random.multivariate_normal(mean, X_cov, N)\n",
    "X -= np.mean(X,0).reshape(1,-1)\n",
    "X /= np.std(X,0)\n",
    "intercept_design_column = np.ones(N).reshape(N, 1)\n",
    "X_sim = np.concatenate((intercept_design_column, X), 1)\n",
    "true_sigma_sim = np.sqrt(true_beta.T@X_cov@true_beta/SNR)\n",
    "true_beta_intercept = np.concatenate((np.array([1.23]), true_beta)) # here just define the intercept to be 1.23 for simulated data \n",
    "epsilon = np.random.normal(0, true_sigma_sim, N)\n",
    "y_sim = X_sim@true_beta_intercept + epsilon\n",
    "\n",
    "\n",
    "\n",
    "lambda_seq = np.arange(40)/400\n",
    "lambda_seq = lambda_seq[1:]\n",
    "lambda_seq = lambda_seq[::-1]\n",
    "\n",
    "# do NOT include the design matrix intercept column \n",
    "LM_beta = solution_path_LM_strongrule(design_matrix=X_sim, outcome=y_sim, lambda_=lambda_seq, beta_0 = np.ones(1), tol=1e-2, maxit=500, penalty=\"SCAD\", a=3.7, gamma=2., add_intercept_column=True)\n",
    "\n",
    "print(LM_beta) # to make sure strong rule runs correctly \n",
    "\n",
    "%timeit solution_path_LM_strongrule(design_matrix=X_sim, outcome=y_sim, lambda_=lambda_seq, beta_0 = np.ones(1), tol=1e-2, maxit=500, penalty=\"SCAD\", a=3.7, gamma=2., add_intercept_column=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-24T20:50:10.643935Z",
     "start_time": "2022-05-24T20:49:45.553382Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# this cell is for profiling the function \n",
    "np.random.seed(0)\n",
    "N = 1000\n",
    "p_zeros = 2000\n",
    "SNR = 5.\n",
    "true_beta = np.array([2,-2,8,-8]+[0]*p_zeros)\n",
    "X_cov = toeplitz(0.6**np.arange(len(true_beta)))\n",
    "mean = np.zeros(len(true_beta))\n",
    "X = np.random.multivariate_normal(mean, X_cov, N)\n",
    "X -= np.mean(X,0).reshape(1,-1)\n",
    "X /= np.std(X,0)\n",
    "intercept_design_column = np.ones(N).reshape(N, 1)\n",
    "X_sim = np.concatenate((intercept_design_column, X), 1)\n",
    "true_sigma_sim = np.sqrt(true_beta.T@X_cov@true_beta/SNR)\n",
    "true_beta_intercept = np.concatenate((np.array([1.23]), true_beta)) # here just define the intercept to be 1.23 for simulated data \n",
    "epsilon = np.random.normal(0, true_sigma_sim, N)\n",
    "y_sim = X_sim@true_beta_intercept + epsilon\n",
    "\n",
    "fit1 = UAG_LM_SCAD_MCP_strongrule(design_matrix=X_sim, outcome=y_sim, tol=1e-2, maxit=500, _lambda=2, penalty=\"SCAD\", a=3.7, gamma=2.)\n",
    "\n",
    "fit2 = solution_path_LM(design_matrix=X_sim, outcome=y_sim, tol=1e-2, maxit=500, lambda_=np.linspace(.1,1,100), penalty=\"SCAD\", a=3.7, gamma=2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-24T20:50:14.170808Z",
     "start_time": "2022-05-24T20:50:10.645918Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365 ms ± 18.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit UAG_LM_SCAD_MCP_strongrule(design_matrix=X_sim, outcome=y_sim, tol=1e-2, maxit=500, _lambda=.5, penalty=\"MCP\", a=3.7, gamma=2.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-24T20:50:14.185811Z",
     "start_time": "2022-05-24T20:50:14.172815Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2005"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fit1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-24T20:50:14.340119Z",
     "start_time": "2022-05-24T20:50:14.189811Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(np.arange(len(fit1[1])), fit1[1], 'o')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-24T20:50:14.356116Z",
     "start_time": "2022-05-24T20:50:14.342116Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitivity:\n",
      "specificity:\n"
     ]
    }
   ],
   "source": [
    "print(\"sensitivity:\", )\n",
    "print(\"specificity:\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
