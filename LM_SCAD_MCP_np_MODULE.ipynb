{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the class fundementals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-22T17:00:01.598433Z",
     "start_time": "2022-05-22T17:00:00.648404Z"
    }
   },
   "outputs": [],
   "source": [
    "# import os, sys\n",
    "# import collections\n",
    "import numpy as np\n",
    "import matplotlib.markers as markers\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit\n",
    "# import collections\n",
    "from scipy.linalg import toeplitz, block_diag\n",
    "from scipy.stats import median_abs_deviation as mad\n",
    "# import multiprocessing\n",
    "# import cProfile\n",
    "# import itertools\n",
    "from numba import jit, njit\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # this is just to hide all the warnings\n",
    "# import rpy2.robjects as robjects\n",
    "# import matplotlib.pyplot as plt # change font globally to Times \n",
    "# plt.style.use('ggplot')\n",
    "# plt.rcParams.update({\n",
    "#     \"text.usetex\": True,\n",
    "#     \"font.family\": \"Times New Roman\",\n",
    "#     \"font.sans-serif\": [\"Times New Roman\"],\n",
    "#     \"font.size\": 12})\n",
    "\n",
    "# os.chdir(sys.path[0]) # ensure working direcotry is set same as the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-22T17:00:01.645430Z",
     "start_time": "2022-05-22T17:00:01.599406Z"
    }
   },
   "outputs": [],
   "source": [
    "######################################  some SCAD and MCP things  #######################################\n",
    "@jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def soft_thresholding(x, lambda_):\n",
    "    '''\n",
    "    To calculate soft-thresholding mapping of a given ONE-DIMENSIONAL tensor, BESIDES THE FIRST TERM (so beta_0 will not be penalized). \n",
    "    This function is to be used for calculation involving L1 penalty term later. \n",
    "    '''\n",
    "    return np.concatenate((np.array([x[0]]), np.where(np.abs(x[1:])>lambda_, x[1:] - np.sign(x[1:])*lambda_, 0)))\n",
    "\n",
    "@jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def SCAD(x, lambda_, a=3.7):\n",
    "    '''\n",
    "    To calculate SCAD penalty value;\n",
    "    #x can be a multi-dimensional tensor;\n",
    "    lambda_, a are scalars;\n",
    "    Fan and Li suggests to take a as 3.7 \n",
    "    '''\n",
    "    # here I notice the function is de facto a function of absolute value of x, therefore take absolute value first to simplify calculation \n",
    "    x = np.abs(x)\n",
    "    temp = np.where(x<=lambda_, lambda_*x, np.where(x<a*lambda_, (2*a*lambda_*x - x**2 - lambda_**2)/(2*(a - 1)), lambda_**2 * (a+1)/2))\n",
    "    temp[0] = 0. # this is to NOT penalize intercept beta later \n",
    "    return temp \n",
    "\n",
    "@jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def SCAD_grad(x, lambda_, a=3.7):\n",
    "    '''\n",
    "    To calculate the gradient of SCAD wrt. input x; \n",
    "    #x can be a multi-dimensional tensor. \n",
    "    '''\n",
    "    # here decompose x to sign and its absolute value for easier calculation\n",
    "    sgn = np.sign(x)\n",
    "    x = np.abs(x)\n",
    "    temp = np.where(x<=lambda_, lambda_*sgn, np.where(x<a*lambda_, (a*lambda_*sgn-sgn*x)/(a-1), 0))\n",
    "    temp[0] = 0. # this is to NOT penalize intercept beta later \n",
    "    return temp \n",
    "\n",
    "@jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def MCP(x, lambda_, gamma):\n",
    "    '''\n",
    "    To calculate MCP penalty value; \n",
    "    #x can be a multi-dimensional tensor. \n",
    "    '''\n",
    "    # the function is a function of absolute value of x \n",
    "    x = np.abs(x)\n",
    "    temp = np.where(x<=gamma*lambda_, lambda_*x - x**2/(2*gamma), .5*gamma*lambda_**2)\n",
    "    temp[0] = 0. # this is to NOT penalize intercept beta later \n",
    "    return temp \n",
    "\n",
    "@jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def MCP_grad(x, lambda_, gamma):\n",
    "    '''\n",
    "    To calculate MCP gradient wrt. input x; \n",
    "    #x can be a multi-dimensional tensor. \n",
    "    '''\n",
    "    temp = np.where(np.abs(x)<gamma*lambda_, lambda_*np.sign(x)-x/gamma, np.zeros_like(x))\n",
    "    temp[0] = 0. # this is to NOT penalize intercept beta later \n",
    "    return temp \n",
    "\n",
    "@jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def SCAD_concave(x, lambda_, a=3.7):\n",
    "    '''\n",
    "    The value of concave part of SCAD penalty; \n",
    "    #x can be a multi-dimensional tensor. \n",
    "    '''\n",
    "    x = np.abs(x)\n",
    "    temp = np.where(x<=lambda_, 0., np.where(x<a*lambda_, (lambda_*x - (x**2 + lambda_**2)/2)/(a-1), (a+1)/2*lambda_**2 - lambda_*x))\n",
    "    temp[0] = 0. # this is to NOT penalize intercept beta later \n",
    "    return temp \n",
    "\n",
    "@jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def SCAD_concave_grad(x, lambda_, a=3.7):\n",
    "    '''\n",
    "    The gradient of concave part of SCAD penalty wrt. input x; \n",
    "    #x can be a multi-dimensional tensor. \n",
    "    '''\n",
    "    sgn = np.sign(x)\n",
    "    x = np.abs(x)\n",
    "    temp = np.where(x<=lambda_, 0., np.where(x<a*lambda_, (lambda_*sgn-sgn*x)/(a-1), -lambda_*sgn))\n",
    "    temp[0] = 0. # this is to NOT penalize intercept beta later \n",
    "    return temp \n",
    "\n",
    "@jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def MCP_concave(x, lambda_, gamma):\n",
    "    '''\n",
    "    The value of concave part of MCP penalty; \n",
    "    #x can be a multi-dimensional tensor. \n",
    "    '''\n",
    "    # similiar as in MCP\n",
    "    x = np.abs(x)\n",
    "    temp = np.where(x<=gamma*lambda_, -(x**2)/(2*gamma), (gamma*lambda_**2)/2 - lambda_*x)\n",
    "    temp[0] = 0. # this is to NOT penalize intercept beta later \n",
    "    return temp \n",
    "\n",
    "@jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def MCP_concave_grad(x, lambda_, gamma):\n",
    "    '''\n",
    "    The gradient of concave part of MCP penalty wrt. input x; \n",
    "    #x can be a multi-dimensional tensor. \n",
    "    '''\n",
    "    temp = np.where(np.abs(x) < gamma*lambda_, -x/gamma, -lambda_*np.sign(x))\n",
    "    temp[0] = 0. # this is to NOT penalize intercept beta later \n",
    "    return temp \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-22T17:00:01.693442Z",
     "start_time": "2022-05-22T17:00:01.646402Z"
    }
   },
   "outputs": [],
   "source": [
    "@jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def update_smooth_grad_convex(N, X, beta_md, y):\n",
    "    '''\n",
    "    Update the gradient of the smooth convex objective component.\n",
    "    '''\n",
    "    return 1/N*X.T@(X@beta_md - y)\n",
    "\n",
    "@jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def update_smooth_grad_SCAD(N, X, beta_md, y, _lambda, a):\n",
    "    '''\n",
    "    Update the gradient of the smooth objective component for SCAD penalty.\n",
    "    '''\n",
    "    return update_smooth_grad_convex(N=N, X=X, beta_md=beta_md, y=y) + SCAD_concave_grad(x=beta_md, lambda_=_lambda, a=a)\n",
    "\n",
    "@jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def update_smooth_grad_MCP(N, X, beta_md, y, _lambda, gamma):\n",
    "    '''\n",
    "    Update the gradient of the smooth objective component for MCP penalty.\n",
    "    '''\n",
    "    return update_smooth_grad_convex(N=N, X=X, beta_md=beta_md, y=y) + MCP_concave_grad(x=beta_md, lambda_=_lambda, gamma=gamma)\n",
    "\n",
    "@jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def eval_obj_SCAD(N, X, beta_md, y, _lambda, a, x_temp):\n",
    "    '''\n",
    "    evaluate value of the objective function.\n",
    "    '''\n",
    "    error = y - X@x_temp\n",
    "    return (error.T@error)/(2.*N) + np.sum(SCAD(x_temp, lambda_=_lambda, a=a))\n",
    "\n",
    "@jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def eval_obj_MCP(N, X, beta_md, y, _lambda, gamma, x_temp):\n",
    "    '''\n",
    "    evaluate value of the objective function.\n",
    "    '''\n",
    "    error = y - X@x_temp\n",
    "    return (error.T@error)/(2*N) + np.sum(SCAD(x_temp, lambda_=_lambda, gamma=gamma))\n",
    "\n",
    "\n",
    "@jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def lambda_max_LM(X, y):\n",
    "    X_temp = X.copy()\n",
    "    X_temp = X_temp[:,1:]\n",
    "    X_temp -= np.mean(X_temp,0).reshape(1,-1)\n",
    "    X_temp /= np.std(X_temp,0)\n",
    "    y_temp = y.copy()\n",
    "    y_temp -= np.mean(y)\n",
    "    y_temp /= np.std(y)\n",
    "    grad_at_0 = y_temp@X_temp/len(y_temp)\n",
    "    lambda_max = np.linalg.norm(grad_at_0, ord=np.infty)\n",
    "    return lambda_max\n",
    "\n",
    "@jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def strong_rule_seq_LM(X, y, beta_old, lambda_new, lambda_old):\n",
    "    X_temp = X.copy()\n",
    "    X_temp -= np.mean(X_temp,0).reshape(1,-1)\n",
    "    X_temp /= np.std(X_temp,0)\n",
    "    y_temp = y.copy()\n",
    "    y_temp -= np.mean(y)\n",
    "    y_temp /= np.std(y)\n",
    "    grad = np.abs((y-X_temp@beta_old[1:])@X_temp/(2*len(y)))\n",
    "    eliminated = (grad < 2*lambda_new - lambda_old) # True means the value gets eliminated\n",
    "    eliminated = np.hstack((np.array([False]), eliminated)) # because intercept coefficient is not penalized\n",
    "    return eliminated\n",
    "\n",
    "@jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def UAG_LM_SCAD_MCP(design_matrix, outcome, beta_0 = np.ones(1), tol=1e-2, maxit=500, _lambda=-.5, penalty=\"SCAD\", a=3.7, gamma=2., L_convex=1.1, add_intercept_column = True):\n",
    "    '''\n",
    "    Carry out the optimization.\n",
    "    '''\n",
    "    X = design_matrix.copy()\n",
    "    y = outcome.copy()\n",
    "    N = X.shape[0]\n",
    "    if np.all(beta_0==np.ones(1)):\n",
    "        cov = (y - np.mean(y))@(X - 1/N*np.sum(X, 0).reshape(1,-1))\n",
    "        beta = np.sign(cov)\n",
    "    else:\n",
    "        beta = beta_0\n",
    "#     add design matrix column for the intercept, if it's not there already\n",
    "    if add_intercept_column == True:\n",
    "        if np.any(X[:,0] != X[0,0]): # check if design matrix has included a column for intercept or not\n",
    "            intercept_design = np.ones(N).reshape(-1, 1)\n",
    "            X = np.hstack((intercept_design, X))\n",
    "            beta = np.hstack((np.array([0.]), beta))\n",
    "    # passing other parameters\n",
    "    p = X.shape[1] # so here p includes the intercept design matrix column \n",
    "    smooth_grad = np.ones(p)\n",
    "    beta_ag = beta.copy()\n",
    "    beta_md = beta.copy()\n",
    "    k = 0\n",
    "    converged = False\n",
    "    opt_alpha = 1.\n",
    "#     L_convex = 1/N*np.max(np.linalg.eigvalsh(X@X.T)[-1]).item()\n",
    "    if L_convex == 1.1:\n",
    "        L_convex = 1/N*(np.linalg.eigvalsh(X@X.T)[-1])\n",
    "    else:\n",
    "        pass\n",
    "    old_speed_norm = 1.\n",
    "    speed_norm = 1.\n",
    "    restart_k = 0\n",
    "    \n",
    "    if penalty == \"SCAD\":\n",
    "#         L = np.max(np.array([L_convex, 1./(a-1)]))\n",
    "        L = np.linalg.norm(np.array([L_convex, 1./(a-1)]), ord=np.infty)\n",
    "        opt_beta = .99/L\n",
    "        while not converged:\n",
    "            k += 1\n",
    "            if k <= maxit: # restarting\n",
    "                if old_speed_norm > speed_norm and k - restart_k>=3: # in this case, restart\n",
    "                    opt_alpha = 1. # restarting\n",
    "                    restart_k = k # restarting\n",
    "                else: # restarting\n",
    "                    opt_alpha = 2/(1+(1+4/opt_alpha**2)**.5) #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound \n",
    "                opt_lambda = opt_beta/opt_alpha #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound\n",
    "                beta_md_old = beta_md.copy() # restarting\n",
    "                beta_md = (1-opt_alpha)*beta_ag + opt_alpha*beta\n",
    "                old_speed_norm = speed_norm # restarting\n",
    "                speed_norm = np.linalg.norm(beta_md - beta_md_old, ord=2) # restarting\n",
    "                smooth_grad = update_smooth_grad_SCAD(N=N, X=X, beta_md=beta_md, y=y, _lambda=_lambda, a=a)\n",
    "                beta = soft_thresholding(x=beta - opt_lambda*smooth_grad, lambda_=opt_lambda*_lambda)\n",
    "                beta_ag = soft_thresholding(x=beta_md - opt_beta*smooth_grad, lambda_=opt_beta*_lambda)\n",
    "#                 converged = np.all(np.max(np.abs(beta_md - beta_ag)/opt_beta) < tol).item()\n",
    "                converged = (np.linalg.norm(beta_md - beta_ag, ord=np.infty) < (tol*opt_beta))\n",
    "            else:\n",
    "                break\n",
    "    else:\n",
    "        L = np.linalg.norm(np.array([L_convex, 1./(gamma-1.)]), ord=np.infty)\n",
    "        opt_beta = .99/L\n",
    "        while not converged:\n",
    "            k += 1\n",
    "            if k <= maxit:\n",
    "                if old_speed_norm > speed_norm and k - restart_k>=3: # in this case, restart\n",
    "                    opt_alpha = 1. # restarting\n",
    "                    restart_k = k # restarting\n",
    "                else: # restarting\n",
    "                    opt_alpha = 2/(1+(1+4/opt_alpha**2)**.5) #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound\n",
    "                opt_lambda = opt_beta/opt_alpha #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound \n",
    "                beta_md = (1-opt_alpha)*beta_ag + opt_alpha*beta\n",
    "                smooth_grad = update_smooth_grad_MCP(N=N, X=X, beta_md=beta_md, y=y, _lambda=_lambda, gamma=gamma)\n",
    "                beta = soft_thresholding(x=beta - opt_lambda*smooth_grad, lambda_=opt_lambda*_lambda)\n",
    "                beta_ag = soft_thresholding(x=beta_md - opt_beta*smooth_grad, lambda_=opt_beta*_lambda)\n",
    "#                 converged = np.all(np.max(np.abs(beta_md - beta_ag)/opt_beta) < tol).item()\n",
    "                converged = (np.linalg.norm(beta_md - beta_ag, ord=np.infty) < (tol*opt_beta))\n",
    "            else:\n",
    "                break\n",
    "    return k, beta_md\n",
    "\n",
    "# def vanilla_proximal(self):\n",
    "#     '''\n",
    "#     Carry out optimization using vanilla gradient descent.\n",
    "#     '''\n",
    "#     if self.penalty == \"SCAD\":\n",
    "#         L = max([self.L_convex, 1/(self.a-1)])\n",
    "#         self.vanilla_stepsize = 1/L\n",
    "#         self.eval_obj_SCAD(self.beta_md, self.obj_value)\n",
    "#         self.eval_obj_SCAD(self.beta, self.obj_value_ORIGINAL)\n",
    "#         self.eval_obj_SCAD(self.beta_ag, self.obj_value_AG)\n",
    "#         self.old_beta = self.beta_md - 10.\n",
    "#         while not self.converged:\n",
    "#             self.k += 1\n",
    "#             if self.k <= self.maxit:\n",
    "#                 self.update_smooth_grad_SCAD()\n",
    "#                 self.beta_md = self.soft_thresholding(self.beta_md - self.vanilla_stepsize*self.smooth_grad, self.vanilla_stepsize*self._lambda)\n",
    "#                 self.converged = np.all(np.max(np.abs(self.beta_md - self.old_beta)) < self.tol).item()\n",
    "#                 self.old_beta = self.beta_md.copy()\n",
    "#                 self.eval_obj_SCAD(self.beta_md, self.obj_value)\n",
    "#                 self.eval_obj_SCAD(self.beta, self.obj_value_ORIGINAL)\n",
    "#                 self.eval_obj_SCAD(self.beta_ag, self.obj_value_AG)\n",
    "#             else:\n",
    "#                 break\n",
    "#     else:\n",
    "#         L = max([self.L_convex, 1/self.gamma])\n",
    "#         self.vanilla_stepsize = 1/L\n",
    "#         self.eval_obj_MCP(self.beta_md, self.obj_value)\n",
    "#         self.eval_obj_MCP(self.beta, self.obj_value_ORIGINAL)\n",
    "#         self.eval_obj_MCP(self.beta_ag, self.obj_value_AG)\n",
    "#         self.old_beta = self.beta_md - 10.\n",
    "#         while not self.converged:\n",
    "#             self.k += 1\n",
    "#             if self.k <= self.maxit:\n",
    "#                 self.update_smooth_grad_MCP()\n",
    "#                 self.beta_md = self.soft_thresholding(self.beta_md - self.vanilla_stepsize*self.smooth_grad, self.vanilla_stepsize*self._lambda)\n",
    "#                 self.converged = np.all(np.max(np.abs(self.beta_md - self.old_beta)) < self.tol).item()\n",
    "#                 self.old_beta = self.beta_md.copy()\n",
    "#                 self.eval_obj_MCP(self.beta_md, self.obj_value)\n",
    "#                 self.eval_obj_MCP(self.beta, self.obj_value_ORIGINAL)\n",
    "#                 self.eval_obj_MCP(self.beta_ag, self.obj_value_AG)\n",
    "#             else:\n",
    "#                 break\n",
    "#     return self.report_results()\n",
    "\n",
    "\n",
    "@jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def solution_path_LM(design_matrix, outcome, lambda_, beta_0 = np.ones(1), tol=1e-2, maxit=500, penalty=\"SCAD\", a=3.7, gamma=2., add_intercept_column=True):\n",
    "    '''\n",
    "    Carry out the optimization.\n",
    "    '''\n",
    "    #     add design matrix column for the intercept, if it's not there already\n",
    "    if add_intercept_column == True:\n",
    "        if np.any(X[:,0] != X[0,0]): # check if design matrix has included a column for intercept or not\n",
    "            intercept_design = np.ones(N).reshape(-1, 1)\n",
    "            _design_matrix = design_matrix.copy()\n",
    "            _design_matrix = np.hstack((intercept_design, _design_matrix))\n",
    "    beta_mat = np.zeros((len(lambda_)+1, _design_matrix.shape[1]))\n",
    "    for j in range(len(lambda_)):\n",
    "        beta_mat[j+1,:] = UAG_LM_SCAD_MCP(design_matrix=_design_matrix, outcome=outcome, beta_0 = beta_mat[j,:], tol=tol, maxit=maxit, _lambda=lambda_[j], penalty=penalty, a=a, gamma=gamma, add_intercept_column=False)[1]\n",
    "    return beta_mat[1:,:]\n",
    "\n",
    "\n",
    "\n",
    "# with strong rule \n",
    "\n",
    "@jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def UAG_LM_SCAD_MCP_strongrule(design_matrix, outcome, beta_0 = np.ones(1), tol=1e-2, maxit=500, _lambda=-.5, penalty=\"SCAD\", a=3.7, gamma=2., L_convex=1.1, add_intercept_column = True):\n",
    "    '''\n",
    "    Carry out the optimization.\n",
    "    '''\n",
    "    X = design_matrix.copy()\n",
    "    y = outcome.copy()\n",
    "    N = X.shape[0]\n",
    "    if np.all(beta_0==np.ones(1)):\n",
    "        cov = (y - np.mean(y))@(X - 1/N*np.sum(X, 0).reshape(1,-1))\n",
    "        beta = np.sign(cov)\n",
    "    else:\n",
    "        beta = beta_0\n",
    "#     add design matrix column for the intercept, if it's not there already\n",
    "    if add_intercept_column == True:\n",
    "        if np.any(X[:,0] != X[0,0]): # check if design matrix has included a column for intercept or not\n",
    "            intercept_design = np.ones(N).reshape(-1, 1)\n",
    "            X = np.hstack((intercept_design, X))\n",
    "            beta = np.hstack((np.array([0.]), beta))\n",
    "    if strongrule == True:\n",
    "        elim = strong_rule_seq_LM(X, y, beta_old=np.zeros(), lambda_new=, lambda_old=)\n",
    "        ### LOOK AT HERE!!!\n",
    "        \n",
    "    # passing other parameters\n",
    "    p = X.shape[1] # so here p includes the intercept design matrix column \n",
    "    smooth_grad = np.ones(p)\n",
    "    beta_ag = beta.copy()\n",
    "    beta_md = beta.copy()\n",
    "    k = 0\n",
    "    converged = False\n",
    "    opt_alpha = 1.\n",
    "#     L_convex = 1/N*np.max(np.linalg.eigvalsh(X@X.T)[-1]).item()\n",
    "    if L_convex == 1.1:\n",
    "        L_convex = 1/N*(np.linalg.eigvalsh(X@X.T)[-1])\n",
    "    else:\n",
    "        pass\n",
    "    old_speed_norm = 1.\n",
    "    speed_norm = 1.\n",
    "    restart_k = 0\n",
    "    \n",
    "    if penalty == \"SCAD\":\n",
    "#         L = np.max(np.array([L_convex, 1./(a-1)]))\n",
    "        L = np.linalg.norm(np.array([L_convex, 1./(a-1)]), ord=np.infty)\n",
    "        opt_beta = .99/L\n",
    "        while not converged:\n",
    "            k += 1\n",
    "            if k <= maxit: # restarting\n",
    "                if old_speed_norm > speed_norm and k - restart_k>=3: # in this case, restart\n",
    "                    opt_alpha = 1. # restarting\n",
    "                    restart_k = k # restarting\n",
    "                else: # restarting\n",
    "                    opt_alpha = 2/(1+(1+4/opt_alpha**2)**.5) #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound \n",
    "                opt_lambda = opt_beta/opt_alpha #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound\n",
    "                beta_md_old = beta_md.copy() # restarting\n",
    "                beta_md = (1-opt_alpha)*beta_ag + opt_alpha*beta\n",
    "                old_speed_norm = speed_norm # restarting\n",
    "                speed_norm = np.linalg.norm(beta_md - beta_md_old, ord=2) # restarting\n",
    "                smooth_grad = update_smooth_grad_SCAD(N=N, X=X, beta_md=beta_md, y=y, _lambda=_lambda, a=a)\n",
    "                beta = soft_thresholding(x=beta - opt_lambda*smooth_grad, lambda_=opt_lambda*_lambda)\n",
    "                beta_ag = soft_thresholding(x=beta_md - opt_beta*smooth_grad, lambda_=opt_beta*_lambda)\n",
    "#                 converged = np.all(np.max(np.abs(beta_md - beta_ag)/opt_beta) < tol).item()\n",
    "                converged = (np.linalg.norm(beta_md - beta_ag, ord=np.infty) < (tol*opt_beta))\n",
    "            else:\n",
    "                break\n",
    "    else:\n",
    "        L = np.linalg.norm(np.array([L_convex, 1./(gamma-1.)]), ord=np.infty)\n",
    "        opt_beta = .99/L\n",
    "        while not converged:\n",
    "            k += 1\n",
    "            if k <= maxit:\n",
    "                if old_speed_norm > speed_norm and k - restart_k>=3: # in this case, restart\n",
    "                    opt_alpha = 1. # restarting\n",
    "                    restart_k = k # restarting\n",
    "                else: # restarting\n",
    "                    opt_alpha = 2/(1+(1+4/opt_alpha**2)**.5) #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound\n",
    "                opt_lambda = opt_beta/opt_alpha #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound \n",
    "                beta_md = (1-opt_alpha)*beta_ag + opt_alpha*beta\n",
    "                smooth_grad = update_smooth_grad_MCP(N=N, X=X, beta_md=beta_md, y=y, _lambda=_lambda, gamma=gamma)\n",
    "                beta = soft_thresholding(x=beta - opt_lambda*smooth_grad, lambda_=opt_lambda*_lambda)\n",
    "                beta_ag = soft_thresholding(x=beta_md - opt_beta*smooth_grad, lambda_=opt_beta*_lambda)\n",
    "#                 converged = np.all(np.max(np.abs(beta_md - beta_ag)/opt_beta) < tol).item()\n",
    "                converged = (np.linalg.norm(beta_md - beta_ag, ord=np.infty) < (tol*opt_beta))\n",
    "            else:\n",
    "                break\n",
    "    return k, beta_md\n",
    "\n",
    "@jit(nopython=True, cache=True, parallel=True, fastmath=True, nogil=True)\n",
    "def solution_path_LM_strongrule(design_matrix, outcome, lambda_, beta_0 = np.ones(1), tol=1e-2, maxit=500, penalty=\"SCAD\", a=3.7, gamma=2., add_intercept_column=True):\n",
    "    '''\n",
    "    Carry out the optimization.\n",
    "    '''\n",
    "    #     add design matrix column for the intercept, if it's not there already\n",
    "    if add_intercept_column == True:\n",
    "        if np.any(X[:,0] != X[0,0]): # check if design matrix has included a column for intercept or not\n",
    "            intercept_design = np.ones(N).reshape(-1, 1)\n",
    "            _design_matrix = design_matrix.copy()\n",
    "            _design_matrix = np.hstack((intercept_design, _design_matrix))\n",
    "    beta_mat = np.zeros((len(lambda_)+1, _design_matrix.shape[1]))\n",
    "    for j in range(len(lambda_)):\n",
    "        beta_mat[j+1,:] = UAG_LM_SCAD_MCP(design_matrix=_design_matrix, outcome=outcome, beta_0 = beta_mat[j,:], tol=tol, maxit=maxit, _lambda=lambda_[j], penalty=penalty, a=a, gamma=gamma, add_intercept_column=False)[1]\n",
    "    return beta_mat[1:,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-22T17:00:19.610402Z",
     "start_time": "2022-05-22T17:00:01.694402Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# this cell is for profiling the function \n",
    "np.random.seed(0)\n",
    "N = 1000\n",
    "p_zeros = 20\n",
    "SNR = 5.\n",
    "true_beta = np.array([2]*100+[0]*p_zeros)\n",
    "X_cov = toeplitz(0.6**np.arange(100+p_zeros))\n",
    "mean = np.zeros(100+p_zeros)\n",
    "X = np.random.multivariate_normal(mean, X_cov, N)\n",
    "X -= np.mean(X,0).reshape(1,-1)\n",
    "X /= np.std(X,0)\n",
    "intercept_design_column = np.ones(N).reshape(N, 1)\n",
    "X_sim = np.concatenate((intercept_design_column, X), 1)\n",
    "true_sigma_sim = np.sqrt(true_beta.T@X_cov@true_beta/SNR)\n",
    "true_beta_intercept = np.concatenate((np.array([1.23]), true_beta)) # here just define the intercept to be 1.23 for simulated data \n",
    "epsilon = np.random.normal(0, true_sigma_sim, N)\n",
    "y_sim = X_sim@true_beta_intercept + epsilon\n",
    "\n",
    "fit1 = UAG_LM_SCAD_MCP(design_matrix=X_sim, outcome=y_sim, tol=1e-2, maxit=500, _lambda=.2, penalty=\"SCAD\", a=3.7, gamma=2.)\n",
    "\n",
    "fit2 = solution_path_LM(design_matrix=X_sim, outcome=y_sim, tol=1e-2, maxit=500, lambda_=np.linspace(.1,1,100), penalty=\"SCAD\", a=3.7, gamma=2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-22T17:00:30.971404Z",
     "start_time": "2022-05-22T17:00:19.611402Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145 ms ± 21.7 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit UAG_LM_SCAD_MCP(design_matrix=X_sim, outcome=y_sim, tol=1e-2, maxit=500, _lambda=.2, penalty=\"MCP\", a=3.7, gamma=2.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-22T17:00:30.987402Z",
     "start_time": "2022-05-22T17:00:30.972403Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60,\n",
       " array([ 1.21352443e+00,  1.55665761e+00,  1.40500778e+00,  2.46887976e+00,\n",
       "         1.83761266e+00,  3.15122244e+00,  1.01836623e+00,  2.46468169e+00,\n",
       "         1.69532302e+00,  3.21271493e+00,  0.00000000e+00,  4.10270705e+00,\n",
       "         1.45938940e+00,  1.82996964e+00,  3.59488409e+00, -3.08452956e-01,\n",
       "         1.69431561e+00,  3.48633485e+00,  0.00000000e+00,  2.18634013e+00,\n",
       "         2.76406643e+00,  1.14996755e+00,  2.54479680e+00,  2.30036936e+00,\n",
       "         2.65905594e+00,  9.50730496e-01,  1.83016277e+00,  1.19656558e+00,\n",
       "         1.44995373e+00,  3.05728294e+00,  1.48723588e+00,  2.31290622e+00,\n",
       "         9.28043041e-01,  2.02531935e+00,  2.73460735e+00,  2.90720146e+00,\n",
       "         0.00000000e+00,  2.61860526e+00,  1.63913768e+00,  1.98396239e+00,\n",
       "         1.90971015e+00,  1.19879154e+00,  3.40716307e+00,  1.12368678e+00,\n",
       "         3.71974778e+00,  0.00000000e+00,  3.03387595e+00,  1.96106548e+00,\n",
       "         2.25199195e+00,  1.32607732e+00,  1.66424011e+00,  3.19584747e+00,\n",
       "         2.34181858e+00,  9.84881330e-01,  1.46783273e+00,  1.65948929e+00,\n",
       "         1.61126500e+00,  2.94468219e+00,  1.68011580e+00,  1.61163624e+00,\n",
       "         1.09569644e+00,  2.98838925e+00,  2.01311748e+00,  1.44981878e+00,\n",
       "         2.59241735e+00,  2.23154017e+00,  1.43903585e+00,  1.80970311e+00,\n",
       "         2.40694101e+00,  1.53913862e+00,  2.44756544e+00,  1.41347944e+00,\n",
       "         1.66131151e+00,  2.12010883e+00,  2.21376454e+00,  1.17866439e+00,\n",
       "         1.02142382e+00,  3.09979985e+00,  2.84282206e+00,  1.49185124e+00,\n",
       "         2.67977312e+00,  1.48549769e+00,  2.62881263e+00,  3.12831176e+00,\n",
       "         0.00000000e+00,  3.92912465e+00,  2.00574329e+00,  2.20447118e+00,\n",
       "         2.14483999e+00,  2.01252044e+00,  2.29469141e+00,  2.59389565e+00,\n",
       "         9.57641157e-01,  2.87448439e+00,  2.08310812e+00,  1.08177498e+00,\n",
       "         3.12611117e+00,  2.01881437e-01,  4.09213322e+00,  1.06588440e+00,\n",
       "         7.47187092e-01,  1.00876176e+00, -1.02551456e+00,  8.34209807e-01,\n",
       "         0.00000000e+00, -9.11403448e-01,  0.00000000e+00,  3.97365131e-03,\n",
       "         0.00000000e+00,  6.27901077e-01,  0.00000000e+00, -6.89491565e-02,\n",
       "         0.00000000e+00,  8.74920483e-01, -8.72310705e-01,  0.00000000e+00,\n",
       "        -8.41342220e-01,  1.22585030e+00, -1.28710172e+00,  0.00000000e+00,\n",
       "         0.00000000e+00]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-22T17:00:31.145404Z",
     "start_time": "2022-05-22T17:00:30.989405Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(np.arange(len(fit1[1])), fit1[1], 'o')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-22T17:00:31.161403Z",
     "start_time": "2022-05-22T17:00:31.146404Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitivity: 0.95\n",
      "specificity: 0.004\n"
     ]
    }
   ],
   "source": [
    "print(\"sensitivity:\", np.sum(fit1[1][1:101]!=0)/100)\n",
    "print(\"specificity:\", np.sum(fit1[1][101:]==0)/2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
